{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E11. 인공지능 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-7. 프로젝트: 멋진 작사가 만들기\n",
    "\n",
    "## 1) 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# tf\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Look, I was gonna go easy on you and not to hurt your feelings', \"But I'm only going to get this one chance\", \"Something's wrong, I can feel it (Six minutes, Slim Shady, you're on)\"]\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/e/e11_lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제\n",
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
    "\n",
    "---\n",
    "`preprocess_sentence()` 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
    "\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가면 잘라내기를 권합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "187088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Look, I was gonna go easy on you and not to hurt your feelings',\n",
       " \"But I'm only going to get this one chance\",\n",
       " \"Something's wrong, I can feel it (Six minutes, Slim Shady, you're on)\",\n",
       " \"Just a feeling I've got, like something's about to happen, but I don't know what\",\n",
       " \"If that means, what I think it means, we're in trouble, big trouble,\",\n",
       " \"And if he is as bananas as you say, I'm not taking any chances\",\n",
       " \"You were just what the doctor ordered I'm beginning to feel like a Rap God, Rap God\",\n",
       " 'All my people from the front to the back nod, back nod',\n",
       " 'Now who thinks their arms are long enough to slap box, slap box?',\n",
       " 'They said I rap like a robot, so call me Rapbot But for me to rap like a computer must be in my genes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(raw_corpus))\n",
    "print(len(raw_corpus))\n",
    "raw_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxlen 설정 시, n단어를 넘어가는 문장의 나머지 부분은 쓰지 않고 버려버리기에 오히려 문법상, 문맥상 의미가 없어진다고 판단하여 `len(list.split())`이 15개 이상, 즉 한 문장의 길이가 긴 경우 고려하지 않도록 설정했다.    \n",
    "이런 식으로 설정하니 추후 param 갯수가 천만 대를 초과하지 않았으며, 학습 또한 비교적 빠른 시간 내에 실행할 수 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> look , i was gonna go easy on you and not to hurt your feelings <end>',\n",
       " '<start> but i m only going to get this one chance <end>',\n",
       " '<start> something s wrong , i can feel it six minutes , slim shady , you re on <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 길이가 0인 문장 정제\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence.split()) >= 15: continue\n",
    "    \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "\n",
    "len(corpus)\n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2 133   4 ...   0   0   0]\n",
      " [  2  32   5 ...   0   0   0]\n",
      " [  2 195  16 ...   0   0   0]\n",
      " ...\n",
      " [  2   5  61 ...   0   0   0]\n",
      " [  2 113 657 ...   0   0   0]\n",
      " [  2   8  50 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f0b3c68a990>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=10000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리\n",
    "훈련 데이터와 평가 데이터를 분리하세요!\n",
    "\n",
    "---\n",
    "\n",
    "`tokenize()` 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 `train_test_split()` 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상으로 설정하세요! 총 데이터의 20%를 평가 데이터셋으로 사용해 주세요!\n",
    "\n",
    "```python\n",
    "enc_train, enc_val, dec_train, dec_val = <코드 작성>\n",
    "```\n",
    "여기까지 올바르게 진행했을 경우, 아래 실행 결과를 확인할 수 있습니다.\n",
    "```python\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "```\n",
    "```python\n",
    "out:\n",
    "\n",
    "Source Train: (124960, 14)\n",
    "Target Train: (124960, 14)\n",
    "```\n",
    "\n",
    "만약 결과가 다르다면 천천히 과정을 다시 살펴 동일한 결과를 얻도록 하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  133    4    5   57   99   54  418   18    7    8   68   10  424\n",
      "   21 1026    3    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0] (166325, 32)\n",
      "[ 133    4    5   57   99   54  418   18    7    8   68   10  424   21\n",
      " 1026    3    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0] (166325, 32)\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0], src_input.shape)\n",
    "print(tgt_input[0], src_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input,\n",
    "                                                          random_state=2020, test_size=0.2)\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input)\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((256, 32), (256, 32)), types: (tf.int32, tf.int32)> \n",
      " <BatchDataset shapes: ((256, 32), (256, 32)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset, \"\\n\", val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. 인공지능 만들기\n",
    "\n",
    "---\n",
    "\n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 `val_loss` 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n",
    "\n",
    "그리고 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!\n",
    "\n",
    "\n",
    "```python\n",
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. 모델이 생성한 가사 한 줄을 제출하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.fnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.fnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.fnn_1(out)\n",
    "        out = self.fnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words+1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 32, 10001), dtype=float32, numpy=\n",
       "array([[[-4.53679968e-05,  3.25786823e-04,  1.75364563e-04, ...,\n",
       "          4.05189785e-05, -1.90848732e-04, -2.00633236e-04],\n",
       "        [-3.87281762e-05,  7.77760812e-04,  2.90703116e-04, ...,\n",
       "          4.27653140e-04, -3.42163519e-04, -4.52699169e-04],\n",
       "        [ 1.07326341e-05,  1.15496211e-03,  7.62828160e-04, ...,\n",
       "          1.04057114e-03, -3.13268480e-04, -5.07660618e-04],\n",
       "        ...,\n",
       "        [-2.64499034e-03,  1.95209740e-03,  8.91469463e-05, ...,\n",
       "          1.53649447e-03, -7.72142038e-03,  4.21432592e-03],\n",
       "        [-2.67033256e-03,  1.99263287e-03,  7.12552428e-05, ...,\n",
       "          1.51124236e-03, -7.73690408e-03,  4.23988653e-03],\n",
       "        [-2.69060652e-03,  2.02699122e-03,  5.43223432e-05, ...,\n",
       "          1.49045046e-03, -7.74812698e-03,  4.26130928e-03]],\n",
       "\n",
       "       [[-4.53679968e-05,  3.25786823e-04,  1.75364563e-04, ...,\n",
       "          4.05189785e-05, -1.90848732e-04, -2.00633236e-04],\n",
       "        [-4.45816142e-04,  4.84901946e-04, -2.79609929e-04, ...,\n",
       "          6.58015430e-04, -6.35090517e-04, -2.95558188e-04],\n",
       "        [-6.71462214e-04,  7.44172896e-04, -5.77634957e-04, ...,\n",
       "          9.63919098e-04, -9.08535672e-04, -3.50788614e-04],\n",
       "        ...,\n",
       "        [-2.59168679e-03,  1.81342766e-03,  1.46138569e-04, ...,\n",
       "          1.54809945e-03, -7.70677300e-03,  4.13447805e-03],\n",
       "        [-2.62478902e-03,  1.87208143e-03,  1.24633909e-04, ...,\n",
       "          1.52092520e-03, -7.72439828e-03,  4.16856026e-03],\n",
       "        [-2.65197642e-03,  1.92274048e-03,  1.03969818e-04, ...,\n",
       "          1.49815460e-03, -7.73682492e-03,  4.19774512e-03]],\n",
       "\n",
       "       [[-4.53679968e-05,  3.25786823e-04,  1.75364563e-04, ...,\n",
       "          4.05189785e-05, -1.90848732e-04, -2.00633236e-04],\n",
       "        [-3.66741042e-05,  3.16495571e-04,  2.81365588e-04, ...,\n",
       "          4.78284841e-04, -3.28830531e-04, -6.93438691e-04],\n",
       "        [-7.69972467e-05,  3.80933197e-04,  3.93225520e-04, ...,\n",
       "          5.62350615e-04,  1.55885675e-04, -1.11624529e-03],\n",
       "        ...,\n",
       "        [-2.51580030e-03,  1.91210792e-03,  9.76734082e-05, ...,\n",
       "          1.54502352e-03, -7.77915539e-03,  4.14608698e-03],\n",
       "        [-2.55853334e-03,  1.95789407e-03,  8.26038886e-05, ...,\n",
       "          1.52260775e-03, -7.78822228e-03,  4.18277830e-03],\n",
       "        [-2.59404234e-03,  1.99680077e-03,  6.77809148e-05, ...,\n",
       "          1.50347059e-03, -7.79375155e-03,  4.21330240e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-4.53679968e-05,  3.25786823e-04,  1.75364563e-04, ...,\n",
       "          4.05189785e-05, -1.90848732e-04, -2.00633236e-04],\n",
       "        [-5.20173518e-04,  8.16054642e-04,  4.20258060e-04, ...,\n",
       "         -1.54807291e-04,  5.50440163e-05, -6.75818650e-04],\n",
       "        [-8.20167421e-04,  1.17876707e-03,  5.40926179e-04, ...,\n",
       "         -3.19662940e-04,  5.79653308e-04, -1.09803712e-03],\n",
       "        ...,\n",
       "        [-2.54345150e-03,  1.82162726e-03,  2.52566446e-04, ...,\n",
       "          1.66429265e-03, -7.65894772e-03,  4.00231732e-03],\n",
       "        [-2.57923175e-03,  1.88210187e-03,  2.23093521e-04, ...,\n",
       "          1.62493775e-03, -7.69260619e-03,  4.06001974e-03],\n",
       "        [-2.60959798e-03,  1.93386443e-03,  1.94389097e-04, ...,\n",
       "          1.59068289e-03, -7.71765364e-03,  4.10885969e-03]],\n",
       "\n",
       "       [[-4.53679968e-05,  3.25786823e-04,  1.75364563e-04, ...,\n",
       "          4.05189785e-05, -1.90848732e-04, -2.00633236e-04],\n",
       "        [ 2.15588225e-04,  8.34787497e-04,  2.97384104e-04, ...,\n",
       "         -1.36728719e-04, -2.74055492e-04, -4.16356401e-04],\n",
       "        [-1.15611634e-04,  8.51430872e-04,  5.04635100e-04, ...,\n",
       "         -6.18974038e-04, -2.43980554e-04, -9.45135835e-04],\n",
       "        ...,\n",
       "        [-2.46237218e-03,  1.79630122e-03,  1.92983804e-04, ...,\n",
       "          1.61917100e-03, -7.71670416e-03,  4.13185172e-03],\n",
       "        [-2.51908833e-03,  1.85858516e-03,  1.75147361e-04, ...,\n",
       "          1.58859801e-03, -7.74453394e-03,  4.16800240e-03],\n",
       "        [-2.56582210e-03,  1.91244611e-03,  1.55916205e-04, ...,\n",
       "          1.56134705e-03, -7.76397390e-03,  4.19828389e-03]],\n",
       "\n",
       "       [[-4.53679968e-05,  3.25786823e-04,  1.75364563e-04, ...,\n",
       "          4.05189785e-05, -1.90848732e-04, -2.00633236e-04],\n",
       "        [-2.77723506e-04,  3.96956719e-04,  4.97500005e-04, ...,\n",
       "          3.89319204e-04, -4.58586524e-04, -6.15736877e-04],\n",
       "        [-3.83498438e-04,  4.20486962e-04,  4.38684394e-04, ...,\n",
       "          4.43700526e-04, -6.89449080e-04, -1.05441036e-03],\n",
       "        ...,\n",
       "        [-2.30456283e-03,  1.39260152e-03,  1.47620114e-04, ...,\n",
       "          1.73028349e-03, -7.47199170e-03,  3.97719536e-03],\n",
       "        [-2.38510175e-03,  1.51103805e-03,  1.32440167e-04, ...,\n",
       "          1.68338965e-03, -7.54515035e-03,  4.03687963e-03],\n",
       "        [-2.45345454e-03,  1.61419273e-03,  1.16305135e-04, ...,\n",
       "          1.64163648e-03, -7.60214590e-03,  4.08804789e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  5120512   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  6295552   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  10251025  \n",
      "=================================================================\n",
      "Total params: 30,059,793\n",
      "Trainable params: 30,059,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "519/519 [==============================] - 157s 302ms/step - loss: 1.6523 - accuracy: 0.7596 - val_loss: 1.4226 - val_accuracy: 0.7751\n",
      "Epoch 2/10\n",
      "519/519 [==============================] - 151s 291ms/step - loss: 1.3662 - accuracy: 0.7791 - val_loss: 1.3320 - val_accuracy: 0.7819\n",
      "Epoch 3/10\n",
      "519/519 [==============================] - 151s 290ms/step - loss: 1.2883 - accuracy: 0.7844 - val_loss: 1.2789 - val_accuracy: 0.7860\n",
      "Epoch 4/10\n",
      "519/519 [==============================] - 187s 360ms/step - loss: 1.2283 - accuracy: 0.7888 - val_loss: 1.2388 - val_accuracy: 0.7894\n",
      "Epoch 5/10\n",
      "519/519 [==============================] - 186s 359ms/step - loss: 1.1768 - accuracy: 0.7928 - val_loss: 1.2093 - val_accuracy: 0.7922\n",
      "Epoch 6/10\n",
      "519/519 [==============================] - 154s 296ms/step - loss: 1.1302 - accuracy: 0.7968 - val_loss: 1.1830 - val_accuracy: 0.7955\n",
      "Epoch 7/10\n",
      "519/519 [==============================] - 161s 311ms/step - loss: 1.0869 - accuracy: 0.8009 - val_loss: 1.1609 - val_accuracy: 0.7987\n",
      "Epoch 8/10\n",
      "519/519 [==============================] - 156s 301ms/step - loss: 1.0464 - accuracy: 0.8053 - val_loss: 1.1432 - val_accuracy: 0.8016\n",
      "Epoch 9/10\n",
      "519/519 [==============================] - 155s 298ms/step - loss: 1.0078 - accuracy: 0.8098 - val_loss: 1.1271 - val_accuracy: 0.8044\n",
      "Epoch 10/10\n",
      "519/519 [==============================] - 156s 300ms/step - loss: 0.9707 - accuracy: 0.8145 - val_loss: 1.1139 - val_accuracy: 0.8075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0abc57fe50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\")\n",
    "\n",
    "model.compile(loss = loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.fit(dataset, validation_data=val_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <END>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i want you <end> '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m the one , yeah , yeah <end> '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i will eat you alive <end> '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i will\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> here i am , baby , i m a star <end> '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> here i\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> why you wanna be with me <end> '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> why\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is a beautiful thing <end> '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 명사들을 넣어 text generation을 해 본 결과 모든 문장들이 그럴듯하게 생성되었다.    \n",
    "다른 분들이 maxlen설정의 유/무에 따라 파라미터 갯수, 학습시간 등이 양분되었음을 감안할 때 속도가 빠르면서도 문장 중간에 maxlen번째 단어까지만 취급하여 반쪽짜리 문장을 넣지 않는 모델을 설계하여 좋은 결과를 얻을 수 있었다.\n",
    "```\n",
    "love is a beautiful thing!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
