{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E15. seq2seq 단어 단위 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "import re\n",
    "print('⏳')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__훈련 데이터 샘플 33000개만 사용하기__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14626</th>\n",
       "      <td>I can't save you.</td>\n",
       "      <td>Je ne peux pas te sauver.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27937</th>\n",
       "      <td>You're a funny man.</td>\n",
       "      <td>Vous êtes un drôle de type.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14712</th>\n",
       "      <td>I don't hate you.</td>\n",
       "      <td>Je ne te hais point.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20209</th>\n",
       "      <td>It depends on you.</td>\n",
       "      <td>Cela dépend de vous.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7262</th>\n",
       "      <td>Are you hiring?</td>\n",
       "      <td>Recrutez-vous ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       eng                          fra\n",
       "14626    I can't save you.    Je ne peux pas te sauver.\n",
       "27937  You're a funny man.  Vous êtes un drôle de type.\n",
       "14712    I don't hate you.         Je ne te hais point.\n",
       "20209   It depends on you.         Cela dépend de vous.\n",
       "7262       Are you hiring?              Recrutez-vous ?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/e/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "\n",
    "lines = lines[['eng','fra']][:33000]\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 정제, 정규화, 전처리 (영어, 프랑스어 모두!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 2. 소문자로 바꾸기\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # 1. 구두점을 단어와 분리\n",
    "    sentence = re.sub(r\"([.,!?])\",r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r\"[']\", r\"\", sentence)\n",
    "    \n",
    "    sentence = sentence.strip().split()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'my', 'names', 'slim', 'shady', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hi my name's Slim Shady.\"\n",
    "sentence = preprocess_sentence(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>33000</td>\n",
       "      <td>33000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20971</td>\n",
       "      <td>30180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>You're the teacher.</td>\n",
       "      <td>Merci bien.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng          fra\n",
       "count                 33000        33000\n",
       "unique                20971        30180\n",
       "top     You're the teacher.  Merci bien.\n",
       "freq                     26            9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. 디코더의 문장에 시작 토큰과 종료 토큰을 넣어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3346</th>\n",
       "      <td>[come, join, us, .]</td>\n",
       "      <td>[viens, te, joindre, à, nous, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30451</th>\n",
       "      <td>[im, a, beginner, ,, too, .]</td>\n",
       "      <td>[je, suis, également, un, débutant, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>[he, let, us, go, .]</td>\n",
       "      <td>[il, nous, a, laissés, partir, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27304</th>\n",
       "      <td>[what, a, strange, dog, !]</td>\n",
       "      <td>[quel, chien, étrange, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20448</th>\n",
       "      <td>[let, me, pay, for, it, .]</td>\n",
       "      <td>[laissez-moi, le, payer, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                eng                                     fra\n",
       "3346            [come, join, us, .]        [viens, te, joindre, à, nous, !]\n",
       "30451  [im, a, beginner, ,, too, .]  [je, suis, également, un, débutant, .]\n",
       "3507           [he, let, us, go, .]       [il, nous, a, laissés, partir, .]\n",
       "27304    [what, a, strange, dog, !]               [quel, chien, étrange, !]\n",
       "20448    [let, me, pay, for, it, .]             [laissez-moi, le, payer, .]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.eng = lines.eng.apply(lambda x: preprocess_sentence(x))\n",
    "lines.fra = lines.fra.apply(lambda x: preprocess_sentence(x))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25939</th>\n",
       "      <td>[she, looks, lonesome, .]</td>\n",
       "      <td>[&lt;sos&gt;, elle, a, lair, solitaire, ., &lt;eos&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12568</th>\n",
       "      <td>[they, can, manage, .]</td>\n",
       "      <td>[&lt;sos&gt;, ils, peuvent, gérer, ., &lt;eos&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25679</th>\n",
       "      <td>[my, battery, is, dead, .]</td>\n",
       "      <td>[&lt;sos&gt;, ma, batterie, est, vide, ., &lt;eos&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31854</th>\n",
       "      <td>[the, crops, need, rain, .]</td>\n",
       "      <td>[&lt;sos&gt;, les, cultures, ont, besoin, de, pluie,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>[he, banged, his, knee, .]</td>\n",
       "      <td>[&lt;sos&gt;, il, sest, cogné, au, genou, ., &lt;eos&gt;]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               eng  \\\n",
       "25939    [she, looks, lonesome, .]   \n",
       "12568       [they, can, manage, .]   \n",
       "25679   [my, battery, is, dead, .]   \n",
       "31854  [the, crops, need, rain, .]   \n",
       "23565   [he, banged, his, knee, .]   \n",
       "\n",
       "                                                     fra  \n",
       "25939        [<sos>, elle, a, lair, solitaire, ., <eos>]  \n",
       "12568             [<sos>, ils, peuvent, gérer, ., <eos>]  \n",
       "25679         [<sos>, ma, batterie, est, vide, ., <eos>]  \n",
       "31854  [<sos>, les, cultures, ont, besoin, de, pluie,...  \n",
       "23565      [<sos>, il, sest, cogné, au, genou, ., <eos>]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'\n",
    "\n",
    "lines.fra = lines.fra.apply(lambda x: ['<sos>'] + x + ['<eos>'])\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 케라스의 토크나이저로 텍스트를 숫자로 바꿔보세요.\n",
    "---\n",
    "tokenizer.texts_to_sequences()를 사용하여 모든 샘플에 대해서 정수 시퀀스로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[28, 1], [1160, 1], [1160, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=False) # 단어 단위 Tokenizer\n",
    "eng_tokenizer.fit_on_texts(lines.eng)       # 33000개 eng 행에 각각 토큰화\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)  # 단어 -> 숫자값 인덱스 변환 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 77, 7, 2], [1, 1087, 7, 2], [1, 1087, 3, 2]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=False)\n",
    "fra_tokenizer.fit_on_texts(lines.fra)\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장 크기: 4786 \n",
      "불어 단어장 크기 9839\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print(\"영어 단어장 크기:\", eng_vocab_size, \"\\n불어 단어장 크기\", fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 15\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__전체 통계량__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4786\n",
      "프랑스어 단어장의 크기 : 9839\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 15\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__시작, 종료 토큰 각각 제거__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16601      [<sos>, ils, ont, tous, arrêté, ., <eos>]\n",
       "8137           [<sos>, je, vis, en, ville, ., <eos>]\n",
       "29010    [<sos>, il, est, fort, au, rugby, ., <eos>]\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.fra.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'\n",
    "\n",
    "encoder_input = input_text\n",
    "decoder_input = [[word for word in line if word != fra_tokenizer.word_index[eos_token]] for line in target_text]\n",
    "decoder_target = [[word for word in line if word != fra_tokenizer.word_index[sos_token]] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 77, 7], [1, 1087, 7], [1, 1087, 3]]\n",
      "[[77, 7, 2], [1087, 7, 2], [1087, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 15)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 15)\n"
     ]
    }
   ],
   "source": [
    "# padding\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding=\"post\")\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding=\"post\")\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding=\"post\")\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__shuffle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "encoder_input = encoder_input[idx]\n",
    "decoder_input = decoder_input[idx]\n",
    "decoder_target = decoder_target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20   4  37 297   3   0   0   0]\n",
      "[13912 25462 26631 ... 32897 22264 13212]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (30000, 8)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (30000, 15)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (30000, 15)\n",
      "영어 검증데이터의 크기(shape) : (3000, 8)\n",
      "프랑스어 검증 입력데이터의 크기(shape) : (3000, 15)\n",
      "프랑스어 검증 출력데이터의 크기(shape) : (3000, 15)\n"
     ]
    }
   ],
   "source": [
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_train))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_train))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_train))\n",
    "\n",
    "print('영어 검증데이터의 크기(shape) :',np.shape(encoder_input_test))\n",
    "print('프랑스어 검증 입력데이터의 크기(shape) :',np.shape(decoder_input_test))\n",
    "print('프랑스어 검증 출력데이터의 크기(shape) :',np.shape(decoder_target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 임베딩 층(Embedding layer) 사용하기\n",
    "---\n",
    "이번에는 입력이 되는 각 단어를 임베딩 층을 사용하여 벡터화하겠습니다.\n",
    "\n",
    "임베딩 층을 사용하는 방법과 그 설명에 대해서는 아래의 링크의\n",
    "\n",
    "__1. 케라스 임베딩 층(Keras Embedding layer)__를 참고하세요.\n",
    "\n",
    "[위키독스](https://wikidocs.net/33793)\n",
    "\n",
    "실제 번역기 구현을 위해서 사용할 수 있는 인코더 코드의 예시는 다음과 같습니다.\n",
    "\n",
    "이를 통해서 인코더와 디코더의 임베딩 층을 각각 구현해보세요.\n",
    "\n",
    "from tensorflow.keras.layers import Input, mbedding, Masking\n",
    "\n",
    "```python\n",
    "# 인코더에서 사용할 임베딩 층 사용 예시\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(단어장의 크기, 임베딩 벡터의 차원)(encoder_inputs)\n",
    "encoder_lstm = LSTM(hidden state의 크기, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "```\n",
    "\n",
    "주의할 점은 인코더와 디코더의 임베딩 층은 서로 다른 임베딩 층을 사용해야 하지만,\n",
    "\n",
    "디코더의 훈련 과정과 테스트 과정(예측 과정)에서의 임베딩 층은 동일해야 합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__인코더 임베딩__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "enc_emb = Embedding(eng_vocab_size, output_size, input_length=max_eng_seq_len)(encoder_inputs)\n",
    "# padding값은 연산하지 않는다\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(units=output_size, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, None])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__디코더 임베딩__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None)\n",
      "(None, None, 32) \n",
      "\n",
      "(None, None, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([None, None, 9839])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_inputs = Input(shape=(None, ))\n",
    "dec_emb = Embedding(fra_vocab_size, output_size)(decoder_inputs)\n",
    "\n",
    "print(decoder_inputs.shape)\n",
    "print(dec_emb.shape, \"\\n\")\n",
    "\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "decoder_lstm = LSTM(units=output_size, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state = encoder_states)\n",
    "\n",
    "print(dec_masking.shape)\n",
    "\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation=\"softmax\")\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. 모델 구현하기\n",
    "---\n",
    "글자 단위 번역기에서 구현한 모델을 참고로 단어 단위 번역기의 모델을 완성시켜보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 32)     153152      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 32)     314848      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 32)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 32)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 32), (None,  8320        masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 32), ( 8320        masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 9839)   324687      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 809,327\n",
      "Trainable params: 809,327\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__학습__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 4.0328 - acc: 0.6055 - val_loss: 2.2377 - val_acc: 0.6071\n",
      "Epoch 2/40\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 1.9729 - acc: 0.6766 - val_loss: 1.8125 - val_acc: 0.7363\n",
      "Epoch 3/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 1.7049 - acc: 0.7414 - val_loss: 1.6582 - val_acc: 0.7484\n",
      "Epoch 4/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 1.5846 - acc: 0.7589 - val_loss: 1.5727 - val_acc: 0.7652\n",
      "Epoch 5/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 1.5076 - acc: 0.7691 - val_loss: 1.5155 - val_acc: 0.7727\n",
      "Epoch 6/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 1.4524 - acc: 0.7772 - val_loss: 1.4671 - val_acc: 0.7805\n",
      "Epoch 7/40\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 1.4059 - acc: 0.7841 - val_loss: 1.4240 - val_acc: 0.7872\n",
      "Epoch 8/40\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 1.3597 - acc: 0.7903 - val_loss: 1.3810 - val_acc: 0.7922\n",
      "Epoch 9/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 1.3151 - acc: 0.7964 - val_loss: 1.3431 - val_acc: 0.7985\n",
      "Epoch 10/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 1.2756 - acc: 0.8025 - val_loss: 1.3113 - val_acc: 0.8040\n",
      "Epoch 11/40\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 1.2400 - acc: 0.8070 - val_loss: 1.2836 - val_acc: 0.8068\n",
      "Epoch 12/40\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 1.2078 - acc: 0.8108 - val_loss: 1.2592 - val_acc: 0.8102\n",
      "Epoch 13/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 1.1808 - acc: 0.8140 - val_loss: 1.2392 - val_acc: 0.8136\n",
      "Epoch 14/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 1.1560 - acc: 0.8167 - val_loss: 1.2226 - val_acc: 0.8142\n",
      "Epoch 15/40\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 1.1342 - acc: 0.8186 - val_loss: 1.2179 - val_acc: 0.8143\n",
      "Epoch 16/40\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 1.1151 - acc: 0.8207 - val_loss: 1.1916 - val_acc: 0.8173\n",
      "Epoch 17/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 1.0983 - acc: 0.8228 - val_loss: 1.1810 - val_acc: 0.8199\n",
      "Epoch 18/40\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 1.0836 - acc: 0.8247 - val_loss: 1.1707 - val_acc: 0.8200\n",
      "Epoch 19/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 1.0706 - acc: 0.8266 - val_loss: 1.1632 - val_acc: 0.8210\n",
      "Epoch 20/40\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 1.0590 - acc: 0.8281 - val_loss: 1.1579 - val_acc: 0.8230\n",
      "Epoch 21/40\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 1.0479 - acc: 0.8298 - val_loss: 1.1513 - val_acc: 0.8240\n",
      "Epoch 22/40\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 1.0376 - acc: 0.8311 - val_loss: 1.1420 - val_acc: 0.8245\n",
      "Epoch 23/40\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 1.0272 - acc: 0.8325 - val_loss: 1.1366 - val_acc: 0.8251\n",
      "Epoch 24/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 1.0176 - acc: 0.8338 - val_loss: 1.1302 - val_acc: 0.8255\n",
      "Epoch 25/40\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 1.0085 - acc: 0.8349 - val_loss: 1.1269 - val_acc: 0.8264\n",
      "Epoch 26/40\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 1.0000 - acc: 0.8359 - val_loss: 1.1244 - val_acc: 0.8270\n",
      "Epoch 27/40\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.9922 - acc: 0.8371 - val_loss: 1.1156 - val_acc: 0.8278\n",
      "Epoch 28/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.9845 - acc: 0.8380 - val_loss: 1.1106 - val_acc: 0.8290\n",
      "Epoch 29/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.9770 - acc: 0.8391 - val_loss: 1.1069 - val_acc: 0.8294\n",
      "Epoch 30/40\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.9698 - acc: 0.8401 - val_loss: 1.1040 - val_acc: 0.8300\n",
      "Epoch 31/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.9619 - acc: 0.8410 - val_loss: 1.0994 - val_acc: 0.8308\n",
      "Epoch 32/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.9543 - acc: 0.8420 - val_loss: 1.0988 - val_acc: 0.8308\n",
      "Epoch 33/40\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.9475 - acc: 0.8430 - val_loss: 1.0904 - val_acc: 0.8312\n",
      "Epoch 34/40\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.9414 - acc: 0.8442 - val_loss: 1.0910 - val_acc: 0.8320\n",
      "Epoch 35/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.9362 - acc: 0.8452 - val_loss: 1.0885 - val_acc: 0.8319\n",
      "Epoch 36/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.9314 - acc: 0.8462 - val_loss: 1.0849 - val_acc: 0.8328\n",
      "Epoch 37/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.9267 - acc: 0.8474 - val_loss: 1.0825 - val_acc: 0.8337\n",
      "Epoch 38/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.9220 - acc: 0.8484 - val_loss: 1.0839 - val_acc: 0.8333\n",
      "Epoch 39/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.9177 - acc: 0.8493 - val_loss: 1.0784 - val_acc: 0.8348\n",
      "Epoch 40/40\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.9129 - acc: 0.8503 - val_loss: 1.0783 - val_acc: 0.8347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5be7394410>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_train, decoder_input_train], decoder_target_train, \\\n",
    "         validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "         batch_size=128, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 32)          153152    \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 32), (None, 32),  8320      \n",
      "=================================================================\n",
      "Total params: 161,472\n",
      "Trainable params: 161,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc_model = Model(inputs=encoder_inputs, outputs = encoder_states)\n",
    "enc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "decoder_state_input_h = Input(shape=(output_size, ))\n",
    "decoder_state_input_c = Input(shape=(output_size, ))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb_test = Embedding(fra_vocab_size, output_size)(decoder_inputs)\n",
    "decoder_outputs_test, state_h_test, state_c_test = decoder_lstm(dec_emb_test, initial_state = decoder_states_inputs)\n",
    "decoder_states_test = [state_h_test, state_c_test]\n",
    "\n",
    "decoder_outputs_test = decoder_softmax_layer(decoder_outputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 32)     314848      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 32), ( 8320        embedding_2[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 9839)   324687      lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 647,855\n",
      "Trainable params: 647,855\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs_test] + decoder_states_test)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = enc_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = fra2idx[\"<sos>\"]\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 반환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + idx2eng[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=fra2idx['<sos>']) and i!=fra2idx['<eos>']):\n",
    "            temp = temp + idx2fra[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. 모델 평가하기\n",
    "---\n",
    "단어 단위 번역기에 대해서 훈련 데이터의 샘플과 테스트 데이터의 샘플에 대해서 번역 문장을 만들어보고 정답 문장과 번역 문장을 비교해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: ['hi', '.']\n",
      "정답 문장: ['salut', '!']\n",
      "번역기가 번역한 문장:  ce pas . . . . \n",
      "-----------------------------------\n",
      "입력 문장: ['call', 'us', '.']\n",
      "정답 문장: ['appelez-nous', '!']\n",
      "번역기가 번역한 문장:  la en . . . . \n",
      "-----------------------------------\n",
      "입력 문장: ['i', 'dozed', '.']\n",
      "정답 문장: ['je', 'me', 'suis', 'assoupie', '.']\n",
      "번역기가 번역한 문장:  les les . . . \n",
      "-----------------------------------\n",
      "입력 문장: ['how', 'rude', '!']\n",
      "정답 문장: ['quelle', 'grossièreté', '!']\n",
      "번역기가 번역한 문장:  ne à . . . . . \n",
      "-----------------------------------\n",
      "입력 문장: ['of', 'course', '!']\n",
      "정답 문장: ['pardi', '!']\n",
      "번역기가 번역한 문장:  la . . . . . . \n",
      "-----------------------------------\n",
      "입력 문장: ['we', 'did', 'it', '.']\n",
      "정답 문장: ['nous', 'lavons', 'fait', '.']\n",
      "번역기가 번역한 문장:  vous de de . . \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [1, 100, 155, 303, 888, 1012]:\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론\n",
    "- 번역기가 전혀 제대로 작동하고 있지 않다\n",
    "- 정규표현식과 파라미터를 바꿔도 마찬가지이다\n",
    "- nlp에 대해 좀 더 집중적으로 공부해보고 나서 다시 도전해봐야겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel002",
   "language": "python",
   "name": "aiffel2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
