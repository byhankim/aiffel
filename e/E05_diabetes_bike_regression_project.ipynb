{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 1. 선형회귀 설계하여 당뇨병 수치 맞춰보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈, 라이브러리 import\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.datasets as sd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename'])\n",
      "(442, 10)\n",
      "[ 0.03807591  0.05068012  0.06169621  0.02187235 -0.0442235  -0.03482076\n",
      " -0.04340085 -0.00259226  0.01990842 -0.01764613]\n",
      "(442,)\n",
      "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, T-Cells (a type of white blood cells)\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, thyroid stimulating hormone\n",
      "      - s5      ltg, lamotrigine\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 살펴보기\n",
    "diabetes = sd.load_diabetes()\n",
    "\n",
    "# diabetes 타입\n",
    "print(type(diabetes))\n",
    "# diabetes 안에 담긴 정보 확인하기\n",
    "print(diabetes.keys())\n",
    "# 데이터를 따로 변수에 저장\n",
    "db_data = diabetes.data\n",
    "# db_data 형상정보 출력\n",
    "print(db_data.shape)\n",
    "# 샘플 확인\n",
    "print(db_data[0])\n",
    "\n",
    "# label\n",
    "db_label = diabetes.target\n",
    "print(db_label.shape)\n",
    "print(diabetes.feature_names)\n",
    "\n",
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# df_X, df_y = sd.load_diabetes(return_X_y=True)\n",
    "df_X, df_y = diabetes.data, diabetes.target\n",
    "\n",
    "print(df_X.shape)\n",
    "print(df_y.shape)\n",
    "print(type(df_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 모델에 입력할 데이터 `X` 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 1)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "X=df_X\n",
    "y=pd.DataFrame(df_y).to_numpy() \n",
    "print(y.shape)\n",
    "\n",
    "y=df_y\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) 모델에 예측할 데이터 `y` 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "print(type(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) train, test 떼이터로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (353,)\n",
      "(89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) 모델 준비하기\n",
    "- 입력 데이터 개수에 맞는 가중치 **`W`**와 **`b`**를 준비해주세요.\n",
    "- 모델 함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33437582 0.88461612 0.42041461 0.25858825 0.6415775  0.6842004\n",
      " 0.22434106 0.68607347 0.62197929 0.21196238] 0.35776965170608566\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "print(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33437582, 0.88461612, 0.42041461, 0.25858825, 0.6415775 ,\n",
       "       0.6842004 , 0.22434106, 0.68607347, 0.62197929, 0.21196238])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model(X,W,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) 손실함수 `loss` 정의하기\n",
    "손실함수를 MSE 함수로 정의해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a, b):\n",
    "    mse = ((a-b) ** 2).mean()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28955.834951553712"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X,W,b,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) 기울기를 구하는 `gradient` 함수 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, W, b, y):\n",
    "    # N은 가중치의 개수\n",
    "    N = len(W)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "#error=model(X,W,b)-y\n",
    "#error.shape\n",
    "#y_train.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: [ -60.57164922  -13.65457225 -189.57445542 -142.64895505  -68.18805608\n",
      "  -55.89992543  127.57895064 -138.89760756 -182.80454803 -123.50796562]\n",
      "db: -303.5514290223797\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dW, db = gradient(X, W, b, y)\n",
    "print(\"dW:\", dW)\n",
    "print(\"db:\", db)\n",
    "print(type(dW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (8) 하이퍼 파라미터인 학습률 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[ -60.57164922  -13.65457225 -189.57445542 -142.64895505  -68.18805608\n",
      "  -55.89992543  127.57895064 -138.89760756 -182.80454803 -123.50796562]\n"
     ]
    }
   ],
   "source": [
    "print(type(dW))\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (9) 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 10 : Loss 28859.8424\n",
      "Iteration # 20 : Loss 28764.2276\n",
      "Iteration # 30 : Loss 28668.9891\n",
      "Iteration # 40 : Loss 28574.1254\n",
      "Iteration # 50 : Loss 28479.6350\n",
      "Iteration # 60 : Loss 28385.5165\n",
      "Iteration # 70 : Loss 28291.7682\n",
      "Iteration # 80 : Loss 28198.3889\n",
      "Iteration # 90 : Loss 28105.3769\n",
      "Iteration # 100 : Loss 28012.7309\n",
      "Iteration # 110 : Loss 27920.4493\n",
      "Iteration # 120 : Loss 27828.5308\n",
      "Iteration # 130 : Loss 27736.9738\n",
      "Iteration # 140 : Loss 27645.7769\n",
      "Iteration # 150 : Loss 27554.9388\n",
      "Iteration # 160 : Loss 27464.4579\n",
      "Iteration # 170 : Loss 27374.3328\n",
      "Iteration # 180 : Loss 27284.5622\n",
      "Iteration # 190 : Loss 27195.1446\n",
      "Iteration # 200 : Loss 27106.0786\n",
      "Iteration # 210 : Loss 27017.3628\n",
      "Iteration # 220 : Loss 26928.9958\n",
      "Iteration # 230 : Loss 26840.9763\n",
      "Iteration # 240 : Loss 26753.3027\n",
      "Iteration # 250 : Loss 26665.9738\n",
      "Iteration # 260 : Loss 26578.9882\n",
      "Iteration # 270 : Loss 26492.3446\n",
      "Iteration # 280 : Loss 26406.0414\n",
      "Iteration # 290 : Loss 26320.0775\n",
      "Iteration # 300 : Loss 26234.4514\n",
      "Iteration # 310 : Loss 26149.1618\n",
      "Iteration # 320 : Loss 26064.2074\n",
      "Iteration # 330 : Loss 25979.5868\n",
      "Iteration # 340 : Loss 25895.2987\n",
      "Iteration # 350 : Loss 25811.3418\n",
      "Iteration # 360 : Loss 25727.7147\n",
      "Iteration # 370 : Loss 25644.4161\n",
      "Iteration # 380 : Loss 25561.4448\n",
      "Iteration # 390 : Loss 25478.7994\n",
      "Iteration # 400 : Loss 25396.4786\n",
      "Iteration # 410 : Loss 25314.4811\n",
      "Iteration # 420 : Loss 25232.8057\n",
      "Iteration # 430 : Loss 25151.4511\n",
      "Iteration # 440 : Loss 25070.4159\n",
      "Iteration # 450 : Loss 24989.6990\n",
      "Iteration # 460 : Loss 24909.2990\n",
      "Iteration # 470 : Loss 24829.2146\n",
      "Iteration # 480 : Loss 24749.4447\n",
      "Iteration # 490 : Loss 24669.9880\n",
      "Iteration # 500 : Loss 24590.8432\n",
      "Iteration # 510 : Loss 24512.0090\n",
      "Iteration # 520 : Loss 24433.4843\n",
      "Iteration # 530 : Loss 24355.2679\n",
      "Iteration # 540 : Loss 24277.3584\n",
      "Iteration # 550 : Loss 24199.7546\n",
      "Iteration # 560 : Loss 24122.4554\n",
      "Iteration # 570 : Loss 24045.4596\n",
      "Iteration # 580 : Loss 23968.7658\n",
      "Iteration # 590 : Loss 23892.3730\n",
      "Iteration # 600 : Loss 23816.2799\n",
      "Iteration # 610 : Loss 23740.4853\n",
      "Iteration # 620 : Loss 23664.9881\n",
      "Iteration # 630 : Loss 23589.7870\n",
      "Iteration # 640 : Loss 23514.8809\n",
      "Iteration # 650 : Loss 23440.2686\n",
      "Iteration # 660 : Loss 23365.9489\n",
      "Iteration # 670 : Loss 23291.9207\n",
      "Iteration # 680 : Loss 23218.1828\n",
      "Iteration # 690 : Loss 23144.7340\n",
      "Iteration # 700 : Loss 23071.5732\n",
      "Iteration # 710 : Loss 22998.6993\n",
      "Iteration # 720 : Loss 22926.1111\n",
      "Iteration # 730 : Loss 22853.8074\n",
      "Iteration # 740 : Loss 22781.7872\n",
      "Iteration # 750 : Loss 22710.0493\n",
      "Iteration # 760 : Loss 22638.5926\n",
      "Iteration # 770 : Loss 22567.4159\n",
      "Iteration # 780 : Loss 22496.5182\n",
      "Iteration # 790 : Loss 22425.8984\n",
      "Iteration # 800 : Loss 22355.5552\n",
      "Iteration # 810 : Loss 22285.4877\n",
      "Iteration # 820 : Loss 22215.6947\n",
      "Iteration # 830 : Loss 22146.1752\n",
      "Iteration # 840 : Loss 22076.9280\n",
      "Iteration # 850 : Loss 22007.9521\n",
      "Iteration # 860 : Loss 21939.2464\n",
      "Iteration # 870 : Loss 21870.8097\n",
      "Iteration # 880 : Loss 21802.6412\n",
      "Iteration # 890 : Loss 21734.7395\n",
      "Iteration # 900 : Loss 21667.1038\n",
      "Iteration # 910 : Loss 21599.7330\n",
      "Iteration # 920 : Loss 21532.6259\n",
      "Iteration # 930 : Loss 21465.7816\n",
      "Iteration # 940 : Loss 21399.1990\n",
      "Iteration # 950 : Loss 21332.8771\n",
      "Iteration # 960 : Loss 21266.8147\n",
      "Iteration # 970 : Loss 21201.0110\n",
      "Iteration # 980 : Loss 21135.4648\n",
      "Iteration # 990 : Loss 21070.1752\n",
      "Iteration # 1000 : Loss 21005.1410\n",
      "Iteration # 1010 : Loss 20940.3614\n",
      "Iteration # 1020 : Loss 20875.8352\n",
      "Iteration # 1030 : Loss 20811.5616\n",
      "Iteration # 1040 : Loss 20747.5393\n",
      "Iteration # 1050 : Loss 20683.7676\n",
      "Iteration # 1060 : Loss 20620.2453\n",
      "Iteration # 1070 : Loss 20556.9715\n",
      "Iteration # 1080 : Loss 20493.9452\n",
      "Iteration # 1090 : Loss 20431.1654\n",
      "Iteration # 1100 : Loss 20368.6311\n",
      "Iteration # 1110 : Loss 20306.3414\n",
      "Iteration # 1120 : Loss 20244.2952\n",
      "Iteration # 1130 : Loss 20182.4916\n",
      "Iteration # 1140 : Loss 20120.9297\n",
      "Iteration # 1150 : Loss 20059.6085\n",
      "Iteration # 1160 : Loss 19998.5269\n",
      "Iteration # 1170 : Loss 19937.6842\n",
      "Iteration # 1180 : Loss 19877.0792\n",
      "Iteration # 1190 : Loss 19816.7111\n",
      "Iteration # 1200 : Loss 19756.5790\n",
      "Iteration # 1210 : Loss 19696.6818\n",
      "Iteration # 1220 : Loss 19637.0187\n",
      "Iteration # 1230 : Loss 19577.5887\n",
      "Iteration # 1240 : Loss 19518.3909\n",
      "Iteration # 1250 : Loss 19459.4243\n",
      "Iteration # 1260 : Loss 19400.6881\n",
      "Iteration # 1270 : Loss 19342.1814\n",
      "Iteration # 1280 : Loss 19283.9031\n",
      "Iteration # 1290 : Loss 19225.8525\n",
      "Iteration # 1300 : Loss 19168.0286\n",
      "Iteration # 1310 : Loss 19110.4305\n",
      "Iteration # 1320 : Loss 19053.0572\n",
      "Iteration # 1330 : Loss 18995.9080\n",
      "Iteration # 1340 : Loss 18938.9819\n",
      "Iteration # 1350 : Loss 18882.2781\n",
      "Iteration # 1360 : Loss 18825.7956\n",
      "Iteration # 1370 : Loss 18769.5335\n",
      "Iteration # 1380 : Loss 18713.4911\n",
      "Iteration # 1390 : Loss 18657.6673\n",
      "Iteration # 1400 : Loss 18602.0614\n",
      "Iteration # 1410 : Loss 18546.6725\n",
      "Iteration # 1420 : Loss 18491.4997\n",
      "Iteration # 1430 : Loss 18436.5421\n",
      "Iteration # 1440 : Loss 18381.7990\n",
      "Iteration # 1450 : Loss 18327.2694\n",
      "Iteration # 1460 : Loss 18272.9525\n",
      "Iteration # 1470 : Loss 18218.8474\n",
      "Iteration # 1480 : Loss 18164.9534\n",
      "Iteration # 1490 : Loss 18111.2695\n",
      "Iteration # 1500 : Loss 18057.7950\n",
      "Iteration # 1510 : Loss 18004.5290\n",
      "Iteration # 1520 : Loss 17951.4706\n",
      "Iteration # 1530 : Loss 17898.6191\n",
      "Iteration # 1540 : Loss 17845.9736\n",
      "Iteration # 1550 : Loss 17793.5333\n",
      "Iteration # 1560 : Loss 17741.2974\n",
      "Iteration # 1570 : Loss 17689.2651\n",
      "Iteration # 1580 : Loss 17637.4355\n",
      "Iteration # 1590 : Loss 17585.8079\n",
      "Iteration # 1600 : Loss 17534.3815\n",
      "Iteration # 1610 : Loss 17483.1554\n",
      "Iteration # 1620 : Loss 17432.1288\n",
      "Iteration # 1630 : Loss 17381.3011\n",
      "Iteration # 1640 : Loss 17330.6713\n",
      "Iteration # 1650 : Loss 17280.2387\n",
      "Iteration # 1660 : Loss 17230.0025\n",
      "Iteration # 1670 : Loss 17179.9619\n",
      "Iteration # 1680 : Loss 17130.1162\n",
      "Iteration # 1690 : Loss 17080.4646\n",
      "Iteration # 1700 : Loss 17031.0062\n",
      "Iteration # 1710 : Loss 16981.7404\n",
      "Iteration # 1720 : Loss 16932.6663\n",
      "Iteration # 1730 : Loss 16883.7833\n",
      "Iteration # 1740 : Loss 16835.0905\n",
      "Iteration # 1750 : Loss 16786.5871\n",
      "Iteration # 1760 : Loss 16738.2726\n",
      "Iteration # 1770 : Loss 16690.1460\n",
      "Iteration # 1780 : Loss 16642.2066\n",
      "Iteration # 1790 : Loss 16594.4537\n",
      "Iteration # 1800 : Loss 16546.8866\n",
      "Iteration # 1810 : Loss 16499.5045\n",
      "Iteration # 1820 : Loss 16452.3067\n",
      "Iteration # 1830 : Loss 16405.2924\n",
      "Iteration # 1840 : Loss 16358.4609\n",
      "Iteration # 1850 : Loss 16311.8115\n",
      "Iteration # 1860 : Loss 16265.3435\n",
      "Iteration # 1870 : Loss 16219.0562\n",
      "Iteration # 1880 : Loss 16172.9487\n",
      "Iteration # 1890 : Loss 16127.0205\n",
      "Iteration # 1900 : Loss 16081.2707\n",
      "Iteration # 1910 : Loss 16035.6988\n",
      "Iteration # 1920 : Loss 15990.3039\n",
      "Iteration # 1930 : Loss 15945.0855\n",
      "Iteration # 1940 : Loss 15900.0426\n",
      "Iteration # 1950 : Loss 15855.1748\n",
      "Iteration # 1960 : Loss 15810.4813\n",
      "Iteration # 1970 : Loss 15765.9613\n",
      "Iteration # 1980 : Loss 15721.6143\n",
      "Iteration # 1990 : Loss 15677.4395\n",
      "Iteration # 2000 : Loss 15633.4362\n",
      "Iteration # 2010 : Loss 15589.6037\n",
      "Iteration # 2020 : Loss 15545.9414\n",
      "Iteration # 2030 : Loss 15502.4487\n",
      "Iteration # 2040 : Loss 15459.1247\n",
      "Iteration # 2050 : Loss 15415.9689\n",
      "Iteration # 2060 : Loss 15372.9806\n",
      "Iteration # 2070 : Loss 15330.1590\n",
      "Iteration # 2080 : Loss 15287.5037\n",
      "Iteration # 2090 : Loss 15245.0138\n",
      "Iteration # 2100 : Loss 15202.6888\n",
      "Iteration # 2110 : Loss 15160.5280\n",
      "Iteration # 2120 : Loss 15118.5306\n",
      "Iteration # 2130 : Loss 15076.6962\n",
      "Iteration # 2140 : Loss 15035.0240\n",
      "Iteration # 2150 : Loss 14993.5134\n",
      "Iteration # 2160 : Loss 14952.1637\n",
      "Iteration # 2170 : Loss 14910.9744\n",
      "Iteration # 2180 : Loss 14869.9447\n",
      "Iteration # 2190 : Loss 14829.0740\n",
      "Iteration # 2200 : Loss 14788.3618\n",
      "Iteration # 2210 : Loss 14747.8073\n",
      "Iteration # 2220 : Loss 14707.4100\n",
      "Iteration # 2230 : Loss 14667.1692\n",
      "Iteration # 2240 : Loss 14627.0843\n",
      "Iteration # 2250 : Loss 14587.1547\n",
      "Iteration # 2260 : Loss 14547.3797\n",
      "Iteration # 2270 : Loss 14507.7588\n",
      "Iteration # 2280 : Loss 14468.2914\n",
      "Iteration # 2290 : Loss 14428.9768\n",
      "Iteration # 2300 : Loss 14389.8144\n",
      "Iteration # 2310 : Loss 14350.8036\n",
      "Iteration # 2320 : Loss 14311.9438\n",
      "Iteration # 2330 : Loss 14273.2345\n",
      "Iteration # 2340 : Loss 14234.6750\n",
      "Iteration # 2350 : Loss 14196.2647\n",
      "Iteration # 2360 : Loss 14158.0031\n",
      "Iteration # 2370 : Loss 14119.8895\n",
      "Iteration # 2380 : Loss 14081.9233\n",
      "Iteration # 2390 : Loss 14044.1040\n",
      "Iteration # 2400 : Loss 14006.4310\n",
      "Iteration # 2410 : Loss 13968.9038\n",
      "Iteration # 2420 : Loss 13931.5216\n",
      "Iteration # 2430 : Loss 13894.2840\n",
      "Iteration # 2440 : Loss 13857.1904\n",
      "Iteration # 2450 : Loss 13820.2402\n",
      "Iteration # 2460 : Loss 13783.4328\n",
      "Iteration # 2470 : Loss 13746.7677\n",
      "Iteration # 2480 : Loss 13710.2443\n",
      "Iteration # 2490 : Loss 13673.8620\n",
      "Iteration # 2500 : Loss 13637.6203\n",
      "Iteration # 2510 : Loss 13601.5187\n",
      "Iteration # 2520 : Loss 13565.5565\n",
      "Iteration # 2530 : Loss 13529.7332\n",
      "Iteration # 2540 : Loss 13494.0483\n",
      "Iteration # 2550 : Loss 13458.5013\n",
      "Iteration # 2560 : Loss 13423.0914\n",
      "Iteration # 2570 : Loss 13387.8184\n",
      "Iteration # 2580 : Loss 13352.6815\n",
      "Iteration # 2590 : Loss 13317.6802\n",
      "Iteration # 2600 : Loss 13282.8140\n",
      "Iteration # 2610 : Loss 13248.0824\n",
      "Iteration # 2620 : Loss 13213.4849\n",
      "Iteration # 2630 : Loss 13179.0208\n",
      "Iteration # 2640 : Loss 13144.6897\n",
      "Iteration # 2650 : Loss 13110.4911\n",
      "Iteration # 2660 : Loss 13076.4243\n",
      "Iteration # 2670 : Loss 13042.4890\n",
      "Iteration # 2680 : Loss 13008.6845\n",
      "Iteration # 2690 : Loss 12975.0104\n",
      "Iteration # 2700 : Loss 12941.4662\n",
      "Iteration # 2710 : Loss 12908.0512\n",
      "Iteration # 2720 : Loss 12874.7651\n",
      "Iteration # 2730 : Loss 12841.6072\n",
      "Iteration # 2740 : Loss 12808.5772\n",
      "Iteration # 2750 : Loss 12775.6744\n",
      "Iteration # 2760 : Loss 12742.8984\n",
      "Iteration # 2770 : Loss 12710.2487\n",
      "Iteration # 2780 : Loss 12677.7248\n",
      "Iteration # 2790 : Loss 12645.3261\n",
      "Iteration # 2800 : Loss 12613.0522\n",
      "Iteration # 2810 : Loss 12580.9026\n",
      "Iteration # 2820 : Loss 12548.8768\n",
      "Iteration # 2830 : Loss 12516.9743\n",
      "Iteration # 2840 : Loss 12485.1946\n",
      "Iteration # 2850 : Loss 12453.5372\n",
      "Iteration # 2860 : Loss 12422.0016\n",
      "Iteration # 2870 : Loss 12390.5874\n",
      "Iteration # 2880 : Loss 12359.2941\n",
      "Iteration # 2890 : Loss 12328.1212\n",
      "Iteration # 2900 : Loss 12297.0682\n",
      "Iteration # 2910 : Loss 12266.1346\n",
      "Iteration # 2920 : Loss 12235.3200\n",
      "Iteration # 2930 : Loss 12204.6239\n",
      "Iteration # 2940 : Loss 12174.0459\n",
      "Iteration # 2950 : Loss 12143.5853\n",
      "Iteration # 2960 : Loss 12113.2419\n",
      "Iteration # 2970 : Loss 12083.0151\n",
      "Iteration # 2980 : Loss 12052.9045\n",
      "Iteration # 2990 : Loss 12022.9096\n",
      "Iteration # 3000 : Loss 11993.0299\n",
      "Iteration # 3010 : Loss 11963.2650\n",
      "Iteration # 3020 : Loss 11933.6145\n",
      "Iteration # 3030 : Loss 11904.0778\n",
      "Iteration # 3040 : Loss 11874.6546\n",
      "Iteration # 3050 : Loss 11845.3443\n",
      "Iteration # 3060 : Loss 11816.1466\n",
      "Iteration # 3070 : Loss 11787.0610\n",
      "Iteration # 3080 : Loss 11758.0870\n",
      "Iteration # 3090 : Loss 11729.2242\n",
      "Iteration # 3100 : Loss 11700.4721\n",
      "Iteration # 3110 : Loss 11671.8304\n",
      "Iteration # 3120 : Loss 11643.2986\n",
      "Iteration # 3130 : Loss 11614.8762\n",
      "Iteration # 3140 : Loss 11586.5629\n",
      "Iteration # 3150 : Loss 11558.3581\n",
      "Iteration # 3160 : Loss 11530.2615\n",
      "Iteration # 3170 : Loss 11502.2726\n",
      "Iteration # 3180 : Loss 11474.3909\n",
      "Iteration # 3190 : Loss 11446.6162\n",
      "Iteration # 3200 : Loss 11418.9479\n",
      "Iteration # 3210 : Loss 11391.3857\n",
      "Iteration # 3220 : Loss 11363.9290\n",
      "Iteration # 3230 : Loss 11336.5775\n",
      "Iteration # 3240 : Loss 11309.3308\n",
      "Iteration # 3250 : Loss 11282.1885\n",
      "Iteration # 3260 : Loss 11255.1501\n",
      "Iteration # 3270 : Loss 11228.2153\n",
      "Iteration # 3280 : Loss 11201.3835\n",
      "Iteration # 3290 : Loss 11174.6545\n",
      "Iteration # 3300 : Loss 11148.0278\n",
      "Iteration # 3310 : Loss 11121.5030\n",
      "Iteration # 3320 : Loss 11095.0796\n",
      "Iteration # 3330 : Loss 11068.7574\n",
      "Iteration # 3340 : Loss 11042.5359\n",
      "Iteration # 3350 : Loss 11016.4146\n",
      "Iteration # 3360 : Loss 10990.3933\n",
      "Iteration # 3370 : Loss 10964.4715\n",
      "Iteration # 3380 : Loss 10938.6488\n",
      "Iteration # 3390 : Loss 10912.9248\n",
      "Iteration # 3400 : Loss 10887.2991\n",
      "Iteration # 3410 : Loss 10861.7713\n",
      "Iteration # 3420 : Loss 10836.3412\n",
      "Iteration # 3430 : Loss 10811.0081\n",
      "Iteration # 3440 : Loss 10785.7719\n",
      "Iteration # 3450 : Loss 10760.6320\n",
      "Iteration # 3460 : Loss 10735.5882\n",
      "Iteration # 3470 : Loss 10710.6400\n",
      "Iteration # 3480 : Loss 10685.7870\n",
      "Iteration # 3490 : Loss 10661.0290\n",
      "Iteration # 3500 : Loss 10636.3654\n",
      "Iteration # 3510 : Loss 10611.7960\n",
      "Iteration # 3520 : Loss 10587.3203\n",
      "Iteration # 3530 : Loss 10562.9380\n",
      "Iteration # 3540 : Loss 10538.6487\n",
      "Iteration # 3550 : Loss 10514.4521\n",
      "Iteration # 3560 : Loss 10490.3477\n",
      "Iteration # 3570 : Loss 10466.3353\n",
      "Iteration # 3580 : Loss 10442.4144\n",
      "Iteration # 3590 : Loss 10418.5847\n",
      "Iteration # 3600 : Loss 10394.8458\n",
      "Iteration # 3610 : Loss 10371.1975\n",
      "Iteration # 3620 : Loss 10347.6392\n",
      "Iteration # 3630 : Loss 10324.1706\n",
      "Iteration # 3640 : Loss 10300.7915\n",
      "Iteration # 3650 : Loss 10277.5014\n",
      "Iteration # 3660 : Loss 10254.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 3670 : Loss 10231.1870\n",
      "Iteration # 3680 : Loss 10208.1619\n",
      "Iteration # 3690 : Loss 10185.2245\n",
      "Iteration # 3700 : Loss 10162.3744\n",
      "Iteration # 3710 : Loss 10139.6113\n",
      "Iteration # 3720 : Loss 10116.9347\n",
      "Iteration # 3730 : Loss 10094.3444\n",
      "Iteration # 3740 : Loss 10071.8401\n",
      "Iteration # 3750 : Loss 10049.4213\n",
      "Iteration # 3760 : Loss 10027.0878\n",
      "Iteration # 3770 : Loss 10004.8392\n",
      "Iteration # 3780 : Loss 9982.6752\n",
      "Iteration # 3790 : Loss 9960.5954\n",
      "Iteration # 3800 : Loss 9938.5995\n",
      "Iteration # 3810 : Loss 9916.6872\n",
      "Iteration # 3820 : Loss 9894.8581\n",
      "Iteration # 3830 : Loss 9873.1120\n",
      "Iteration # 3840 : Loss 9851.4484\n",
      "Iteration # 3850 : Loss 9829.8671\n",
      "Iteration # 3860 : Loss 9808.3678\n",
      "Iteration # 3870 : Loss 9786.9500\n",
      "Iteration # 3880 : Loss 9765.6136\n",
      "Iteration # 3890 : Loss 9744.3581\n",
      "Iteration # 3900 : Loss 9723.1833\n",
      "Iteration # 3910 : Loss 9702.0887\n",
      "Iteration # 3920 : Loss 9681.0742\n",
      "Iteration # 3930 : Loss 9660.1394\n",
      "Iteration # 3940 : Loss 9639.2840\n",
      "Iteration # 3950 : Loss 9618.5077\n",
      "Iteration # 3960 : Loss 9597.8101\n",
      "Iteration # 3970 : Loss 9577.1909\n",
      "Iteration # 3980 : Loss 9556.6499\n",
      "Iteration # 3990 : Loss 9536.1867\n",
      "Iteration # 4000 : Loss 9515.8010\n",
      "Iteration # 4010 : Loss 9495.4926\n",
      "Iteration # 4020 : Loss 9475.2610\n",
      "Iteration # 4030 : Loss 9455.1061\n",
      "Iteration # 4040 : Loss 9435.0274\n",
      "Iteration # 4050 : Loss 9415.0248\n",
      "Iteration # 4060 : Loss 9395.0978\n",
      "Iteration # 4070 : Loss 9375.2462\n",
      "Iteration # 4080 : Loss 9355.4698\n",
      "Iteration # 4090 : Loss 9335.7681\n",
      "Iteration # 4100 : Loss 9316.1410\n",
      "Iteration # 4110 : Loss 9296.5881\n",
      "Iteration # 4120 : Loss 9277.1091\n",
      "Iteration # 4130 : Loss 9257.7037\n",
      "Iteration # 4140 : Loss 9238.3716\n",
      "Iteration # 4150 : Loss 9219.1127\n",
      "Iteration # 4160 : Loss 9199.9264\n",
      "Iteration # 4170 : Loss 9180.8127\n",
      "Iteration # 4180 : Loss 9161.7711\n",
      "Iteration # 4190 : Loss 9142.8015\n",
      "Iteration # 4200 : Loss 9123.9034\n",
      "Iteration # 4210 : Loss 9105.0767\n",
      "Iteration # 4220 : Loss 9086.3211\n",
      "Iteration # 4230 : Loss 9067.6362\n",
      "Iteration # 4240 : Loss 9049.0218\n",
      "Iteration # 4250 : Loss 9030.4776\n",
      "Iteration # 4260 : Loss 9012.0034\n",
      "Iteration # 4270 : Loss 8993.5989\n",
      "Iteration # 4280 : Loss 8975.2637\n",
      "Iteration # 4290 : Loss 8956.9976\n",
      "Iteration # 4300 : Loss 8938.8004\n",
      "Iteration # 4310 : Loss 8920.6718\n",
      "Iteration # 4320 : Loss 8902.6114\n",
      "Iteration # 4330 : Loss 8884.6191\n",
      "Iteration # 4340 : Loss 8866.6945\n",
      "Iteration # 4350 : Loss 8848.8375\n",
      "Iteration # 4360 : Loss 8831.0476\n",
      "Iteration # 4370 : Loss 8813.3248\n",
      "Iteration # 4380 : Loss 8795.6686\n",
      "Iteration # 4390 : Loss 8778.0789\n",
      "Iteration # 4400 : Loss 8760.5553\n",
      "Iteration # 4410 : Loss 8743.0976\n",
      "Iteration # 4420 : Loss 8725.7056\n",
      "Iteration # 4430 : Loss 8708.3790\n",
      "Iteration # 4440 : Loss 8691.1176\n",
      "Iteration # 4450 : Loss 8673.9210\n",
      "Iteration # 4460 : Loss 8656.7890\n",
      "Iteration # 4470 : Loss 8639.7214\n",
      "Iteration # 4480 : Loss 8622.7179\n",
      "Iteration # 4490 : Loss 8605.7783\n",
      "Iteration # 4500 : Loss 8588.9023\n",
      "Iteration # 4510 : Loss 8572.0896\n",
      "Iteration # 4520 : Loss 8555.3401\n",
      "Iteration # 4530 : Loss 8538.6534\n",
      "Iteration # 4540 : Loss 8522.0293\n",
      "Iteration # 4550 : Loss 8505.4676\n",
      "Iteration # 4560 : Loss 8488.9680\n",
      "Iteration # 4570 : Loss 8472.5303\n",
      "Iteration # 4580 : Loss 8456.1542\n",
      "Iteration # 4590 : Loss 8439.8395\n",
      "Iteration # 4600 : Loss 8423.5859\n",
      "Iteration # 4610 : Loss 8407.3933\n",
      "Iteration # 4620 : Loss 8391.2613\n",
      "Iteration # 4630 : Loss 8375.1897\n",
      "Iteration # 4640 : Loss 8359.1784\n",
      "Iteration # 4650 : Loss 8343.2270\n",
      "Iteration # 4660 : Loss 8327.3353\n",
      "Iteration # 4670 : Loss 8311.5030\n",
      "Iteration # 4680 : Loss 8295.7300\n",
      "Iteration # 4690 : Loss 8280.0161\n",
      "Iteration # 4700 : Loss 8264.3609\n",
      "Iteration # 4710 : Loss 8248.7642\n",
      "Iteration # 4720 : Loss 8233.2259\n",
      "Iteration # 4730 : Loss 8217.7457\n",
      "Iteration # 4740 : Loss 8202.3233\n",
      "Iteration # 4750 : Loss 8186.9585\n",
      "Iteration # 4760 : Loss 8171.6512\n",
      "Iteration # 4770 : Loss 8156.4010\n",
      "Iteration # 4780 : Loss 8141.2078\n",
      "Iteration # 4790 : Loss 8126.0713\n",
      "Iteration # 4800 : Loss 8110.9913\n",
      "Iteration # 4810 : Loss 8095.9676\n",
      "Iteration # 4820 : Loss 8081.0000\n",
      "Iteration # 4830 : Loss 8066.0882\n",
      "Iteration # 4840 : Loss 8051.2320\n",
      "Iteration # 4850 : Loss 8036.4312\n",
      "Iteration # 4860 : Loss 8021.6856\n",
      "Iteration # 4870 : Loss 8006.9950\n",
      "Iteration # 4880 : Loss 7992.3591\n",
      "Iteration # 4890 : Loss 7977.7777\n",
      "Iteration # 4900 : Loss 7963.2507\n",
      "Iteration # 4910 : Loss 7948.7778\n",
      "Iteration # 4920 : Loss 7934.3587\n",
      "Iteration # 4930 : Loss 7919.9934\n",
      "Iteration # 4940 : Loss 7905.6815\n",
      "Iteration # 4950 : Loss 7891.4229\n",
      "Iteration # 4960 : Loss 7877.2173\n",
      "Iteration # 4970 : Loss 7863.0646\n",
      "Iteration # 4980 : Loss 7848.9645\n",
      "Iteration # 4990 : Loss 7834.9168\n",
      "Iteration # 5000 : Loss 7820.9213\n",
      "Iteration # 5010 : Loss 7806.9779\n",
      "Iteration # 5020 : Loss 7793.0863\n",
      "Iteration # 5030 : Loss 7779.2462\n",
      "Iteration # 5040 : Loss 7765.4576\n",
      "Iteration # 5050 : Loss 7751.7202\n",
      "Iteration # 5060 : Loss 7738.0337\n",
      "Iteration # 5070 : Loss 7724.3981\n",
      "Iteration # 5080 : Loss 7710.8131\n",
      "Iteration # 5090 : Loss 7697.2785\n",
      "Iteration # 5100 : Loss 7683.7940\n",
      "Iteration # 5110 : Loss 7670.3596\n",
      "Iteration # 5120 : Loss 7656.9750\n",
      "Iteration # 5130 : Loss 7643.6400\n",
      "Iteration # 5140 : Loss 7630.3544\n",
      "Iteration # 5150 : Loss 7617.1180\n",
      "Iteration # 5160 : Loss 7603.9307\n",
      "Iteration # 5170 : Loss 7590.7922\n",
      "Iteration # 5180 : Loss 7577.7024\n",
      "Iteration # 5190 : Loss 7564.6610\n",
      "Iteration # 5200 : Loss 7551.6678\n",
      "Iteration # 5210 : Loss 7538.7228\n",
      "Iteration # 5220 : Loss 7525.8256\n",
      "Iteration # 5230 : Loss 7512.9761\n",
      "Iteration # 5240 : Loss 7500.1741\n",
      "Iteration # 5250 : Loss 7487.4195\n",
      "Iteration # 5260 : Loss 7474.7120\n",
      "Iteration # 5270 : Loss 7462.0514\n",
      "Iteration # 5280 : Loss 7449.4376\n",
      "Iteration # 5290 : Loss 7436.8704\n",
      "Iteration # 5300 : Loss 7424.3496\n",
      "Iteration # 5310 : Loss 7411.8750\n",
      "Iteration # 5320 : Loss 7399.4465\n",
      "Iteration # 5330 : Loss 7387.0638\n",
      "Iteration # 5340 : Loss 7374.7268\n",
      "Iteration # 5350 : Loss 7362.4352\n",
      "Iteration # 5360 : Loss 7350.1890\n",
      "Iteration # 5370 : Loss 7337.9880\n",
      "Iteration # 5380 : Loss 7325.8318\n",
      "Iteration # 5390 : Loss 7313.7205\n",
      "Iteration # 5400 : Loss 7301.6538\n",
      "Iteration # 5410 : Loss 7289.6316\n",
      "Iteration # 5420 : Loss 7277.6535\n",
      "Iteration # 5430 : Loss 7265.7196\n",
      "Iteration # 5440 : Loss 7253.8296\n",
      "Iteration # 5450 : Loss 7241.9833\n",
      "Iteration # 5460 : Loss 7230.1806\n",
      "Iteration # 5470 : Loss 7218.4214\n",
      "Iteration # 5480 : Loss 7206.7053\n",
      "Iteration # 5490 : Loss 7195.0323\n",
      "Iteration # 5500 : Loss 7183.4022\n",
      "Iteration # 5510 : Loss 7171.8148\n",
      "Iteration # 5520 : Loss 7160.2699\n",
      "Iteration # 5530 : Loss 7148.7675\n",
      "Iteration # 5540 : Loss 7137.3072\n",
      "Iteration # 5550 : Loss 7125.8891\n",
      "Iteration # 5560 : Loss 7114.5128\n",
      "Iteration # 5570 : Loss 7103.1782\n",
      "Iteration # 5580 : Loss 7091.8852\n",
      "Iteration # 5590 : Loss 7080.6337\n",
      "Iteration # 5600 : Loss 7069.4233\n",
      "Iteration # 5610 : Loss 7058.2540\n",
      "Iteration # 5620 : Loss 7047.1257\n",
      "Iteration # 5630 : Loss 7036.0381\n",
      "Iteration # 5640 : Loss 7024.9911\n",
      "Iteration # 5650 : Loss 7013.9845\n",
      "Iteration # 5660 : Loss 7003.0182\n",
      "Iteration # 5670 : Loss 6992.0920\n",
      "Iteration # 5680 : Loss 6981.2058\n",
      "Iteration # 5690 : Loss 6970.3594\n",
      "Iteration # 5700 : Loss 6959.5526\n",
      "Iteration # 5710 : Loss 6948.7854\n",
      "Iteration # 5720 : Loss 6938.0575\n",
      "Iteration # 5730 : Loss 6927.3687\n",
      "Iteration # 5740 : Loss 6916.7190\n",
      "Iteration # 5750 : Loss 6906.1082\n",
      "Iteration # 5760 : Loss 6895.5361\n",
      "Iteration # 5770 : Loss 6885.0026\n",
      "Iteration # 5780 : Loss 6874.5075\n",
      "Iteration # 5790 : Loss 6864.0507\n",
      "Iteration # 5800 : Loss 6853.6320\n",
      "Iteration # 5810 : Loss 6843.2513\n",
      "Iteration # 5820 : Loss 6832.9084\n",
      "Iteration # 5830 : Loss 6822.6031\n",
      "Iteration # 5840 : Loss 6812.3355\n",
      "Iteration # 5850 : Loss 6802.1052\n",
      "Iteration # 5860 : Loss 6791.9121\n",
      "Iteration # 5870 : Loss 6781.7561\n",
      "Iteration # 5880 : Loss 6771.6371\n",
      "Iteration # 5890 : Loss 6761.5549\n",
      "Iteration # 5900 : Loss 6751.5093\n",
      "Iteration # 5910 : Loss 6741.5003\n",
      "Iteration # 5920 : Loss 6731.5276\n",
      "Iteration # 5930 : Loss 6721.5912\n",
      "Iteration # 5940 : Loss 6711.6908\n",
      "Iteration # 5950 : Loss 6701.8264\n",
      "Iteration # 5960 : Loss 6691.9978\n",
      "Iteration # 5970 : Loss 6682.2048\n",
      "Iteration # 5980 : Loss 6672.4474\n",
      "Iteration # 5990 : Loss 6662.7254\n",
      "Iteration # 6000 : Loss 6653.0386\n",
      "Iteration # 6010 : Loss 6643.3870\n",
      "Iteration # 6020 : Loss 6633.7703\n",
      "Iteration # 6030 : Loss 6624.1884\n",
      "Iteration # 6040 : Loss 6614.6413\n",
      "Iteration # 6050 : Loss 6605.1287\n",
      "Iteration # 6060 : Loss 6595.6505\n",
      "Iteration # 6070 : Loss 6586.2067\n",
      "Iteration # 6080 : Loss 6576.7970\n",
      "Iteration # 6090 : Loss 6567.4213\n",
      "Iteration # 6100 : Loss 6558.0795\n",
      "Iteration # 6110 : Loss 6548.7715\n",
      "Iteration # 6120 : Loss 6539.4972\n",
      "Iteration # 6130 : Loss 6530.2563\n",
      "Iteration # 6140 : Loss 6521.0488\n",
      "Iteration # 6150 : Loss 6511.8745\n",
      "Iteration # 6160 : Loss 6502.7333\n",
      "Iteration # 6170 : Loss 6493.6252\n",
      "Iteration # 6180 : Loss 6484.5498\n",
      "Iteration # 6190 : Loss 6475.5072\n",
      "Iteration # 6200 : Loss 6466.4972\n",
      "Iteration # 6210 : Loss 6457.5197\n",
      "Iteration # 6220 : Loss 6448.5745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 6230 : Loss 6439.6615\n",
      "Iteration # 6240 : Loss 6430.7806\n",
      "Iteration # 6250 : Loss 6421.9316\n",
      "Iteration # 6260 : Loss 6413.1145\n",
      "Iteration # 6270 : Loss 6404.3291\n",
      "Iteration # 6280 : Loss 6395.5754\n",
      "Iteration # 6290 : Loss 6386.8530\n",
      "Iteration # 6300 : Loss 6378.1620\n",
      "Iteration # 6310 : Loss 6369.5023\n",
      "Iteration # 6320 : Loss 6360.8736\n",
      "Iteration # 6330 : Loss 6352.2760\n",
      "Iteration # 6340 : Loss 6343.7091\n",
      "Iteration # 6350 : Loss 6335.1730\n",
      "Iteration # 6360 : Loss 6326.6676\n",
      "Iteration # 6370 : Loss 6318.1926\n",
      "Iteration # 6380 : Loss 6309.7480\n",
      "Iteration # 6390 : Loss 6301.3337\n",
      "Iteration # 6400 : Loss 6292.9495\n",
      "Iteration # 6410 : Loss 6284.5954\n",
      "Iteration # 6420 : Loss 6276.2711\n",
      "Iteration # 6430 : Loss 6267.9767\n",
      "Iteration # 6440 : Loss 6259.7119\n",
      "Iteration # 6450 : Loss 6251.4767\n",
      "Iteration # 6460 : Loss 6243.2709\n",
      "Iteration # 6470 : Loss 6235.0945\n",
      "Iteration # 6480 : Loss 6226.9472\n",
      "Iteration # 6490 : Loss 6218.8291\n",
      "Iteration # 6500 : Loss 6210.7400\n",
      "Iteration # 6510 : Loss 6202.6797\n",
      "Iteration # 6520 : Loss 6194.6482\n",
      "Iteration # 6530 : Loss 6186.6454\n",
      "Iteration # 6540 : Loss 6178.6711\n",
      "Iteration # 6550 : Loss 6170.7252\n",
      "Iteration # 6560 : Loss 6162.8077\n",
      "Iteration # 6570 : Loss 6154.9183\n",
      "Iteration # 6580 : Loss 6147.0571\n",
      "Iteration # 6590 : Loss 6139.2238\n",
      "Iteration # 6600 : Loss 6131.4185\n",
      "Iteration # 6610 : Loss 6123.6408\n",
      "Iteration # 6620 : Loss 6115.8909\n",
      "Iteration # 6630 : Loss 6108.1685\n",
      "Iteration # 6640 : Loss 6100.4736\n",
      "Iteration # 6650 : Loss 6092.8060\n",
      "Iteration # 6660 : Loss 6085.1656\n",
      "Iteration # 6670 : Loss 6077.5524\n",
      "Iteration # 6680 : Loss 6069.9662\n",
      "Iteration # 6690 : Loss 6062.4069\n",
      "Iteration # 6700 : Loss 6054.8744\n",
      "Iteration # 6710 : Loss 6047.3687\n",
      "Iteration # 6720 : Loss 6039.8895\n",
      "Iteration # 6730 : Loss 6032.4368\n",
      "Iteration # 6740 : Loss 6025.0106\n",
      "Iteration # 6750 : Loss 6017.6106\n",
      "Iteration # 6760 : Loss 6010.2369\n",
      "Iteration # 6770 : Loss 6002.8892\n",
      "Iteration # 6780 : Loss 5995.5675\n",
      "Iteration # 6790 : Loss 5988.2717\n",
      "Iteration # 6800 : Loss 5981.0017\n",
      "Iteration # 6810 : Loss 5973.7574\n",
      "Iteration # 6820 : Loss 5966.5387\n",
      "Iteration # 6830 : Loss 5959.3454\n",
      "Iteration # 6840 : Loss 5952.1776\n",
      "Iteration # 6850 : Loss 5945.0351\n",
      "Iteration # 6860 : Loss 5937.9177\n",
      "Iteration # 6870 : Loss 5930.8254\n",
      "Iteration # 6880 : Loss 5923.7582\n",
      "Iteration # 6890 : Loss 5916.7158\n",
      "Iteration # 6900 : Loss 5909.6983\n",
      "Iteration # 6910 : Loss 5902.7054\n",
      "Iteration # 6920 : Loss 5895.7372\n",
      "Iteration # 6930 : Loss 5888.7935\n",
      "Iteration # 6940 : Loss 5881.8742\n",
      "Iteration # 6950 : Loss 5874.9793\n",
      "Iteration # 6960 : Loss 5868.1085\n",
      "Iteration # 6970 : Loss 5861.2619\n",
      "Iteration # 6980 : Loss 5854.4394\n",
      "Iteration # 6990 : Loss 5847.6408\n",
      "Iteration # 7000 : Loss 5840.8661\n",
      "Iteration # 7010 : Loss 5834.1151\n",
      "Iteration # 7020 : Loss 5827.3878\n",
      "Iteration # 7030 : Loss 5820.6841\n",
      "Iteration # 7040 : Loss 5814.0039\n",
      "Iteration # 7050 : Loss 5807.3471\n",
      "Iteration # 7060 : Loss 5800.7136\n",
      "Iteration # 7070 : Loss 5794.1033\n",
      "Iteration # 7080 : Loss 5787.5162\n",
      "Iteration # 7090 : Loss 5780.9521\n",
      "Iteration # 7100 : Loss 5774.4109\n",
      "Iteration # 7110 : Loss 5767.8926\n",
      "Iteration # 7120 : Loss 5761.3971\n",
      "Iteration # 7130 : Loss 5754.9242\n",
      "Iteration # 7140 : Loss 5748.4740\n",
      "Iteration # 7150 : Loss 5742.0462\n",
      "Iteration # 7160 : Loss 5735.6409\n",
      "Iteration # 7170 : Loss 5729.2579\n",
      "Iteration # 7180 : Loss 5722.8972\n",
      "Iteration # 7190 : Loss 5716.5586\n",
      "Iteration # 7200 : Loss 5710.2422\n",
      "Iteration # 7210 : Loss 5703.9477\n",
      "Iteration # 7220 : Loss 5697.6751\n",
      "Iteration # 7230 : Loss 5691.4243\n",
      "Iteration # 7240 : Loss 5685.1953\n",
      "Iteration # 7250 : Loss 5678.9879\n",
      "Iteration # 7260 : Loss 5672.8021\n",
      "Iteration # 7270 : Loss 5666.6377\n",
      "Iteration # 7280 : Loss 5660.4948\n",
      "Iteration # 7290 : Loss 5654.3732\n",
      "Iteration # 7300 : Loss 5648.2728\n",
      "Iteration # 7310 : Loss 5642.1936\n",
      "Iteration # 7320 : Loss 5636.1355\n",
      "Iteration # 7330 : Loss 5630.0984\n",
      "Iteration # 7340 : Loss 5624.0821\n",
      "Iteration # 7350 : Loss 5618.0867\n",
      "Iteration # 7360 : Loss 5612.1121\n",
      "Iteration # 7370 : Loss 5606.1581\n",
      "Iteration # 7380 : Loss 5600.2247\n",
      "Iteration # 7390 : Loss 5594.3118\n",
      "Iteration # 7400 : Loss 5588.4193\n",
      "Iteration # 7410 : Loss 5582.5472\n",
      "Iteration # 7420 : Loss 5576.6954\n",
      "Iteration # 7430 : Loss 5570.8638\n",
      "Iteration # 7440 : Loss 5565.0523\n",
      "Iteration # 7450 : Loss 5559.2608\n",
      "Iteration # 7460 : Loss 5553.4893\n",
      "Iteration # 7470 : Loss 5547.7376\n",
      "Iteration # 7480 : Loss 5542.0058\n",
      "Iteration # 7490 : Loss 5536.2937\n",
      "Iteration # 7500 : Loss 5530.6013\n",
      "Iteration # 7510 : Loss 5524.9284\n",
      "Iteration # 7520 : Loss 5519.2751\n",
      "Iteration # 7530 : Loss 5513.6411\n",
      "Iteration # 7540 : Loss 5508.0266\n",
      "Iteration # 7550 : Loss 5502.4313\n",
      "Iteration # 7560 : Loss 5496.8552\n",
      "Iteration # 7570 : Loss 5491.2982\n",
      "Iteration # 7580 : Loss 5485.7603\n",
      "Iteration # 7590 : Loss 5480.2414\n",
      "Iteration # 7600 : Loss 5474.7414\n",
      "Iteration # 7610 : Loss 5469.2602\n",
      "Iteration # 7620 : Loss 5463.7978\n",
      "Iteration # 7630 : Loss 5458.3541\n",
      "Iteration # 7640 : Loss 5452.9290\n",
      "Iteration # 7650 : Loss 5447.5225\n",
      "Iteration # 7660 : Loss 5442.1344\n",
      "Iteration # 7670 : Loss 5436.7648\n",
      "Iteration # 7680 : Loss 5431.4135\n",
      "Iteration # 7690 : Loss 5426.0804\n",
      "Iteration # 7700 : Loss 5420.7655\n",
      "Iteration # 7710 : Loss 5415.4688\n",
      "Iteration # 7720 : Loss 5410.1901\n",
      "Iteration # 7730 : Loss 5404.9294\n",
      "Iteration # 7740 : Loss 5399.6866\n",
      "Iteration # 7750 : Loss 5394.4617\n",
      "Iteration # 7760 : Loss 5389.2545\n",
      "Iteration # 7770 : Loss 5384.0651\n",
      "Iteration # 7780 : Loss 5378.8933\n",
      "Iteration # 7790 : Loss 5373.7390\n",
      "Iteration # 7800 : Loss 5368.6023\n",
      "Iteration # 7810 : Loss 5363.4830\n",
      "Iteration # 7820 : Loss 5358.3811\n",
      "Iteration # 7830 : Loss 5353.2965\n",
      "Iteration # 7840 : Loss 5348.2291\n",
      "Iteration # 7850 : Loss 5343.1789\n",
      "Iteration # 7860 : Loss 5338.1458\n",
      "Iteration # 7870 : Loss 5333.1297\n",
      "Iteration # 7880 : Loss 5328.1306\n",
      "Iteration # 7890 : Loss 5323.1485\n",
      "Iteration # 7900 : Loss 5318.1831\n",
      "Iteration # 7910 : Loss 5313.2346\n",
      "Iteration # 7920 : Loss 5308.3027\n",
      "Iteration # 7930 : Loss 5303.3876\n",
      "Iteration # 7940 : Loss 5298.4890\n",
      "Iteration # 7950 : Loss 5293.6069\n",
      "Iteration # 7960 : Loss 5288.7413\n",
      "Iteration # 7970 : Loss 5283.8921\n",
      "Iteration # 7980 : Loss 5279.0592\n",
      "Iteration # 7990 : Loss 5274.2426\n",
      "Iteration # 8000 : Loss 5269.4422\n",
      "Iteration # 8010 : Loss 5264.6579\n",
      "Iteration # 8020 : Loss 5259.8898\n",
      "Iteration # 8030 : Loss 5255.1376\n",
      "Iteration # 8040 : Loss 5250.4014\n",
      "Iteration # 8050 : Loss 5245.6811\n",
      "Iteration # 8060 : Loss 5240.9767\n",
      "Iteration # 8070 : Loss 5236.2880\n",
      "Iteration # 8080 : Loss 5231.6151\n",
      "Iteration # 8090 : Loss 5226.9578\n",
      "Iteration # 8100 : Loss 5222.3161\n",
      "Iteration # 8110 : Loss 5217.6899\n",
      "Iteration # 8120 : Loss 5213.0793\n",
      "Iteration # 8130 : Loss 5208.4840\n",
      "Iteration # 8140 : Loss 5203.9041\n",
      "Iteration # 8150 : Loss 5199.3395\n",
      "Iteration # 8160 : Loss 5194.7902\n",
      "Iteration # 8170 : Loss 5190.2560\n",
      "Iteration # 8180 : Loss 5185.7370\n",
      "Iteration # 8190 : Loss 5181.2330\n",
      "Iteration # 8200 : Loss 5176.7440\n",
      "Iteration # 8210 : Loss 5172.2700\n",
      "Iteration # 8220 : Loss 5167.8109\n",
      "Iteration # 8230 : Loss 5163.3666\n",
      "Iteration # 8240 : Loss 5158.9371\n",
      "Iteration # 8250 : Loss 5154.5224\n",
      "Iteration # 8260 : Loss 5150.1223\n",
      "Iteration # 8270 : Loss 5145.7368\n",
      "Iteration # 8280 : Loss 5141.3659\n",
      "Iteration # 8290 : Loss 5137.0095\n",
      "Iteration # 8300 : Loss 5132.6675\n",
      "Iteration # 8310 : Loss 5128.3399\n",
      "Iteration # 8320 : Loss 5124.0266\n",
      "Iteration # 8330 : Loss 5119.7277\n",
      "Iteration # 8340 : Loss 5115.4429\n",
      "Iteration # 8350 : Loss 5111.1724\n",
      "Iteration # 8360 : Loss 5106.9159\n",
      "Iteration # 8370 : Loss 5102.6735\n",
      "Iteration # 8380 : Loss 5098.4452\n",
      "Iteration # 8390 : Loss 5094.2307\n",
      "Iteration # 8400 : Loss 5090.0302\n",
      "Iteration # 8410 : Loss 5085.8436\n",
      "Iteration # 8420 : Loss 5081.6707\n",
      "Iteration # 8430 : Loss 5077.5116\n",
      "Iteration # 8440 : Loss 5073.3662\n",
      "Iteration # 8450 : Loss 5069.2344\n",
      "Iteration # 8460 : Loss 5065.1162\n",
      "Iteration # 8470 : Loss 5061.0115\n",
      "Iteration # 8480 : Loss 5056.9204\n",
      "Iteration # 8490 : Loss 5052.8426\n",
      "Iteration # 8500 : Loss 5048.7783\n",
      "Iteration # 8510 : Loss 5044.7272\n",
      "Iteration # 8520 : Loss 5040.6895\n",
      "Iteration # 8530 : Loss 5036.6650\n",
      "Iteration # 8540 : Loss 5032.6536\n",
      "Iteration # 8550 : Loss 5028.6554\n",
      "Iteration # 8560 : Loss 5024.6703\n",
      "Iteration # 8570 : Loss 5020.6982\n",
      "Iteration # 8580 : Loss 5016.7391\n",
      "Iteration # 8590 : Loss 5012.7929\n",
      "Iteration # 8600 : Loss 5008.8596\n",
      "Iteration # 8610 : Loss 5004.9392\n",
      "Iteration # 8620 : Loss 5001.0315\n",
      "Iteration # 8630 : Loss 4997.1365\n",
      "Iteration # 8640 : Loss 4993.2543\n",
      "Iteration # 8650 : Loss 4989.3846\n",
      "Iteration # 8660 : Loss 4985.5276\n",
      "Iteration # 8670 : Loss 4981.6831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 8680 : Loss 4977.8511\n",
      "Iteration # 8690 : Loss 4974.0316\n",
      "Iteration # 8700 : Loss 4970.2244\n",
      "Iteration # 8710 : Loss 4966.4296\n",
      "Iteration # 8720 : Loss 4962.6471\n",
      "Iteration # 8730 : Loss 4958.8769\n",
      "Iteration # 8740 : Loss 4955.1189\n",
      "Iteration # 8750 : Loss 4951.3730\n",
      "Iteration # 8760 : Loss 4947.6393\n",
      "Iteration # 8770 : Loss 4943.9176\n",
      "Iteration # 8780 : Loss 4940.2080\n",
      "Iteration # 8790 : Loss 4936.5103\n",
      "Iteration # 8800 : Loss 4932.8246\n",
      "Iteration # 8810 : Loss 4929.1508\n",
      "Iteration # 8820 : Loss 4925.4888\n",
      "Iteration # 8830 : Loss 4921.8387\n",
      "Iteration # 8840 : Loss 4918.2003\n",
      "Iteration # 8850 : Loss 4914.5736\n",
      "Iteration # 8860 : Loss 4910.9585\n",
      "Iteration # 8870 : Loss 4907.3551\n",
      "Iteration # 8880 : Loss 4903.7633\n",
      "Iteration # 8890 : Loss 4900.1830\n",
      "Iteration # 8900 : Loss 4896.6142\n",
      "Iteration # 8910 : Loss 4893.0568\n",
      "Iteration # 8920 : Loss 4889.5109\n",
      "Iteration # 8930 : Loss 4885.9763\n",
      "Iteration # 8940 : Loss 4882.4530\n",
      "Iteration # 8950 : Loss 4878.9410\n",
      "Iteration # 8960 : Loss 4875.4403\n",
      "Iteration # 8970 : Loss 4871.9507\n",
      "Iteration # 8980 : Loss 4868.4723\n",
      "Iteration # 8990 : Loss 4865.0050\n",
      "Iteration # 9000 : Loss 4861.5487\n",
      "Iteration # 9010 : Loss 4858.1035\n",
      "Iteration # 9020 : Loss 4854.6693\n",
      "Iteration # 9030 : Loss 4851.2460\n",
      "Iteration # 9040 : Loss 4847.8336\n",
      "Iteration # 9050 : Loss 4844.4320\n",
      "Iteration # 9060 : Loss 4841.0413\n",
      "Iteration # 9070 : Loss 4837.6613\n",
      "Iteration # 9080 : Loss 4834.2921\n",
      "Iteration # 9090 : Loss 4830.9336\n",
      "Iteration # 9100 : Loss 4827.5857\n",
      "Iteration # 9110 : Loss 4824.2484\n",
      "Iteration # 9120 : Loss 4820.9217\n",
      "Iteration # 9130 : Loss 4817.6056\n",
      "Iteration # 9140 : Loss 4814.2999\n",
      "Iteration # 9150 : Loss 4811.0047\n",
      "Iteration # 9160 : Loss 4807.7199\n",
      "Iteration # 9170 : Loss 4804.4455\n",
      "Iteration # 9180 : Loss 4801.1814\n",
      "Iteration # 9190 : Loss 4797.9276\n",
      "Iteration # 9200 : Loss 4794.6840\n",
      "Iteration # 9210 : Loss 4791.4507\n",
      "Iteration # 9220 : Loss 4788.2276\n",
      "Iteration # 9230 : Loss 4785.0146\n",
      "Iteration # 9240 : Loss 4781.8117\n",
      "Iteration # 9250 : Loss 4778.6189\n",
      "Iteration # 9260 : Loss 4775.4361\n",
      "Iteration # 9270 : Loss 4772.2633\n",
      "Iteration # 9280 : Loss 4769.1005\n",
      "Iteration # 9290 : Loss 4765.9475\n",
      "Iteration # 9300 : Loss 4762.8045\n",
      "Iteration # 9310 : Loss 4759.6713\n",
      "Iteration # 9320 : Loss 4756.5479\n",
      "Iteration # 9330 : Loss 4753.4342\n",
      "Iteration # 9340 : Loss 4750.3303\n",
      "Iteration # 9350 : Loss 4747.2361\n",
      "Iteration # 9360 : Loss 4744.1515\n",
      "Iteration # 9370 : Loss 4741.0766\n",
      "Iteration # 9380 : Loss 4738.0112\n",
      "Iteration # 9390 : Loss 4734.9554\n",
      "Iteration # 9400 : Loss 4731.9091\n",
      "Iteration # 9410 : Loss 4728.8723\n",
      "Iteration # 9420 : Loss 4725.8449\n",
      "Iteration # 9430 : Loss 4722.8270\n",
      "Iteration # 9440 : Loss 4719.8184\n",
      "Iteration # 9450 : Loss 4716.8191\n",
      "Iteration # 9460 : Loss 4713.8292\n",
      "Iteration # 9470 : Loss 4710.8485\n",
      "Iteration # 9480 : Loss 4707.8770\n",
      "Iteration # 9490 : Loss 4704.9147\n",
      "Iteration # 9500 : Loss 4701.9616\n",
      "Iteration # 9510 : Loss 4699.0176\n",
      "Iteration # 9520 : Loss 4696.0828\n",
      "Iteration # 9530 : Loss 4693.1569\n",
      "Iteration # 9540 : Loss 4690.2401\n",
      "Iteration # 9550 : Loss 4687.3323\n",
      "Iteration # 9560 : Loss 4684.4335\n",
      "Iteration # 9570 : Loss 4681.5435\n",
      "Iteration # 9580 : Loss 4678.6625\n",
      "Iteration # 9590 : Loss 4675.7903\n",
      "Iteration # 9600 : Loss 4672.9270\n",
      "Iteration # 9610 : Loss 4670.0724\n",
      "Iteration # 9620 : Loss 4667.2266\n",
      "Iteration # 9630 : Loss 4664.3895\n",
      "Iteration # 9640 : Loss 4661.5611\n",
      "Iteration # 9650 : Loss 4658.7414\n",
      "Iteration # 9660 : Loss 4655.9303\n",
      "Iteration # 9670 : Loss 4653.1278\n",
      "Iteration # 9680 : Loss 4650.3339\n",
      "Iteration # 9690 : Loss 4647.5485\n",
      "Iteration # 9700 : Loss 4644.7716\n",
      "Iteration # 9710 : Loss 4642.0031\n",
      "Iteration # 9720 : Loss 4639.2431\n",
      "Iteration # 9730 : Loss 4636.4915\n",
      "Iteration # 9740 : Loss 4633.7483\n",
      "Iteration # 9750 : Loss 4631.0134\n",
      "Iteration # 9760 : Loss 4628.2869\n",
      "Iteration # 9770 : Loss 4625.5686\n",
      "Iteration # 9780 : Loss 4622.8586\n",
      "Iteration # 9790 : Loss 4620.1567\n",
      "Iteration # 9800 : Loss 4617.4631\n",
      "Iteration # 9810 : Loss 4614.7776\n",
      "Iteration # 9820 : Loss 4612.1003\n",
      "Iteration # 9830 : Loss 4609.4310\n",
      "Iteration # 9840 : Loss 4606.7699\n",
      "Iteration # 9850 : Loss 4604.1167\n",
      "Iteration # 9860 : Loss 4601.4716\n",
      "Iteration # 9870 : Loss 4598.8344\n",
      "Iteration # 9880 : Loss 4596.2052\n",
      "Iteration # 9890 : Loss 4593.5839\n",
      "Iteration # 9900 : Loss 4590.9705\n",
      "Iteration # 9910 : Loss 4588.3649\n",
      "Iteration # 9920 : Loss 4585.7672\n",
      "Iteration # 9930 : Loss 4583.1773\n",
      "Iteration # 9940 : Loss 4580.5952\n",
      "Iteration # 9950 : Loss 4578.0208\n",
      "Iteration # 9960 : Loss 4575.4541\n",
      "Iteration # 9970 : Loss 4572.8951\n",
      "Iteration # 9980 : Loss 4570.3437\n",
      "Iteration # 9990 : Loss 4567.8000\n",
      "Iteration # 10000 : Loss 4565.2639\n",
      "Iteration # 10010 : Loss 4562.7353\n",
      "Iteration # 10020 : Loss 4560.2143\n",
      "Iteration # 10030 : Loss 4557.7008\n",
      "Iteration # 10040 : Loss 4555.1948\n",
      "Iteration # 10050 : Loss 4552.6963\n",
      "Iteration # 10060 : Loss 4550.2052\n",
      "Iteration # 10070 : Loss 4547.7215\n",
      "Iteration # 10080 : Loss 4545.2451\n",
      "Iteration # 10090 : Loss 4542.7762\n",
      "Iteration # 10100 : Loss 4540.3145\n",
      "Iteration # 10110 : Loss 4537.8601\n",
      "Iteration # 10120 : Loss 4535.4130\n",
      "Iteration # 10130 : Loss 4532.9732\n",
      "Iteration # 10140 : Loss 4530.5406\n",
      "Iteration # 10150 : Loss 4528.1151\n",
      "Iteration # 10160 : Loss 4525.6968\n",
      "Iteration # 10170 : Loss 4523.2857\n",
      "Iteration # 10180 : Loss 4520.8816\n",
      "Iteration # 10190 : Loss 4518.4847\n",
      "Iteration # 10200 : Loss 4516.0948\n",
      "Iteration # 10210 : Loss 4513.7119\n",
      "Iteration # 10220 : Loss 4511.3361\n",
      "Iteration # 10230 : Loss 4508.9672\n",
      "Iteration # 10240 : Loss 4506.6052\n",
      "Iteration # 10250 : Loss 4504.2502\n",
      "Iteration # 10260 : Loss 4501.9021\n",
      "Iteration # 10270 : Loss 4499.5609\n",
      "Iteration # 10280 : Loss 4497.2265\n",
      "Iteration # 10290 : Loss 4494.8989\n",
      "Iteration # 10300 : Loss 4492.5782\n",
      "Iteration # 10310 : Loss 4490.2642\n",
      "Iteration # 10320 : Loss 4487.9570\n",
      "Iteration # 10330 : Loss 4485.6565\n",
      "Iteration # 10340 : Loss 4483.3627\n",
      "Iteration # 10350 : Loss 4481.0755\n",
      "Iteration # 10360 : Loss 4478.7951\n",
      "Iteration # 10370 : Loss 4476.5212\n",
      "Iteration # 10380 : Loss 4474.2540\n",
      "Iteration # 10390 : Loss 4471.9933\n",
      "Iteration # 10400 : Loss 4469.7392\n",
      "Iteration # 10410 : Loss 4467.4916\n",
      "Iteration # 10420 : Loss 4465.2505\n",
      "Iteration # 10430 : Loss 4463.0159\n",
      "Iteration # 10440 : Loss 4460.7878\n",
      "Iteration # 10450 : Loss 4458.5661\n",
      "Iteration # 10460 : Loss 4456.3508\n",
      "Iteration # 10470 : Loss 4454.1418\n",
      "Iteration # 10480 : Loss 4451.9393\n",
      "Iteration # 10490 : Loss 4449.7431\n",
      "Iteration # 10500 : Loss 4447.5532\n",
      "Iteration # 10510 : Loss 4445.3696\n",
      "Iteration # 10520 : Loss 4443.1923\n",
      "Iteration # 10530 : Loss 4441.0212\n",
      "Iteration # 10540 : Loss 4438.8564\n",
      "Iteration # 10550 : Loss 4436.6977\n",
      "Iteration # 10560 : Loss 4434.5452\n",
      "Iteration # 10570 : Loss 4432.3989\n",
      "Iteration # 10580 : Loss 4430.2587\n",
      "Iteration # 10590 : Loss 4428.1247\n",
      "Iteration # 10600 : Loss 4425.9967\n",
      "Iteration # 10610 : Loss 4423.8748\n",
      "Iteration # 10620 : Loss 4421.7589\n",
      "Iteration # 10630 : Loss 4419.6491\n",
      "Iteration # 10640 : Loss 4417.5452\n",
      "Iteration # 10650 : Loss 4415.4473\n",
      "Iteration # 10660 : Loss 4413.3554\n",
      "Iteration # 10670 : Loss 4411.2695\n",
      "Iteration # 10680 : Loss 4409.1894\n",
      "Iteration # 10690 : Loss 4407.1152\n",
      "Iteration # 10700 : Loss 4405.0469\n",
      "Iteration # 10710 : Loss 4402.9845\n",
      "Iteration # 10720 : Loss 4400.9278\n",
      "Iteration # 10730 : Loss 4398.8770\n",
      "Iteration # 10740 : Loss 4396.8320\n",
      "Iteration # 10750 : Loss 4394.7927\n",
      "Iteration # 10760 : Loss 4392.7592\n",
      "Iteration # 10770 : Loss 4390.7314\n",
      "Iteration # 10780 : Loss 4388.7093\n",
      "Iteration # 10790 : Loss 4386.6929\n",
      "Iteration # 10800 : Loss 4384.6821\n",
      "Iteration # 10810 : Loss 4382.6770\n",
      "Iteration # 10820 : Loss 4380.6775\n",
      "Iteration # 10830 : Loss 4378.6836\n",
      "Iteration # 10840 : Loss 4376.6952\n",
      "Iteration # 10850 : Loss 4374.7125\n",
      "Iteration # 10860 : Loss 4372.7352\n",
      "Iteration # 10870 : Loss 4370.7635\n",
      "Iteration # 10880 : Loss 4368.7973\n",
      "Iteration # 10890 : Loss 4366.8365\n",
      "Iteration # 10900 : Loss 4364.8812\n",
      "Iteration # 10910 : Loss 4362.9314\n",
      "Iteration # 10920 : Loss 4360.9870\n",
      "Iteration # 10930 : Loss 4359.0479\n",
      "Iteration # 10940 : Loss 4357.1143\n",
      "Iteration # 10950 : Loss 4355.1860\n",
      "Iteration # 10960 : Loss 4353.2630\n",
      "Iteration # 10970 : Loss 4351.3454\n",
      "Iteration # 10980 : Loss 4349.4330\n",
      "Iteration # 10990 : Loss 4347.5260\n",
      "Iteration # 11000 : Loss 4345.6242\n",
      "Iteration # 11010 : Loss 4343.7276\n",
      "Iteration # 11020 : Loss 4341.8363\n",
      "Iteration # 11030 : Loss 4339.9502\n",
      "Iteration # 11040 : Loss 4338.0692\n",
      "Iteration # 11050 : Loss 4336.1935\n",
      "Iteration # 11060 : Loss 4334.3228\n",
      "Iteration # 11070 : Loss 4332.4574\n",
      "Iteration # 11080 : Loss 4330.5970\n",
      "Iteration # 11090 : Loss 4328.7417\n",
      "Iteration # 11100 : Loss 4326.8915\n",
      "Iteration # 11110 : Loss 4325.0463\n",
      "Iteration # 11120 : Loss 4323.2062\n",
      "Iteration # 11130 : Loss 4321.3711\n",
      "Iteration # 11140 : Loss 4319.5410\n",
      "Iteration # 11150 : Loss 4317.7159\n",
      "Iteration # 11160 : Loss 4315.8958\n",
      "Iteration # 11170 : Loss 4314.0806\n",
      "Iteration # 11180 : Loss 4312.2703\n",
      "Iteration # 11190 : Loss 4310.4650\n",
      "Iteration # 11200 : Loss 4308.6645\n",
      "Iteration # 11210 : Loss 4306.8690\n",
      "Iteration # 11220 : Loss 4305.0782\n",
      "Iteration # 11230 : Loss 4303.2924\n",
      "Iteration # 11240 : Loss 4301.5113\n",
      "Iteration # 11250 : Loss 4299.7351\n",
      "Iteration # 11260 : Loss 4297.9636\n",
      "Iteration # 11270 : Loss 4296.1969\n",
      "Iteration # 11280 : Loss 4294.4350\n",
      "Iteration # 11290 : Loss 4292.6778\n",
      "Iteration # 11300 : Loss 4290.9254\n",
      "Iteration # 11310 : Loss 4289.1776\n",
      "Iteration # 11320 : Loss 4287.4345\n",
      "Iteration # 11330 : Loss 4285.6961\n",
      "Iteration # 11340 : Loss 4283.9624\n",
      "Iteration # 11350 : Loss 4282.2333\n",
      "Iteration # 11360 : Loss 4280.5088\n",
      "Iteration # 11370 : Loss 4278.7889\n",
      "Iteration # 11380 : Loss 4277.0736\n",
      "Iteration # 11390 : Loss 4275.3629\n",
      "Iteration # 11400 : Loss 4273.6567\n",
      "Iteration # 11410 : Loss 4271.9551\n",
      "Iteration # 11420 : Loss 4270.2580\n",
      "Iteration # 11430 : Loss 4268.5654\n",
      "Iteration # 11440 : Loss 4266.8773\n",
      "Iteration # 11450 : Loss 4265.1936\n",
      "Iteration # 11460 : Loss 4263.5145\n",
      "Iteration # 11470 : Loss 4261.8397\n",
      "Iteration # 11480 : Loss 4260.1694\n",
      "Iteration # 11490 : Loss 4258.5035\n",
      "Iteration # 11500 : Loss 4256.8420\n",
      "Iteration # 11510 : Loss 4255.1849\n",
      "Iteration # 11520 : Loss 4253.5321\n",
      "Iteration # 11530 : Loss 4251.8837\n",
      "Iteration # 11540 : Loss 4250.2396\n",
      "Iteration # 11550 : Loss 4248.5999\n",
      "Iteration # 11560 : Loss 4246.9644\n",
      "Iteration # 11570 : Loss 4245.3333\n",
      "Iteration # 11580 : Loss 4243.7063\n",
      "Iteration # 11590 : Loss 4242.0837\n",
      "Iteration # 11600 : Loss 4240.4653\n",
      "Iteration # 11610 : Loss 4238.8511\n",
      "Iteration # 11620 : Loss 4237.2411\n",
      "Iteration # 11630 : Loss 4235.6354\n",
      "Iteration # 11640 : Loss 4234.0338\n",
      "Iteration # 11650 : Loss 4232.4363\n",
      "Iteration # 11660 : Loss 4230.8431\n",
      "Iteration # 11670 : Loss 4229.2539\n",
      "Iteration # 11680 : Loss 4227.6689\n",
      "Iteration # 11690 : Loss 4226.0880\n",
      "Iteration # 11700 : Loss 4224.5112\n",
      "Iteration # 11710 : Loss 4222.9384\n",
      "Iteration # 11720 : Loss 4221.3698\n",
      "Iteration # 11730 : Loss 4219.8051\n",
      "Iteration # 11740 : Loss 4218.2445\n",
      "Iteration # 11750 : Loss 4216.6880\n",
      "Iteration # 11760 : Loss 4215.1354\n",
      "Iteration # 11770 : Loss 4213.5868\n",
      "Iteration # 11780 : Loss 4212.0422\n",
      "Iteration # 11790 : Loss 4210.5016\n",
      "Iteration # 11800 : Loss 4208.9649\n",
      "Iteration # 11810 : Loss 4207.4322\n",
      "Iteration # 11820 : Loss 4205.9033\n",
      "Iteration # 11830 : Loss 4204.3784\n",
      "Iteration # 11840 : Loss 4202.8574\n",
      "Iteration # 11850 : Loss 4201.3403\n",
      "Iteration # 11860 : Loss 4199.8270\n",
      "Iteration # 11870 : Loss 4198.3176\n",
      "Iteration # 11880 : Loss 4196.8120\n",
      "Iteration # 11890 : Loss 4195.3102\n",
      "Iteration # 11900 : Loss 4193.8123\n",
      "Iteration # 11910 : Loss 4192.3181\n",
      "Iteration # 11920 : Loss 4190.8278\n",
      "Iteration # 11930 : Loss 4189.3412\n",
      "Iteration # 11940 : Loss 4187.8584\n",
      "Iteration # 11950 : Loss 4186.3793\n",
      "Iteration # 11960 : Loss 4184.9039\n",
      "Iteration # 11970 : Loss 4183.4323\n",
      "Iteration # 11980 : Loss 4181.9644\n",
      "Iteration # 11990 : Loss 4180.5002\n",
      "Iteration # 12000 : Loss 4179.0396\n",
      "Iteration # 12010 : Loss 4177.5827\n",
      "Iteration # 12020 : Loss 4176.1295\n",
      "Iteration # 12030 : Loss 4174.6799\n",
      "Iteration # 12040 : Loss 4173.2340\n",
      "Iteration # 12050 : Loss 4171.7917\n",
      "Iteration # 12060 : Loss 4170.3529\n",
      "Iteration # 12070 : Loss 4168.9178\n",
      "Iteration # 12080 : Loss 4167.4862\n",
      "Iteration # 12090 : Loss 4166.0582\n",
      "Iteration # 12100 : Loss 4164.6338\n",
      "Iteration # 12110 : Loss 4163.2129\n",
      "Iteration # 12120 : Loss 4161.7955\n",
      "Iteration # 12130 : Loss 4160.3817\n",
      "Iteration # 12140 : Loss 4158.9713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 12150 : Loss 4157.5645\n",
      "Iteration # 12160 : Loss 4156.1611\n",
      "Iteration # 12170 : Loss 4154.7612\n",
      "Iteration # 12180 : Loss 4153.3647\n",
      "Iteration # 12190 : Loss 4151.9717\n",
      "Iteration # 12200 : Loss 4150.5822\n",
      "Iteration # 12210 : Loss 4149.1960\n",
      "Iteration # 12220 : Loss 4147.8133\n",
      "Iteration # 12230 : Loss 4146.4339\n",
      "Iteration # 12240 : Loss 4145.0579\n",
      "Iteration # 12250 : Loss 4143.6853\n",
      "Iteration # 12260 : Loss 4142.3161\n",
      "Iteration # 12270 : Loss 4140.9502\n",
      "Iteration # 12280 : Loss 4139.5877\n",
      "Iteration # 12290 : Loss 4138.2284\n",
      "Iteration # 12300 : Loss 4136.8725\n",
      "Iteration # 12310 : Loss 4135.5199\n",
      "Iteration # 12320 : Loss 4134.1706\n",
      "Iteration # 12330 : Loss 4132.8246\n",
      "Iteration # 12340 : Loss 4131.4818\n",
      "Iteration # 12350 : Loss 4130.1423\n",
      "Iteration # 12360 : Loss 4128.8060\n",
      "Iteration # 12370 : Loss 4127.4730\n",
      "Iteration # 12380 : Loss 4126.1432\n",
      "Iteration # 12390 : Loss 4124.8166\n",
      "Iteration # 12400 : Loss 4123.4932\n",
      "Iteration # 12410 : Loss 4122.1730\n",
      "Iteration # 12420 : Loss 4120.8560\n",
      "Iteration # 12430 : Loss 4119.5421\n",
      "Iteration # 12440 : Loss 4118.2314\n",
      "Iteration # 12450 : Loss 4116.9239\n",
      "Iteration # 12460 : Loss 4115.6195\n",
      "Iteration # 12470 : Loss 4114.3182\n",
      "Iteration # 12480 : Loss 4113.0200\n",
      "Iteration # 12490 : Loss 4111.7249\n",
      "Iteration # 12500 : Loss 4110.4330\n",
      "Iteration # 12510 : Loss 4109.1441\n",
      "Iteration # 12520 : Loss 4107.8582\n",
      "Iteration # 12530 : Loss 4106.5755\n",
      "Iteration # 12540 : Loss 4105.2957\n",
      "Iteration # 12550 : Loss 4104.0191\n",
      "Iteration # 12560 : Loss 4102.7454\n",
      "Iteration # 12570 : Loss 4101.4748\n",
      "Iteration # 12580 : Loss 4100.2072\n",
      "Iteration # 12590 : Loss 4098.9425\n",
      "Iteration # 12600 : Loss 4097.6809\n",
      "Iteration # 12610 : Loss 4096.4222\n",
      "Iteration # 12620 : Loss 4095.1666\n",
      "Iteration # 12630 : Loss 4093.9138\n",
      "Iteration # 12640 : Loss 4092.6640\n",
      "Iteration # 12650 : Loss 4091.4172\n",
      "Iteration # 12660 : Loss 4090.1733\n",
      "Iteration # 12670 : Loss 4088.9322\n",
      "Iteration # 12680 : Loss 4087.6941\n",
      "Iteration # 12690 : Loss 4086.4589\n",
      "Iteration # 12700 : Loss 4085.2266\n",
      "Iteration # 12710 : Loss 4083.9972\n",
      "Iteration # 12720 : Loss 4082.7706\n",
      "Iteration # 12730 : Loss 4081.5469\n",
      "Iteration # 12740 : Loss 4080.3260\n",
      "Iteration # 12750 : Loss 4079.1080\n",
      "Iteration # 12760 : Loss 4077.8928\n",
      "Iteration # 12770 : Loss 4076.6804\n",
      "Iteration # 12780 : Loss 4075.4709\n",
      "Iteration # 12790 : Loss 4074.2641\n",
      "Iteration # 12800 : Loss 4073.0601\n",
      "Iteration # 12810 : Loss 4071.8589\n",
      "Iteration # 12820 : Loss 4070.6605\n",
      "Iteration # 12830 : Loss 4069.4648\n",
      "Iteration # 12840 : Loss 4068.2719\n",
      "Iteration # 12850 : Loss 4067.0817\n",
      "Iteration # 12860 : Loss 4065.8943\n",
      "Iteration # 12870 : Loss 4064.7096\n",
      "Iteration # 12880 : Loss 4063.5276\n",
      "Iteration # 12890 : Loss 4062.3483\n",
      "Iteration # 12900 : Loss 4061.1717\n",
      "Iteration # 12910 : Loss 4059.9978\n",
      "Iteration # 12920 : Loss 4058.8265\n",
      "Iteration # 12930 : Loss 4057.6580\n",
      "Iteration # 12940 : Loss 4056.4921\n",
      "Iteration # 12950 : Loss 4055.3288\n",
      "Iteration # 12960 : Loss 4054.1682\n",
      "Iteration # 12970 : Loss 4053.0102\n",
      "Iteration # 12980 : Loss 4051.8549\n",
      "Iteration # 12990 : Loss 4050.7022\n",
      "Iteration # 13000 : Loss 4049.5520\n",
      "Iteration # 13010 : Loss 4048.4045\n",
      "Iteration # 13020 : Loss 4047.2596\n",
      "Iteration # 13030 : Loss 4046.1172\n",
      "Iteration # 13040 : Loss 4044.9774\n",
      "Iteration # 13050 : Loss 4043.8402\n",
      "Iteration # 13060 : Loss 4042.7055\n",
      "Iteration # 13070 : Loss 4041.5734\n",
      "Iteration # 13080 : Loss 4040.4438\n",
      "Iteration # 13090 : Loss 4039.3168\n",
      "Iteration # 13100 : Loss 4038.1923\n",
      "Iteration # 13110 : Loss 4037.0702\n",
      "Iteration # 13120 : Loss 4035.9507\n",
      "Iteration # 13130 : Loss 4034.8337\n",
      "Iteration # 13140 : Loss 4033.7192\n",
      "Iteration # 13150 : Loss 4032.6071\n",
      "Iteration # 13160 : Loss 4031.4975\n",
      "Iteration # 13170 : Loss 4030.3904\n",
      "Iteration # 13180 : Loss 4029.2857\n",
      "Iteration # 13190 : Loss 4028.1835\n",
      "Iteration # 13200 : Loss 4027.0837\n",
      "Iteration # 13210 : Loss 4025.9864\n",
      "Iteration # 13220 : Loss 4024.8914\n",
      "Iteration # 13230 : Loss 4023.7989\n",
      "Iteration # 13240 : Loss 4022.7088\n",
      "Iteration # 13250 : Loss 4021.6211\n",
      "Iteration # 13260 : Loss 4020.5357\n",
      "Iteration # 13270 : Loss 4019.4528\n",
      "Iteration # 13280 : Loss 4018.3722\n",
      "Iteration # 13290 : Loss 4017.2940\n",
      "Iteration # 13300 : Loss 4016.2182\n",
      "Iteration # 13310 : Loss 4015.1447\n",
      "Iteration # 13320 : Loss 4014.0735\n",
      "Iteration # 13330 : Loss 4013.0047\n",
      "Iteration # 13340 : Loss 4011.9382\n",
      "Iteration # 13350 : Loss 4010.8740\n",
      "Iteration # 13360 : Loss 4009.8121\n",
      "Iteration # 13370 : Loss 4008.7525\n",
      "Iteration # 13380 : Loss 4007.6953\n",
      "Iteration # 13390 : Loss 4006.6403\n",
      "Iteration # 13400 : Loss 4005.5876\n",
      "Iteration # 13410 : Loss 4004.5371\n",
      "Iteration # 13420 : Loss 4003.4890\n",
      "Iteration # 13430 : Loss 4002.4431\n",
      "Iteration # 13440 : Loss 4001.3994\n",
      "Iteration # 13450 : Loss 4000.3580\n",
      "Iteration # 13460 : Loss 3999.3188\n",
      "Iteration # 13470 : Loss 3998.2819\n",
      "Iteration # 13480 : Loss 3997.2471\n",
      "Iteration # 13490 : Loss 3996.2146\n",
      "Iteration # 13500 : Loss 3995.1843\n",
      "Iteration # 13510 : Loss 3994.1562\n",
      "Iteration # 13520 : Loss 3993.1303\n",
      "Iteration # 13530 : Loss 3992.1065\n",
      "Iteration # 13540 : Loss 3991.0850\n",
      "Iteration # 13550 : Loss 3990.0656\n",
      "Iteration # 13560 : Loss 3989.0484\n",
      "Iteration # 13570 : Loss 3988.0333\n",
      "Iteration # 13580 : Loss 3987.0204\n",
      "Iteration # 13590 : Loss 3986.0096\n",
      "Iteration # 13600 : Loss 3985.0010\n",
      "Iteration # 13610 : Loss 3983.9944\n",
      "Iteration # 13620 : Loss 3982.9900\n",
      "Iteration # 13630 : Loss 3981.9878\n",
      "Iteration # 13640 : Loss 3980.9876\n",
      "Iteration # 13650 : Loss 3979.9895\n",
      "Iteration # 13660 : Loss 3978.9935\n",
      "Iteration # 13670 : Loss 3977.9996\n",
      "Iteration # 13680 : Loss 3977.0078\n",
      "Iteration # 13690 : Loss 3976.0181\n",
      "Iteration # 13700 : Loss 3975.0304\n",
      "Iteration # 13710 : Loss 3974.0448\n",
      "Iteration # 13720 : Loss 3973.0612\n",
      "Iteration # 13730 : Loss 3972.0797\n",
      "Iteration # 13740 : Loss 3971.1002\n",
      "Iteration # 13750 : Loss 3970.1228\n",
      "Iteration # 13760 : Loss 3969.1474\n",
      "Iteration # 13770 : Loss 3968.1740\n",
      "Iteration # 13780 : Loss 3967.2026\n",
      "Iteration # 13790 : Loss 3966.2332\n",
      "Iteration # 13800 : Loss 3965.2658\n",
      "Iteration # 13810 : Loss 3964.3004\n",
      "Iteration # 13820 : Loss 3963.3370\n",
      "Iteration # 13830 : Loss 3962.3756\n",
      "Iteration # 13840 : Loss 3961.4162\n",
      "Iteration # 13850 : Loss 3960.4587\n",
      "Iteration # 13860 : Loss 3959.5032\n",
      "Iteration # 13870 : Loss 3958.5496\n",
      "Iteration # 13880 : Loss 3957.5980\n",
      "Iteration # 13890 : Loss 3956.6483\n",
      "Iteration # 13900 : Loss 3955.7006\n",
      "Iteration # 13910 : Loss 3954.7548\n",
      "Iteration # 13920 : Loss 3953.8109\n",
      "Iteration # 13930 : Loss 3952.8689\n",
      "Iteration # 13940 : Loss 3951.9289\n",
      "Iteration # 13950 : Loss 3950.9907\n",
      "Iteration # 13960 : Loss 3950.0545\n",
      "Iteration # 13970 : Loss 3949.1201\n",
      "Iteration # 13980 : Loss 3948.1876\n",
      "Iteration # 13990 : Loss 3947.2571\n",
      "Iteration # 14000 : Loss 3946.3283\n",
      "Iteration # 14010 : Loss 3945.4015\n",
      "Iteration # 14020 : Loss 3944.4765\n",
      "Iteration # 14030 : Loss 3943.5534\n",
      "Iteration # 14040 : Loss 3942.6321\n",
      "Iteration # 14050 : Loss 3941.7127\n",
      "Iteration # 14060 : Loss 3940.7951\n",
      "Iteration # 14070 : Loss 3939.8793\n",
      "Iteration # 14080 : Loss 3938.9654\n",
      "Iteration # 14090 : Loss 3938.0533\n",
      "Iteration # 14100 : Loss 3937.1430\n",
      "Iteration # 14110 : Loss 3936.2345\n",
      "Iteration # 14120 : Loss 3935.3278\n",
      "Iteration # 14130 : Loss 3934.4229\n",
      "Iteration # 14140 : Loss 3933.5199\n",
      "Iteration # 14150 : Loss 3932.6186\n",
      "Iteration # 14160 : Loss 3931.7190\n",
      "Iteration # 14170 : Loss 3930.8213\n",
      "Iteration # 14180 : Loss 3929.9253\n",
      "Iteration # 14190 : Loss 3929.0311\n",
      "Iteration # 14200 : Loss 3928.1386\n",
      "Iteration # 14210 : Loss 3927.2479\n",
      "Iteration # 14220 : Loss 3926.3590\n",
      "Iteration # 14230 : Loss 3925.4718\n",
      "Iteration # 14240 : Loss 3924.5863\n",
      "Iteration # 14250 : Loss 3923.7026\n",
      "Iteration # 14260 : Loss 3922.8205\n",
      "Iteration # 14270 : Loss 3921.9403\n",
      "Iteration # 14280 : Loss 3921.0617\n",
      "Iteration # 14290 : Loss 3920.1848\n",
      "Iteration # 14300 : Loss 3919.3096\n",
      "Iteration # 14310 : Loss 3918.4362\n",
      "Iteration # 14320 : Loss 3917.5644\n",
      "Iteration # 14330 : Loss 3916.6943\n",
      "Iteration # 14340 : Loss 3915.8259\n",
      "Iteration # 14350 : Loss 3914.9591\n",
      "Iteration # 14360 : Loss 3914.0941\n",
      "Iteration # 14370 : Loss 3913.2307\n",
      "Iteration # 14380 : Loss 3912.3690\n",
      "Iteration # 14390 : Loss 3911.5089\n",
      "Iteration # 14400 : Loss 3910.6505\n",
      "Iteration # 14410 : Loss 3909.7937\n",
      "Iteration # 14420 : Loss 3908.9385\n",
      "Iteration # 14430 : Loss 3908.0850\n",
      "Iteration # 14440 : Loss 3907.2332\n",
      "Iteration # 14450 : Loss 3906.3829\n",
      "Iteration # 14460 : Loss 3905.5343\n",
      "Iteration # 14470 : Loss 3904.6873\n",
      "Iteration # 14480 : Loss 3903.8419\n",
      "Iteration # 14490 : Loss 3902.9981\n",
      "Iteration # 14500 : Loss 3902.1559\n",
      "Iteration # 14510 : Loss 3901.3153\n",
      "Iteration # 14520 : Loss 3900.4763\n",
      "Iteration # 14530 : Loss 3899.6389\n",
      "Iteration # 14540 : Loss 3898.8030\n",
      "Iteration # 14550 : Loss 3897.9687\n",
      "Iteration # 14560 : Loss 3897.1361\n",
      "Iteration # 14570 : Loss 3896.3049\n",
      "Iteration # 14580 : Loss 3895.4753\n",
      "Iteration # 14590 : Loss 3894.6473\n",
      "Iteration # 14600 : Loss 3893.8209\n",
      "Iteration # 14610 : Loss 3892.9959\n",
      "Iteration # 14620 : Loss 3892.1726\n",
      "Iteration # 14630 : Loss 3891.3507\n",
      "Iteration # 14640 : Loss 3890.5304\n",
      "Iteration # 14650 : Loss 3889.7116\n",
      "Iteration # 14660 : Loss 3888.8944\n",
      "Iteration # 14670 : Loss 3888.0786\n",
      "Iteration # 14680 : Loss 3887.2644\n",
      "Iteration # 14690 : Loss 3886.4517\n",
      "Iteration # 14700 : Loss 3885.6405\n",
      "Iteration # 14710 : Loss 3884.8307\n",
      "Iteration # 14720 : Loss 3884.0225\n",
      "Iteration # 14730 : Loss 3883.2158\n",
      "Iteration # 14740 : Loss 3882.4105\n",
      "Iteration # 14750 : Loss 3881.6068\n",
      "Iteration # 14760 : Loss 3880.8045\n",
      "Iteration # 14770 : Loss 3880.0037\n",
      "Iteration # 14780 : Loss 3879.2043\n",
      "Iteration # 14790 : Loss 3878.4064\n",
      "Iteration # 14800 : Loss 3877.6100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 14810 : Loss 3876.8150\n",
      "Iteration # 14820 : Loss 3876.0215\n",
      "Iteration # 14830 : Loss 3875.2294\n",
      "Iteration # 14840 : Loss 3874.4388\n",
      "Iteration # 14850 : Loss 3873.6496\n",
      "Iteration # 14860 : Loss 3872.8618\n",
      "Iteration # 14870 : Loss 3872.0755\n",
      "Iteration # 14880 : Loss 3871.2905\n",
      "Iteration # 14890 : Loss 3870.5070\n",
      "Iteration # 14900 : Loss 3869.7249\n",
      "Iteration # 14910 : Loss 3868.9443\n",
      "Iteration # 14920 : Loss 3868.1650\n",
      "Iteration # 14930 : Loss 3867.3871\n",
      "Iteration # 14940 : Loss 3866.6106\n",
      "Iteration # 14950 : Loss 3865.8356\n",
      "Iteration # 14960 : Loss 3865.0619\n",
      "Iteration # 14970 : Loss 3864.2896\n",
      "Iteration # 14980 : Loss 3863.5186\n",
      "Iteration # 14990 : Loss 3862.7491\n",
      "Iteration # 15000 : Loss 3861.9809\n",
      "Iteration # 15010 : Loss 3861.2141\n",
      "Iteration # 15020 : Loss 3860.4487\n",
      "Iteration # 15030 : Loss 3859.6846\n",
      "Iteration # 15040 : Loss 3858.9218\n",
      "Iteration # 15050 : Loss 3858.1605\n",
      "Iteration # 15060 : Loss 3857.4004\n",
      "Iteration # 15070 : Loss 3856.6417\n",
      "Iteration # 15080 : Loss 3855.8844\n",
      "Iteration # 15090 : Loss 3855.1284\n",
      "Iteration # 15100 : Loss 3854.3737\n",
      "Iteration # 15110 : Loss 3853.6203\n",
      "Iteration # 15120 : Loss 3852.8683\n",
      "Iteration # 15130 : Loss 3852.1176\n",
      "Iteration # 15140 : Loss 3851.3682\n",
      "Iteration # 15150 : Loss 3850.6201\n",
      "Iteration # 15160 : Loss 3849.8734\n",
      "Iteration # 15170 : Loss 3849.1279\n",
      "Iteration # 15180 : Loss 3848.3837\n",
      "Iteration # 15190 : Loss 3847.6408\n",
      "Iteration # 15200 : Loss 3846.8993\n",
      "Iteration # 15210 : Loss 3846.1590\n",
      "Iteration # 15220 : Loss 3845.4199\n",
      "Iteration # 15230 : Loss 3844.6822\n",
      "Iteration # 15240 : Loss 3843.9458\n",
      "Iteration # 15250 : Loss 3843.2106\n",
      "Iteration # 15260 : Loss 3842.4767\n",
      "Iteration # 15270 : Loss 3841.7440\n",
      "Iteration # 15280 : Loss 3841.0126\n",
      "Iteration # 15290 : Loss 3840.2825\n",
      "Iteration # 15300 : Loss 3839.5536\n",
      "Iteration # 15310 : Loss 3838.8260\n",
      "Iteration # 15320 : Loss 3838.0997\n",
      "Iteration # 15330 : Loss 3837.3745\n",
      "Iteration # 15340 : Loss 3836.6506\n",
      "Iteration # 15350 : Loss 3835.9280\n",
      "Iteration # 15360 : Loss 3835.2066\n",
      "Iteration # 15370 : Loss 3834.4864\n",
      "Iteration # 15380 : Loss 3833.7674\n",
      "Iteration # 15390 : Loss 3833.0496\n",
      "Iteration # 15400 : Loss 3832.3331\n",
      "Iteration # 15410 : Loss 3831.6178\n",
      "Iteration # 15420 : Loss 3830.9037\n",
      "Iteration # 15430 : Loss 3830.1908\n",
      "Iteration # 15440 : Loss 3829.4791\n",
      "Iteration # 15450 : Loss 3828.7686\n",
      "Iteration # 15460 : Loss 3828.0593\n",
      "Iteration # 15470 : Loss 3827.3512\n",
      "Iteration # 15480 : Loss 3826.6443\n",
      "Iteration # 15490 : Loss 3825.9386\n",
      "Iteration # 15500 : Loss 3825.2340\n",
      "Iteration # 15510 : Loss 3824.5307\n",
      "Iteration # 15520 : Loss 3823.8285\n",
      "Iteration # 15530 : Loss 3823.1275\n",
      "Iteration # 15540 : Loss 3822.4276\n",
      "Iteration # 15550 : Loss 3821.7289\n",
      "Iteration # 15560 : Loss 3821.0314\n",
      "Iteration # 15570 : Loss 3820.3350\n",
      "Iteration # 15580 : Loss 3819.6398\n",
      "Iteration # 15590 : Loss 3818.9458\n",
      "Iteration # 15600 : Loss 3818.2529\n",
      "Iteration # 15610 : Loss 3817.5611\n",
      "Iteration # 15620 : Loss 3816.8705\n",
      "Iteration # 15630 : Loss 3816.1810\n",
      "Iteration # 15640 : Loss 3815.4927\n",
      "Iteration # 15650 : Loss 3814.8055\n",
      "Iteration # 15660 : Loss 3814.1194\n",
      "Iteration # 15670 : Loss 3813.4344\n",
      "Iteration # 15680 : Loss 3812.7506\n",
      "Iteration # 15690 : Loss 3812.0679\n",
      "Iteration # 15700 : Loss 3811.3863\n",
      "Iteration # 15710 : Loss 3810.7058\n",
      "Iteration # 15720 : Loss 3810.0264\n",
      "Iteration # 15730 : Loss 3809.3481\n",
      "Iteration # 15740 : Loss 3808.6710\n",
      "Iteration # 15750 : Loss 3807.9949\n",
      "Iteration # 15760 : Loss 3807.3199\n",
      "Iteration # 15770 : Loss 3806.6461\n",
      "Iteration # 15780 : Loss 3805.9733\n",
      "Iteration # 15790 : Loss 3805.3016\n",
      "Iteration # 15800 : Loss 3804.6310\n",
      "Iteration # 15810 : Loss 3803.9614\n",
      "Iteration # 15820 : Loss 3803.2930\n",
      "Iteration # 15830 : Loss 3802.6256\n",
      "Iteration # 15840 : Loss 3801.9593\n",
      "Iteration # 15850 : Loss 3801.2941\n",
      "Iteration # 15860 : Loss 3800.6299\n",
      "Iteration # 15870 : Loss 3799.9668\n",
      "Iteration # 15880 : Loss 3799.3047\n",
      "Iteration # 15890 : Loss 3798.6438\n",
      "Iteration # 15900 : Loss 3797.9838\n",
      "Iteration # 15910 : Loss 3797.3249\n",
      "Iteration # 15920 : Loss 3796.6671\n",
      "Iteration # 15930 : Loss 3796.0103\n",
      "Iteration # 15940 : Loss 3795.3546\n",
      "Iteration # 15950 : Loss 3794.6999\n",
      "Iteration # 15960 : Loss 3794.0462\n",
      "Iteration # 15970 : Loss 3793.3935\n",
      "Iteration # 15980 : Loss 3792.7419\n",
      "Iteration # 15990 : Loss 3792.0914\n",
      "Iteration # 16000 : Loss 3791.4418\n",
      "Iteration # 16010 : Loss 3790.7933\n",
      "Iteration # 16020 : Loss 3790.1457\n",
      "Iteration # 16030 : Loss 3789.4992\n",
      "Iteration # 16040 : Loss 3788.8538\n",
      "Iteration # 16050 : Loss 3788.2093\n",
      "Iteration # 16060 : Loss 3787.5658\n",
      "Iteration # 16070 : Loss 3786.9233\n",
      "Iteration # 16080 : Loss 3786.2819\n",
      "Iteration # 16090 : Loss 3785.6414\n",
      "Iteration # 16100 : Loss 3785.0020\n",
      "Iteration # 16110 : Loss 3784.3635\n",
      "Iteration # 16120 : Loss 3783.7260\n",
      "Iteration # 16130 : Loss 3783.0895\n",
      "Iteration # 16140 : Loss 3782.4540\n",
      "Iteration # 16150 : Loss 3781.8195\n",
      "Iteration # 16160 : Loss 3781.1859\n",
      "Iteration # 16170 : Loss 3780.5534\n",
      "Iteration # 16180 : Loss 3779.9218\n",
      "Iteration # 16190 : Loss 3779.2912\n",
      "Iteration # 16200 : Loss 3778.6615\n",
      "Iteration # 16210 : Loss 3778.0328\n",
      "Iteration # 16220 : Loss 3777.4051\n",
      "Iteration # 16230 : Loss 3776.7783\n",
      "Iteration # 16240 : Loss 3776.1525\n",
      "Iteration # 16250 : Loss 3775.5277\n",
      "Iteration # 16260 : Loss 3774.9038\n",
      "Iteration # 16270 : Loss 3774.2808\n",
      "Iteration # 16280 : Loss 3773.6588\n",
      "Iteration # 16290 : Loss 3773.0378\n",
      "Iteration # 16300 : Loss 3772.4177\n",
      "Iteration # 16310 : Loss 3771.7985\n",
      "Iteration # 16320 : Loss 3771.1803\n",
      "Iteration # 16330 : Loss 3770.5630\n",
      "Iteration # 16340 : Loss 3769.9466\n",
      "Iteration # 16350 : Loss 3769.3312\n",
      "Iteration # 16360 : Loss 3768.7167\n",
      "Iteration # 16370 : Loss 3768.1031\n",
      "Iteration # 16380 : Loss 3767.4905\n",
      "Iteration # 16390 : Loss 3766.8787\n",
      "Iteration # 16400 : Loss 3766.2679\n",
      "Iteration # 16410 : Loss 3765.6580\n",
      "Iteration # 16420 : Loss 3765.0490\n",
      "Iteration # 16430 : Loss 3764.4409\n",
      "Iteration # 16440 : Loss 3763.8338\n",
      "Iteration # 16450 : Loss 3763.2275\n",
      "Iteration # 16460 : Loss 3762.6221\n",
      "Iteration # 16470 : Loss 3762.0176\n",
      "Iteration # 16480 : Loss 3761.4141\n",
      "Iteration # 16490 : Loss 3760.8114\n",
      "Iteration # 16500 : Loss 3760.2096\n",
      "Iteration # 16510 : Loss 3759.6087\n",
      "Iteration # 16520 : Loss 3759.0087\n",
      "Iteration # 16530 : Loss 3758.4096\n",
      "Iteration # 16540 : Loss 3757.8114\n",
      "Iteration # 16550 : Loss 3757.2140\n",
      "Iteration # 16560 : Loss 3756.6175\n",
      "Iteration # 16570 : Loss 3756.0219\n",
      "Iteration # 16580 : Loss 3755.4272\n",
      "Iteration # 16590 : Loss 3754.8334\n",
      "Iteration # 16600 : Loss 3754.2404\n",
      "Iteration # 16610 : Loss 3753.6482\n",
      "Iteration # 16620 : Loss 3753.0570\n",
      "Iteration # 16630 : Loss 3752.4666\n",
      "Iteration # 16640 : Loss 3751.8771\n",
      "Iteration # 16650 : Loss 3751.2884\n",
      "Iteration # 16660 : Loss 3750.7006\n",
      "Iteration # 16670 : Loss 3750.1136\n",
      "Iteration # 16680 : Loss 3749.5275\n",
      "Iteration # 16690 : Loss 3748.9422\n",
      "Iteration # 16700 : Loss 3748.3578\n",
      "Iteration # 16710 : Loss 3747.7742\n",
      "Iteration # 16720 : Loss 3747.1915\n",
      "Iteration # 16730 : Loss 3746.6096\n",
      "Iteration # 16740 : Loss 3746.0285\n",
      "Iteration # 16750 : Loss 3745.4483\n",
      "Iteration # 16760 : Loss 3744.8689\n",
      "Iteration # 16770 : Loss 3744.2904\n",
      "Iteration # 16780 : Loss 3743.7126\n",
      "Iteration # 16790 : Loss 3743.1357\n",
      "Iteration # 16800 : Loss 3742.5596\n",
      "Iteration # 16810 : Loss 3741.9844\n",
      "Iteration # 16820 : Loss 3741.4099\n",
      "Iteration # 16830 : Loss 3740.8363\n",
      "Iteration # 16840 : Loss 3740.2635\n",
      "Iteration # 16850 : Loss 3739.6915\n",
      "Iteration # 16860 : Loss 3739.1203\n",
      "Iteration # 16870 : Loss 3738.5500\n",
      "Iteration # 16880 : Loss 3737.9804\n",
      "Iteration # 16890 : Loss 3737.4116\n",
      "Iteration # 16900 : Loss 3736.8437\n",
      "Iteration # 16910 : Loss 3736.2765\n",
      "Iteration # 16920 : Loss 3735.7102\n",
      "Iteration # 16930 : Loss 3735.1446\n",
      "Iteration # 16940 : Loss 3734.5798\n",
      "Iteration # 16950 : Loss 3734.0158\n",
      "Iteration # 16960 : Loss 3733.4527\n",
      "Iteration # 16970 : Loss 3732.8903\n",
      "Iteration # 16980 : Loss 3732.3287\n",
      "Iteration # 16990 : Loss 3731.7678\n",
      "Iteration # 17000 : Loss 3731.2078\n",
      "Iteration # 17010 : Loss 3730.6485\n",
      "Iteration # 17020 : Loss 3730.0900\n",
      "Iteration # 17030 : Loss 3729.5323\n",
      "Iteration # 17040 : Loss 3728.9754\n",
      "Iteration # 17050 : Loss 3728.4192\n",
      "Iteration # 17060 : Loss 3727.8638\n",
      "Iteration # 17070 : Loss 3727.3092\n",
      "Iteration # 17080 : Loss 3726.7553\n",
      "Iteration # 17090 : Loss 3726.2022\n",
      "Iteration # 17100 : Loss 3725.6499\n",
      "Iteration # 17110 : Loss 3725.0983\n",
      "Iteration # 17120 : Loss 3724.5475\n",
      "Iteration # 17130 : Loss 3723.9975\n",
      "Iteration # 17140 : Loss 3723.4482\n",
      "Iteration # 17150 : Loss 3722.8996\n",
      "Iteration # 17160 : Loss 3722.3518\n",
      "Iteration # 17170 : Loss 3721.8048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 17180 : Loss 3721.2585\n",
      "Iteration # 17190 : Loss 3720.7129\n",
      "Iteration # 17200 : Loss 3720.1681\n",
      "Iteration # 17210 : Loss 3719.6240\n",
      "Iteration # 17220 : Loss 3719.0807\n",
      "Iteration # 17230 : Loss 3718.5381\n",
      "Iteration # 17240 : Loss 3717.9962\n",
      "Iteration # 17250 : Loss 3717.4551\n",
      "Iteration # 17260 : Loss 3716.9147\n",
      "Iteration # 17270 : Loss 3716.3750\n",
      "Iteration # 17280 : Loss 3715.8361\n",
      "Iteration # 17290 : Loss 3715.2979\n",
      "Iteration # 17300 : Loss 3714.7604\n",
      "Iteration # 17310 : Loss 3714.2236\n",
      "Iteration # 17320 : Loss 3713.6876\n",
      "Iteration # 17330 : Loss 3713.1523\n",
      "Iteration # 17340 : Loss 3712.6176\n",
      "Iteration # 17350 : Loss 3712.0838\n",
      "Iteration # 17360 : Loss 3711.5506\n",
      "Iteration # 17370 : Loss 3711.0181\n",
      "Iteration # 17380 : Loss 3710.4863\n",
      "Iteration # 17390 : Loss 3709.9553\n",
      "Iteration # 17400 : Loss 3709.4250\n",
      "Iteration # 17410 : Loss 3708.8953\n",
      "Iteration # 17420 : Loss 3708.3664\n",
      "Iteration # 17430 : Loss 3707.8382\n",
      "Iteration # 17440 : Loss 3707.3106\n",
      "Iteration # 17450 : Loss 3706.7838\n",
      "Iteration # 17460 : Loss 3706.2576\n",
      "Iteration # 17470 : Loss 3705.7322\n",
      "Iteration # 17480 : Loss 3705.2074\n",
      "Iteration # 17490 : Loss 3704.6834\n",
      "Iteration # 17500 : Loss 3704.1600\n",
      "Iteration # 17510 : Loss 3703.6373\n",
      "Iteration # 17520 : Loss 3703.1153\n",
      "Iteration # 17530 : Loss 3702.5940\n",
      "Iteration # 17540 : Loss 3702.0734\n",
      "Iteration # 17550 : Loss 3701.5534\n",
      "Iteration # 17560 : Loss 3701.0342\n",
      "Iteration # 17570 : Loss 3700.5156\n",
      "Iteration # 17580 : Loss 3699.9976\n",
      "Iteration # 17590 : Loss 3699.4804\n",
      "Iteration # 17600 : Loss 3698.9638\n",
      "Iteration # 17610 : Loss 3698.4479\n",
      "Iteration # 17620 : Loss 3697.9327\n",
      "Iteration # 17630 : Loss 3697.4181\n",
      "Iteration # 17640 : Loss 3696.9042\n",
      "Iteration # 17650 : Loss 3696.3910\n",
      "Iteration # 17660 : Loss 3695.8784\n",
      "Iteration # 17670 : Loss 3695.3665\n",
      "Iteration # 17680 : Loss 3694.8553\n",
      "Iteration # 17690 : Loss 3694.3447\n",
      "Iteration # 17700 : Loss 3693.8347\n",
      "Iteration # 17710 : Loss 3693.3254\n",
      "Iteration # 17720 : Loss 3692.8168\n",
      "Iteration # 17730 : Loss 3692.3088\n",
      "Iteration # 17740 : Loss 3691.8015\n",
      "Iteration # 17750 : Loss 3691.2948\n",
      "Iteration # 17760 : Loss 3690.7888\n",
      "Iteration # 17770 : Loss 3690.2834\n",
      "Iteration # 17780 : Loss 3689.7787\n",
      "Iteration # 17790 : Loss 3689.2746\n",
      "Iteration # 17800 : Loss 3688.7711\n",
      "Iteration # 17810 : Loss 3688.2683\n",
      "Iteration # 17820 : Loss 3687.7661\n",
      "Iteration # 17830 : Loss 3687.2645\n",
      "Iteration # 17840 : Loss 3686.7636\n",
      "Iteration # 17850 : Loss 3686.2633\n",
      "Iteration # 17860 : Loss 3685.7637\n",
      "Iteration # 17870 : Loss 3685.2647\n",
      "Iteration # 17880 : Loss 3684.7663\n",
      "Iteration # 17890 : Loss 3684.2685\n",
      "Iteration # 17900 : Loss 3683.7714\n",
      "Iteration # 17910 : Loss 3683.2749\n",
      "Iteration # 17920 : Loss 3682.7790\n",
      "Iteration # 17930 : Loss 3682.2837\n",
      "Iteration # 17940 : Loss 3681.7891\n",
      "Iteration # 17950 : Loss 3681.2950\n",
      "Iteration # 17960 : Loss 3680.8016\n",
      "Iteration # 17970 : Loss 3680.3088\n",
      "Iteration # 17980 : Loss 3679.8166\n",
      "Iteration # 17990 : Loss 3679.3250\n",
      "Iteration # 18000 : Loss 3678.8341\n",
      "Iteration # 18010 : Loss 3678.3437\n",
      "Iteration # 18020 : Loss 3677.8540\n",
      "Iteration # 18030 : Loss 3677.3648\n",
      "Iteration # 18040 : Loss 3676.8763\n",
      "Iteration # 18050 : Loss 3676.3884\n",
      "Iteration # 18060 : Loss 3675.9010\n",
      "Iteration # 18070 : Loss 3675.4143\n",
      "Iteration # 18080 : Loss 3674.9282\n",
      "Iteration # 18090 : Loss 3674.4426\n",
      "Iteration # 18100 : Loss 3673.9577\n",
      "Iteration # 18110 : Loss 3673.4734\n",
      "Iteration # 18120 : Loss 3672.9896\n",
      "Iteration # 18130 : Loss 3672.5065\n",
      "Iteration # 18140 : Loss 3672.0239\n",
      "Iteration # 18150 : Loss 3671.5419\n",
      "Iteration # 18160 : Loss 3671.0605\n",
      "Iteration # 18170 : Loss 3670.5797\n",
      "Iteration # 18180 : Loss 3670.0995\n",
      "Iteration # 18190 : Loss 3669.6199\n",
      "Iteration # 18200 : Loss 3669.1409\n",
      "Iteration # 18210 : Loss 3668.6624\n",
      "Iteration # 18220 : Loss 3668.1845\n",
      "Iteration # 18230 : Loss 3667.7072\n",
      "Iteration # 18240 : Loss 3667.2305\n",
      "Iteration # 18250 : Loss 3666.7543\n",
      "Iteration # 18260 : Loss 3666.2788\n",
      "Iteration # 18270 : Loss 3665.8038\n",
      "Iteration # 18280 : Loss 3665.3293\n",
      "Iteration # 18290 : Loss 3664.8555\n",
      "Iteration # 18300 : Loss 3664.3822\n",
      "Iteration # 18310 : Loss 3663.9095\n",
      "Iteration # 18320 : Loss 3663.4373\n",
      "Iteration # 18330 : Loss 3662.9657\n",
      "Iteration # 18340 : Loss 3662.4947\n",
      "Iteration # 18350 : Loss 3662.0242\n",
      "Iteration # 18360 : Loss 3661.5543\n",
      "Iteration # 18370 : Loss 3661.0850\n",
      "Iteration # 18380 : Loss 3660.6162\n",
      "Iteration # 18390 : Loss 3660.1480\n",
      "Iteration # 18400 : Loss 3659.6804\n",
      "Iteration # 18410 : Loss 3659.2132\n",
      "Iteration # 18420 : Loss 3658.7467\n",
      "Iteration # 18430 : Loss 3658.2807\n",
      "Iteration # 18440 : Loss 3657.8153\n",
      "Iteration # 18450 : Loss 3657.3504\n",
      "Iteration # 18460 : Loss 3656.8860\n",
      "Iteration # 18470 : Loss 3656.4222\n",
      "Iteration # 18480 : Loss 3655.9590\n",
      "Iteration # 18490 : Loss 3655.4962\n",
      "Iteration # 18500 : Loss 3655.0341\n",
      "Iteration # 18510 : Loss 3654.5725\n",
      "Iteration # 18520 : Loss 3654.1114\n",
      "Iteration # 18530 : Loss 3653.6509\n",
      "Iteration # 18540 : Loss 3653.1909\n",
      "Iteration # 18550 : Loss 3652.7314\n",
      "Iteration # 18560 : Loss 3652.2725\n",
      "Iteration # 18570 : Loss 3651.8141\n",
      "Iteration # 18580 : Loss 3651.3562\n",
      "Iteration # 18590 : Loss 3650.8989\n",
      "Iteration # 18600 : Loss 3650.4421\n",
      "Iteration # 18610 : Loss 3649.9859\n",
      "Iteration # 18620 : Loss 3649.5302\n",
      "Iteration # 18630 : Loss 3649.0750\n",
      "Iteration # 18640 : Loss 3648.6203\n",
      "Iteration # 18650 : Loss 3648.1662\n",
      "Iteration # 18660 : Loss 3647.7126\n",
      "Iteration # 18670 : Loss 3647.2595\n",
      "Iteration # 18680 : Loss 3646.8069\n",
      "Iteration # 18690 : Loss 3646.3549\n",
      "Iteration # 18700 : Loss 3645.9033\n",
      "Iteration # 18710 : Loss 3645.4523\n",
      "Iteration # 18720 : Loss 3645.0018\n",
      "Iteration # 18730 : Loss 3644.5519\n",
      "Iteration # 18740 : Loss 3644.1024\n",
      "Iteration # 18750 : Loss 3643.6535\n",
      "Iteration # 18760 : Loss 3643.2051\n",
      "Iteration # 18770 : Loss 3642.7571\n",
      "Iteration # 18780 : Loss 3642.3097\n",
      "Iteration # 18790 : Loss 3641.8629\n",
      "Iteration # 18800 : Loss 3641.4165\n",
      "Iteration # 18810 : Loss 3640.9706\n",
      "Iteration # 18820 : Loss 3640.5253\n",
      "Iteration # 18830 : Loss 3640.0804\n",
      "Iteration # 18840 : Loss 3639.6360\n",
      "Iteration # 18850 : Loss 3639.1922\n",
      "Iteration # 18860 : Loss 3638.7489\n",
      "Iteration # 18870 : Loss 3638.3060\n",
      "Iteration # 18880 : Loss 3637.8637\n",
      "Iteration # 18890 : Loss 3637.4218\n",
      "Iteration # 18900 : Loss 3636.9805\n",
      "Iteration # 18910 : Loss 3636.5396\n",
      "Iteration # 18920 : Loss 3636.0993\n",
      "Iteration # 18930 : Loss 3635.6594\n",
      "Iteration # 18940 : Loss 3635.2201\n",
      "Iteration # 18950 : Loss 3634.7812\n",
      "Iteration # 18960 : Loss 3634.3428\n",
      "Iteration # 18970 : Loss 3633.9049\n",
      "Iteration # 18980 : Loss 3633.4676\n",
      "Iteration # 18990 : Loss 3633.0306\n",
      "Iteration # 19000 : Loss 3632.5942\n",
      "Iteration # 19010 : Loss 3632.1583\n",
      "Iteration # 19020 : Loss 3631.7229\n",
      "Iteration # 19030 : Loss 3631.2879\n",
      "Iteration # 19040 : Loss 3630.8534\n",
      "Iteration # 19050 : Loss 3630.4194\n",
      "Iteration # 19060 : Loss 3629.9859\n",
      "Iteration # 19070 : Loss 3629.5529\n",
      "Iteration # 19080 : Loss 3629.1203\n",
      "Iteration # 19090 : Loss 3628.6883\n",
      "Iteration # 19100 : Loss 3628.2567\n",
      "Iteration # 19110 : Loss 3627.8256\n",
      "Iteration # 19120 : Loss 3627.3949\n",
      "Iteration # 19130 : Loss 3626.9648\n",
      "Iteration # 19140 : Loss 3626.5351\n",
      "Iteration # 19150 : Loss 3626.1059\n",
      "Iteration # 19160 : Loss 3625.6771\n",
      "Iteration # 19170 : Loss 3625.2488\n",
      "Iteration # 19180 : Loss 3624.8210\n",
      "Iteration # 19190 : Loss 3624.3937\n",
      "Iteration # 19200 : Loss 3623.9668\n",
      "Iteration # 19210 : Loss 3623.5404\n",
      "Iteration # 19220 : Loss 3623.1145\n",
      "Iteration # 19230 : Loss 3622.6890\n",
      "Iteration # 19240 : Loss 3622.2640\n",
      "Iteration # 19250 : Loss 3621.8395\n",
      "Iteration # 19260 : Loss 3621.4154\n",
      "Iteration # 19270 : Loss 3620.9917\n",
      "Iteration # 19280 : Loss 3620.5686\n",
      "Iteration # 19290 : Loss 3620.1459\n",
      "Iteration # 19300 : Loss 3619.7236\n",
      "Iteration # 19310 : Loss 3619.3019\n",
      "Iteration # 19320 : Loss 3618.8805\n",
      "Iteration # 19330 : Loss 3618.4596\n",
      "Iteration # 19340 : Loss 3618.0392\n",
      "Iteration # 19350 : Loss 3617.6193\n",
      "Iteration # 19360 : Loss 3617.1997\n",
      "Iteration # 19370 : Loss 3616.7807\n",
      "Iteration # 19380 : Loss 3616.3621\n",
      "Iteration # 19390 : Loss 3615.9439\n",
      "Iteration # 19400 : Loss 3615.5262\n",
      "Iteration # 19410 : Loss 3615.1089\n",
      "Iteration # 19420 : Loss 3614.6921\n",
      "Iteration # 19430 : Loss 3614.2757\n",
      "Iteration # 19440 : Loss 3613.8598\n",
      "Iteration # 19450 : Loss 3613.4443\n",
      "Iteration # 19460 : Loss 3613.0293\n",
      "Iteration # 19470 : Loss 3612.6147\n",
      "Iteration # 19480 : Loss 3612.2005\n",
      "Iteration # 19490 : Loss 3611.7868\n",
      "Iteration # 19500 : Loss 3611.3736\n",
      "Iteration # 19510 : Loss 3610.9607\n",
      "Iteration # 19520 : Loss 3610.5483\n",
      "Iteration # 19530 : Loss 3610.1364\n",
      "Iteration # 19540 : Loss 3609.7248\n",
      "Iteration # 19550 : Loss 3609.3137\n",
      "Iteration # 19560 : Loss 3608.9031\n",
      "Iteration # 19570 : Loss 3608.4929\n",
      "Iteration # 19580 : Loss 3608.0831\n",
      "Iteration # 19590 : Loss 3607.6737\n",
      "Iteration # 19600 : Loss 3607.2648\n",
      "Iteration # 19610 : Loss 3606.8563\n",
      "Iteration # 19620 : Loss 3606.4482\n",
      "Iteration # 19630 : Loss 3606.0406\n",
      "Iteration # 19640 : Loss 3605.6334\n",
      "Iteration # 19650 : Loss 3605.2266\n",
      "Iteration # 19660 : Loss 3604.8203\n",
      "Iteration # 19670 : Loss 3604.4143\n",
      "Iteration # 19680 : Loss 3604.0088\n",
      "Iteration # 19690 : Loss 3603.6037\n",
      "Iteration # 19700 : Loss 3603.1991\n",
      "Iteration # 19710 : Loss 3602.7948\n",
      "Iteration # 19720 : Loss 3602.3910\n",
      "Iteration # 19730 : Loss 3601.9876\n",
      "Iteration # 19740 : Loss 3601.5847\n",
      "Iteration # 19750 : Loss 3601.1821\n",
      "Iteration # 19760 : Loss 3600.7799\n",
      "Iteration # 19770 : Loss 3600.3782\n",
      "Iteration # 19780 : Loss 3599.9769\n",
      "Iteration # 19790 : Loss 3599.5760\n",
      "Iteration # 19800 : Loss 3599.1755\n",
      "Iteration # 19810 : Loss 3598.7755\n",
      "Iteration # 19820 : Loss 3598.3758\n",
      "Iteration # 19830 : Loss 3597.9766\n",
      "Iteration # 19840 : Loss 3597.5777\n",
      "Iteration # 19850 : Loss 3597.1793\n",
      "Iteration # 19860 : Loss 3596.7813\n",
      "Iteration # 19870 : Loss 3596.3837\n",
      "Iteration # 19880 : Loss 3595.9865\n",
      "Iteration # 19890 : Loss 3595.5897\n",
      "Iteration # 19900 : Loss 3595.1933\n",
      "Iteration # 19910 : Loss 3594.7974\n",
      "Iteration # 19920 : Loss 3594.4018\n",
      "Iteration # 19930 : Loss 3594.0066\n",
      "Iteration # 19940 : Loss 3593.6119\n",
      "Iteration # 19950 : Loss 3593.2175\n",
      "Iteration # 19960 : Loss 3592.8235\n",
      "Iteration # 19970 : Loss 3592.4300\n",
      "Iteration # 19980 : Loss 3592.0368\n",
      "Iteration # 19990 : Loss 3591.6441\n",
      "Iteration # 20000 : Loss 3591.2517\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(1, 20001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X, W, b, y)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration # %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  56.58524518,  -25.32717535,  221.82312882,  159.65116699,\n",
       "          31.05852955,   10.61157797, -125.18310103,  119.99000119,\n",
       "         187.72542319,  123.12518667]),\n",
       " 149.90112186991888)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W, b 확인\n",
    "W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10) test 데이터에 대한 성능 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3248.369053242001"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (11) 정답 데이터와 예측한 데이터 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test[:, 0], y_test, label=\"true\")\n",
    "plt.scatter(X_test[:, 0], prediction, label=\"pred\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 2 : 맑은 날 자전거 타는 사람 머릿수 추측하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kaggle 데이터넷 가져오기\n",
    "- 시간, 온도, 습도, 계절 등의 정보가 담긴 데이터를 통해 자전거의 대여량을 예측하는 문제\n",
    "\n",
    "    → 시간, 요일, 계절 모두 영향을 미칠 것\n",
    "\n",
    "- 직접 손실함수와 기울기를 계산하지 않고 `sklearn`의 LinearRegression 모델 활용할 것(순서는 위와 그대로)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) datetime 컬럼을 datetime 자료형으로 변환하고 연, 월, 일, 시, 분, 초까지 6가지 컬럼 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) year, month, day, hour, minute, second 데이터 개수 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) X, y 컬럼 선택 및 train/test 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) LinearRegression 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) 학습된 모델로 X_test에 대한 예측값 출력 및 손실함수값 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) x축은 temp 또는 humidity로, y축은 count로 예측 결과 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
