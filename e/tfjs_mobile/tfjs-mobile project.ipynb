{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E13 project\n",
    "---\n",
    "## step1. 표정 데이터로 mobilenet 학습시키기\n",
    "- 데이터셋 구경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35887/35887 [00:26<00:00, 1363.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 32298, eval :3589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "eval_x = []\n",
    "eval_y = []\n",
    "\n",
    "csv_path = os.getenv('HOME')+'/aiffel/e/tfjs_mobile/data/fer2013.csv'\n",
    "\n",
    "with open(csv_path) as f:\n",
    "  for line in tqdm.tqdm(f.read().splitlines()[1:]):\n",
    "    emotion, pixels, usage = line.split(',')\n",
    "    \n",
    "    x = np.array(pixels.split(' ')).astype(float).reshape(48, 48, 1) / 255\n",
    "    y = int(emotion)\n",
    "\n",
    "    if usage == 'PrivateTest':\n",
    "      eval_x.append(x)\n",
    "      eval_y.append(y)\n",
    "    else:\n",
    "      train_x.append(x)\n",
    "      train_y.append(y)\n",
    "\n",
    "print('train : {}, eval :{}'.format(len(train_x), len(eval_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 완비 확인을 위해 첫 데이터의 이미지, 라벨을 확인한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angry\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9162ffcd50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "LABELS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "print(LABELS[train_y[0]])\n",
    "plt.imshow(train_x[0].reshape([48, 48]), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 epoch만 학습하여 MobileNetV2 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 85s 1s/step - loss: 1.8357 - categorical_accuracy: 0.2403 - val_loss: 1.9364 - val_categorical_accuracy: 0.1655\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 6s 97ms/step - loss: 1.6993 - categorical_accuracy: 0.3175 - val_loss: 1.9263 - val_categorical_accuracy: 0.1655\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 6s 96ms/step - loss: 1.5752 - categorical_accuracy: 0.3835 - val_loss: 1.9136 - val_categorical_accuracy: 0.1655\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 6s 97ms/step - loss: 1.4724 - categorical_accuracy: 0.4343 - val_loss: 1.9036 - val_categorical_accuracy: 0.1655\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 6s 97ms/step - loss: 1.3892 - categorical_accuracy: 0.4693 - val_loss: 1.8984 - val_categorical_accuracy: 0.1655\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 6s 97ms/step - loss: 1.3075 - categorical_accuracy: 0.5079 - val_loss: 1.8999 - val_categorical_accuracy: 0.1655\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 6s 98ms/step - loss: 1.2457 - categorical_accuracy: 0.5319 - val_loss: 1.9007 - val_categorical_accuracy: 0.1655\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 6s 91ms/step - loss: 1.1983 - categorical_accuracy: 0.5531 - val_loss: 1.8978 - val_categorical_accuracy: 0.1655\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 1.1304 - categorical_accuracy: 0.5795 - val_loss: 1.9116 - val_categorical_accuracy: 0.1655\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 1.0660 - categorical_accuracy: 0.6051 - val_loss: 1.9166 - val_categorical_accuracy: 0.1655\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 1.0123 - categorical_accuracy: 0.6243 - val_loss: 1.9239 - val_categorical_accuracy: 0.1655\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.9635 - categorical_accuracy: 0.6479 - val_loss: 1.9271 - val_categorical_accuracy: 0.1655\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.8959 - categorical_accuracy: 0.6734 - val_loss: 1.9444 - val_categorical_accuracy: 0.1655\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.8720 - categorical_accuracy: 0.6792 - val_loss: 1.9720 - val_categorical_accuracy: 0.1655\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.8107 - categorical_accuracy: 0.7060 - val_loss: 1.9849 - val_categorical_accuracy: 0.1655\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.8004 - categorical_accuracy: 0.7112 - val_loss: 1.9908 - val_categorical_accuracy: 0.1655\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 6s 89ms/step - loss: 0.7019 - categorical_accuracy: 0.7448 - val_loss: 2.0239 - val_categorical_accuracy: 0.1655\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.6657 - categorical_accuracy: 0.7619 - val_loss: 2.0355 - val_categorical_accuracy: 0.1655\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.6224 - categorical_accuracy: 0.7768 - val_loss: 2.0554 - val_categorical_accuracy: 0.1655\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 6s 89ms/step - loss: 0.5829 - categorical_accuracy: 0.7912 - val_loss: 2.0643 - val_categorical_accuracy: 0.1655\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.5225 - categorical_accuracy: 0.8126 - val_loss: 2.0843 - val_categorical_accuracy: 0.1655\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.5136 - categorical_accuracy: 0.8203 - val_loss: 2.0857 - val_categorical_accuracy: 0.1655\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.4658 - categorical_accuracy: 0.8338 - val_loss: 2.1309 - val_categorical_accuracy: 0.1655\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 6s 89ms/step - loss: 0.4439 - categorical_accuracy: 0.8407 - val_loss: 2.1517 - val_categorical_accuracy: 0.1655\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.4604 - categorical_accuracy: 0.8346 - val_loss: 2.1946 - val_categorical_accuracy: 0.1655\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.3548 - categorical_accuracy: 0.8733 - val_loss: 2.2308 - val_categorical_accuracy: 0.1655\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 5s 85ms/step - loss: 0.3692 - categorical_accuracy: 0.8682 - val_loss: 2.2442 - val_categorical_accuracy: 0.1655\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 5s 85ms/step - loss: 0.3155 - categorical_accuracy: 0.8864 - val_loss: 2.2887 - val_categorical_accuracy: 0.1655\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 6s 92ms/step - loss: 0.2870 - categorical_accuracy: 0.8982 - val_loss: 2.2795 - val_categorical_accuracy: 0.1655\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 6s 89ms/step - loss: 0.2772 - categorical_accuracy: 0.9021 - val_loss: 2.2665 - val_categorical_accuracy: 0.1655\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.2673 - categorical_accuracy: 0.9041 - val_loss: 2.2940 - val_categorical_accuracy: 0.1655\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.2935 - categorical_accuracy: 0.8947 - val_loss: 2.2807 - val_categorical_accuracy: 0.1655\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2511 - categorical_accuracy: 0.9109 - val_loss: 2.3381 - val_categorical_accuracy: 0.1655\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2382 - categorical_accuracy: 0.9162 - val_loss: 2.3240 - val_categorical_accuracy: 0.1655\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 6s 89ms/step - loss: 0.2168 - categorical_accuracy: 0.9233 - val_loss: 2.4402 - val_categorical_accuracy: 0.1655\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2314 - categorical_accuracy: 0.9178 - val_loss: 2.4269 - val_categorical_accuracy: 0.1655\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2304 - categorical_accuracy: 0.9189 - val_loss: 2.4436 - val_categorical_accuracy: 0.1655\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2923 - categorical_accuracy: 0.8969 - val_loss: 2.3691 - val_categorical_accuracy: 0.1655\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 6s 89ms/step - loss: 0.2085 - categorical_accuracy: 0.9285 - val_loss: 2.4329 - val_categorical_accuracy: 0.1655\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2921 - categorical_accuracy: 0.8956 - val_loss: 2.4229 - val_categorical_accuracy: 0.1655\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2003 - categorical_accuracy: 0.9285 - val_loss: 2.5505 - val_categorical_accuracy: 0.1655\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2155 - categorical_accuracy: 0.9235 - val_loss: 2.4630 - val_categorical_accuracy: 0.1655\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1978 - categorical_accuracy: 0.9305 - val_loss: 2.5394 - val_categorical_accuracy: 0.1655\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1718 - categorical_accuracy: 0.9410 - val_loss: 2.4886 - val_categorical_accuracy: 0.1655\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2075 - categorical_accuracy: 0.9273 - val_loss: 2.4754 - val_categorical_accuracy: 0.1655\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1752 - categorical_accuracy: 0.9400 - val_loss: 2.4720 - val_categorical_accuracy: 0.1655\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2246 - categorical_accuracy: 0.9230 - val_loss: 2.5354 - val_categorical_accuracy: 0.1655\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1768 - categorical_accuracy: 0.9383 - val_loss: 2.5639 - val_categorical_accuracy: 0.1655\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1291 - categorical_accuracy: 0.9552 - val_loss: 2.6333 - val_categorical_accuracy: 0.1655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.1374 - categorical_accuracy: 0.9527 - val_loss: 2.5215 - val_categorical_accuracy: 0.1655\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1873 - categorical_accuracy: 0.9345 - val_loss: 2.4147 - val_categorical_accuracy: 0.1655\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1217 - categorical_accuracy: 0.9583 - val_loss: 2.5333 - val_categorical_accuracy: 0.1655\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2135 - categorical_accuracy: 0.9281 - val_loss: 2.4234 - val_categorical_accuracy: 0.1655\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.2019 - categorical_accuracy: 0.9295 - val_loss: 2.7250 - val_categorical_accuracy: 0.1655\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1906 - categorical_accuracy: 0.9332 - val_loss: 2.6115 - val_categorical_accuracy: 0.1655\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1650 - categorical_accuracy: 0.9431 - val_loss: 2.6938 - val_categorical_accuracy: 0.1655\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1247 - categorical_accuracy: 0.9575 - val_loss: 2.4798 - val_categorical_accuracy: 0.1658\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.1201 - categorical_accuracy: 0.9578 - val_loss: 2.5357 - val_categorical_accuracy: 0.1677\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.1684 - categorical_accuracy: 0.9426 - val_loss: 2.3764 - val_categorical_accuracy: 0.1655\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.1873 - categorical_accuracy: 0.9365 - val_loss: 2.4711 - val_categorical_accuracy: 0.1817\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.1545 - categorical_accuracy: 0.9468 - val_loss: 2.5018 - val_categorical_accuracy: 0.1688\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.1117 - categorical_accuracy: 0.9618 - val_loss: 2.3809 - val_categorical_accuracy: 0.1705\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1276 - categorical_accuracy: 0.9571 - val_loss: 2.2655 - val_categorical_accuracy: 0.2090\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.1244 - categorical_accuracy: 0.9584 - val_loss: 2.3325 - val_categorical_accuracy: 0.2048\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.1668 - categorical_accuracy: 0.9434 - val_loss: 2.3682 - val_categorical_accuracy: 0.2299\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1430 - categorical_accuracy: 0.9511 - val_loss: 2.2531 - val_categorical_accuracy: 0.2970\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0803 - categorical_accuracy: 0.9730 - val_loss: 2.2844 - val_categorical_accuracy: 0.2937\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1628 - categorical_accuracy: 0.9441 - val_loss: 2.2597 - val_categorical_accuracy: 0.2862\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0671 - categorical_accuracy: 0.9777 - val_loss: 2.4933 - val_categorical_accuracy: 0.2630\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.0807 - categorical_accuracy: 0.9736 - val_loss: 2.4946 - val_categorical_accuracy: 0.2569\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1747 - categorical_accuracy: 0.9397 - val_loss: 2.3838 - val_categorical_accuracy: 0.2733\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1078 - categorical_accuracy: 0.9626 - val_loss: 2.4537 - val_categorical_accuracy: 0.2898\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.1045 - categorical_accuracy: 0.9639 - val_loss: 2.5842 - val_categorical_accuracy: 0.2912\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2421 - categorical_accuracy: 0.9163 - val_loss: 2.7740 - val_categorical_accuracy: 0.2903\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0964 - categorical_accuracy: 0.9674 - val_loss: 3.0413 - val_categorical_accuracy: 0.2914\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1655 - categorical_accuracy: 0.9428 - val_loss: 2.8492 - val_categorical_accuracy: 0.3305\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 6s 89ms/step - loss: 0.0864 - categorical_accuracy: 0.9711 - val_loss: 3.0386 - val_categorical_accuracy: 0.3388\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0838 - categorical_accuracy: 0.9713 - val_loss: 3.1724 - val_categorical_accuracy: 0.3224\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0961 - categorical_accuracy: 0.9670 - val_loss: 3.0911 - val_categorical_accuracy: 0.3497\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0749 - categorical_accuracy: 0.9740 - val_loss: 3.3037 - val_categorical_accuracy: 0.3647\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0924 - categorical_accuracy: 0.9684 - val_loss: 3.4187 - val_categorical_accuracy: 0.3628\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1354 - categorical_accuracy: 0.9516 - val_loss: 3.7360 - val_categorical_accuracy: 0.3725\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.2400 - categorical_accuracy: 0.9161 - val_loss: 3.3906 - val_categorical_accuracy: 0.3984\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0902 - categorical_accuracy: 0.9688 - val_loss: 3.6117 - val_categorical_accuracy: 0.4079\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0852 - categorical_accuracy: 0.9710 - val_loss: 3.8902 - val_categorical_accuracy: 0.3787\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1340 - categorical_accuracy: 0.9536 - val_loss: 3.6693 - val_categorical_accuracy: 0.4221\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1233 - categorical_accuracy: 0.9582 - val_loss: 3.7429 - val_categorical_accuracy: 0.4366\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.1052 - categorical_accuracy: 0.9635 - val_loss: 3.7369 - val_categorical_accuracy: 0.4316\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1570 - categorical_accuracy: 0.9454 - val_loss: 3.8845 - val_categorical_accuracy: 0.4335\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0761 - categorical_accuracy: 0.9743 - val_loss: 3.9854 - val_categorical_accuracy: 0.4361\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1213 - categorical_accuracy: 0.9584 - val_loss: 4.0396 - val_categorical_accuracy: 0.4288\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1073 - categorical_accuracy: 0.9626 - val_loss: 4.4658 - val_categorical_accuracy: 0.4296\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0785 - categorical_accuracy: 0.9722 - val_loss: 4.3622 - val_categorical_accuracy: 0.4436\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0689 - categorical_accuracy: 0.9759 - val_loss: 4.6873 - val_categorical_accuracy: 0.4238\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1168 - categorical_accuracy: 0.9588 - val_loss: 4.6996 - val_categorical_accuracy: 0.4152\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.0763 - categorical_accuracy: 0.9738 - val_loss: 4.8258 - val_categorical_accuracy: 0.4054\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 0.1037 - categorical_accuracy: 0.9630 - val_loss: 4.7586 - val_categorical_accuracy: 0.4263\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 6s 92ms/step - loss: 0.0999 - categorical_accuracy: 0.9655 - val_loss: 5.2312 - val_categorical_accuracy: 0.4244\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 86ms/step - loss: 0.1053 - categorical_accuracy: 0.9640 - val_loss: 4.9334 - val_categorical_accuracy: 0.4224\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 6s 87ms/step - loss: 0.0474 - categorical_accuracy: 0.9839 - val_loss: 4.8639 - val_categorical_accuracy: 0.4263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f900457b3d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "# model 선언 \n",
    "model = tf.keras.applications.MobileNetV2(input_shape=(48, 48, 1), weights=None, classes=7)\n",
    "\n",
    "# model.compile\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "#model.fit\n",
    "model.fit(np.stack(train_x),\n",
    "          tf.keras.utils.to_categorical(train_y),\n",
    "          epochs=100,\n",
    "          batch_size=512,\n",
    "          validation_data=(np.stack(eval_x),\n",
    "                           tf.keras.utils.to_categorical(eval_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2. 학습된 모델을 TensorFlow.js 형식으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_path = os.getenv('HOME')+'/aiffel/e/tfjs_mobile/model.h5'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cp_path = os.getenv('HOME')+'/aiffel/e/tfjs_mobile/model_cp.h5'\n",
    "model.save(model_cp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install tensorflowjs`로 설치하여 tf.js가 활용할 수 있는 형태로 변환하기    \n",
    "터미널에서 `tensorflowjs_converter` 명령어만 실행    \n",
    "```shell\n",
    "$ cd ~/aiffel/tfjs_mobile\n",
    "$ tensorflowjs_converter --input_format=keras model.h5 model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3. 카메라에서 가져온 영상 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow.js mobile project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-a9a75d56d272>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-a9a75d56d272>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ---\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# E13 project\n",
    "---\n",
    "## step1. 표정 데이터로 mobilenet 학습시키기\n",
    "- 데이터셋 구경하기# E13 project\n",
    "---\n",
    "## step1. 표정 데이터로 mobilenet 학습시키기\n",
    "- 데이터셋 구경하기\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "eval_x = []\n",
    "eval_y = []\n",
    "\n",
    "csv_path = os.getenv('HOME')+'/aiffel/e/tfjs_mobile/data/fer2013.csv'\n",
    "\n",
    "with open(csv_path) as f:\n",
    "  for line in tqdm.tqdm(f.read().splitlines()[1:]):\n",
    "    emotion, pixels, usage = line.split(',')\n",
    "    \n",
    "    x = np.array(pixels.split(' ')).astype(float).reshape(48, 48, 1) / 255\n",
    "    y = int(emotion)\n",
    "\n",
    "    if usage == 'PrivateTest':\n",
    "      eval_x.append(x)\n",
    "      eval_y.append(y)\n",
    "    else:\n",
    "      train_x.append(x)\n",
    "      train_y.append(y)\n",
    "\n",
    "print('train : {}, eval :{}'.format(len(train_x), len(eval_x)))\n",
    "\n",
    "데이터 완비 확인을 위해 첫 데이터의 이미지, 라벨을 확인한다\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "LABELS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "print(LABELS[train_y[0]])\n",
    "plt.imshow(train_x[0].reshape([48, 48]), cmap='gray')\n",
    "\n",
    "100 epoch만 학습하여 MobileNetV2 모델 학습하기\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "# model 선언 \n",
    "model = tf.keras.applications.MobileNetV2(input_shape=(48, 48, 1), weights=None, classes=7)\n",
    "\n",
    "# model.compile\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "#model.fit\n",
    "model.fit(np.stack(train_x),\n",
    "          tf.keras.utils.to_categorical(train_y),\n",
    "          epochs=100,\n",
    "          batch_size=512,\n",
    "          validation_data=(np.stack(eval_x),\n",
    "                           tf.keras.utils.to_categorical(eval_y)))\n",
    "\n",
    "## step 2. 학습된 모델을 TensorFlow.js 형식으로 변환하기\n",
    "\n",
    "import os\n",
    "model_path = os.getenv('HOME')+'/aiffel/e/tfjs_mobile/model.h5'\n",
    "model.save(model_path)\n",
    "\n",
    "`pip install tensorflowjs`로 설치하여 tf.js가 활용할 수 있는 형태로 변환하기    \n",
    "터미널에서 `tensorflowjs_converter` 명령어만 실행    \n",
    "```shell\n",
    "$ cd ~/aiffel/tfjs_mobile\n",
    "$ tensorflowjs_converter --input_format=keras model.h5 model\n",
    "```\n",
    "\n",
    "## step 3. 카메라에서 가져온 영상 전처리하기\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel-cp",
   "language": "python",
   "name": "aiffelcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
