{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E08_캐글 경진대회 무작정 따라하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/kaggle-kakr-housing-data.zip\n",
    "$ mv kaggle-kakr-housing-data.zip ~/aiffel/e/e08_kaggle_kakr_housing\n",
    "$ cd ~/aiffel/e/e08_kaggle_kakr_housing\n",
    "$ unzip kaggle-kakr-housing-data.zip\n",
    "\n",
    "# 중간에 데이터 변동이 있어 그냥 제출시 길이 안맞는 에러가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 모델 - setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화를 위한 matplotlib import \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# 필요 라이브러리 import\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('얍💢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 지정\n",
    "data_dir = os.getenv('HOME')+'/aiffel/e/e08_kaggle_kakr_housing/data'\n",
    "\n",
    "train_data_path = join(data_dir, 'train.csv')\n",
    "sub_data_path = join(data_dir, 'test.csv')      # 테스트, 즉 submission 시 사용할 데이터 경로\n",
    "\n",
    "print(train_data_path)\n",
    "print(sub_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 모델 - 데이터 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(train_data_path)\n",
    "sub = pd.read_csv(sub_data_path)\n",
    "print('train data dim : {}'.format(data.shape))\n",
    "print('sub data dim : {}'.format(sub.shape))\n",
    "\n",
    "# train data dim : (15035, 21)\n",
    "# sub data dim : (6468, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습데이터 라벨 제거\n",
    "y = data['price']\n",
    "del data['price']\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "# 전체 데이터 탐색을 위한 train, test 데이터 합치기\n",
    "# model 학습시 다시 분리해야하므로 병합 전 train_len에 training data개수 저장하여 추후 학습데이터만 불러올 수 있는 인덱스로 사용\n",
    "\n",
    "train_len = len(data)\n",
    "data = pd.concat((data, sub), axis=0)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 전처리 - missingno\n",
    "msno.matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치의 개수 출력\n",
    "for c in data.columns:\n",
    "    print('{} : {}'.format(c, len(data.loc[pd.isnull(data[c]), c].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id컬럼 제거 but 예측결과 제출을 대비한 sub_id변수에 id 컬럼 저장하고 지우기\n",
    "sub_id = data['id'][train_len:]\n",
    "del data['id']\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "# data column apply()로 필요한 부분만 잘라내기\n",
    "data['date'] = data['date'].apply(lambda x : str(x[:6]))\n",
    "\n",
    "data.head()\n",
    "\n",
    "# str(x[:6]) : 20141013T000000 형식 데이ㅓ에서 연/월 데이터만 사용하기 위해 자르는것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 변수별 분포 확인\n",
    "\n",
    "# 전체 데이터 분포 확인 - 컬럼의 분포가 치우쳤다면 다듬기 작업\n",
    "# id column 제외한 19 컬럼에 대ㅐ해 한번에 모든 그래프 그리기\n",
    "\n",
    "# sns.kdeplot 사용\n",
    "# kdeplot : diescrete 데이터 또한 부드러운 곡선으로 전체 분포 확인할 수 있는 시각화 함수\n",
    "fig, ax = plt.subplots(10, 2, figsize=(12, 60))   # 가로스크롤 때문에 그래프 확인이 불편하다면 figsize의 x값을 조절해 보세요. \n",
    "\n",
    "# id 변수는 제외하고 분포를 확인\n",
    "count = 0\n",
    "columns = data.columns\n",
    "for row in range(10):\n",
    "    for col in range(2):\n",
    "        sns.kdeplot(data[columns[count]], ax=ax[row][col])\n",
    "        ax[row][col].set_title(columns[count], fontsize=15)\n",
    "        count += 1\n",
    "        if count == 19 :\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bedrooms, sqft_living, sqft_lot, sqft_above, sqft_basement 변수가 한쪽으로 치우친 경향\n",
    "- 치우친 분포 -> 로그 변환을 통해 데이터 분포를 정규분포에 가깝게 만든다\n",
    "    \n",
    "- 치우친 컬럼들을 `skew_columns` 리스트 안에 넣고 모두 `np.log1p()` 를 활용하여 로그 변환을 진행\n",
    "- `numpy.log1p()` 함수는 입력 배열 각 요소에 대해 자연로그 log(1 + x)을 반환해 주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_columns = ['bedrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement']\n",
    "\n",
    "for c in skew_columns:\n",
    "    data[c] = np.log1p(data[c].values)\n",
    "\n",
    "print('얍💢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환 후 분포\n",
    "fig, ax = plt.subplots(3, 2, figsize=(12, 15))\n",
    "\n",
    "count = 0\n",
    "for row in range(3):\n",
    "    for col in range(2):\n",
    "        if count == 5:\n",
    "            break\n",
    "        sns.kdeplot(data[skew_columns[count]], ax=ax[row][col])\n",
    "        ax[row][col].set_title(skew_columns[count], fontsize=15)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그변환이 분포의 치우침을 줄이는 원리는?\n",
    "# 로그 함수의 형태를 보면 알 수 있다\n",
    "xx = np.linspace(0, 10, 500)\n",
    "yy = np.log(xx)\n",
    "\n",
    "plt.hlines(0, 0, 10)\n",
    "plt.vlines(0, -5, 5)\n",
    "plt.plot(xx, yy, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로그 함수 특징\n",
    "- $0 < x < 1$ 범위에서는 기울기가 매우 가파릅니다. 즉, $x$의 구간은 $(0, 1)$로 매우 짧은 반면, $y$의 구간은 $(-\\infty, 0)$으로 매우 큽니다.\n",
    "- 따라서 0에 가깝게 모여있는 값들이 $x$로 입력되면, 그 함수값인 $y$ 값들은 매우 큰 범위로 벌어지게 됩니다. 즉, 로그 함수는 0에 가까운 값들이 조밀하게 모여있는 입력값을, 넓은 범위로 펼칠 수 있는 특징을 가집니다.\n",
    "- 반면, $x$값이 점점 커짐에 따라 로그 함수의 기울기는 급격히 작아집니다. 이는 곧 큰 $x$값들에 대해서는 $y$값이 크게 차이나지 않게 된다는 뜻이고, 따라서 넓은 범위를 가지는 $x$를 비교적 작은 $y$값의 구간 내에 모이게 하는 특징을 가집니다.    \n",
    "이러한 특성으로 인해 한 쪽에 몰린 분포에 로그 변환을 취하면 넓게 퍼진다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `data[price]` 분포를 로그 변환\n",
    "sns.kdeplot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 분포를 로그변환하게 되면 어떤 모양일까?\n",
    "- 위 분포는 0 쪽으로 매우 심하게 치우쳐져 있는 분포를 보인다. 즉, 0과 1000000 사이에 대부분의 값들이 몰려있고, 아주 소수의 집들이 굉장히 높은 가격을 보인다.\n",
    "\n",
    "- 따라서 이 분포에 로그 변환을 취하면, 0에 가깝게 몰려있는 데이터들은 넓게 퍼질 것이고, 매우 크게 퍼져있는 소수의 데이터들은 작은 y값으로 모일 것이다.\n",
    "\n",
    "- 즉, 왼쪽으로 치우친 값들은 보다 넓은 범위로 고르게 퍼지고 오른쪽으로 얇고 넓게 퍼진 값들은 보다 작은 범위로 모이게 되므로 전체 분포는 정규분포의 형상을 띄는 방향으로 변환될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log_transformation = np.log1p(y)\n",
    "\n",
    "sns.kdeplot(y_log_transformation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 변환이 필요한 데이터에 대한 처리 완료\n",
    "# 전체데이터를 다시 나눈다\n",
    "\n",
    "# `train_len`이 인덱스가 되어 :train_len까지는 학습 데이터, train_len: 부터는 테스트 데이터이므로 `sub` 변수에 저장\n",
    "sub = data.iloc[train_len:, :]\n",
    "x = data.iloc[:train_len, :]\n",
    "\n",
    "print(x.shape)\n",
    "print(sub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-7. 일단 제출하고 시작해! Baseline 모델 (4) 모델 설계\n",
    "---\n",
    "- 모델링 : baseline 커널은 여러 모델을 함께 사용하여 결과를 섞는 블렌딩(blending) 기법을 활용한다\n",
    "- 블렌딩(or 앙상블 기법) -> 하나의 개별모델이 아닌 다양한 모델들을 종합하여 결과를 얻는 기법\n",
    "    - 하나의 강한 머신러닝 알고리즘보다 여러 개의 약한 머신러닝 알고리즘이 낫다\n",
    "    - img, video, voice 등의 비정형 데이터 분류는 딥러닝이 뛰어나나, 대부분의 정형 데이터 분류시에는 앙상블의 성능이 뛰어나다\n",
    "    - voting, bagging, boosting, stacking 등이 있다\n",
    "    - [수비니움 블로그 - 앙상블 기법](https://subinium.github.io/introduction-to-ensemble-1/#:~:text=%EC%95%99%EC%83%81%EB%B8%94(Ensemble)%20%ED%95%99%EC%8A%B5%EC%9D%80%20%EC%97%AC%EB%9F%AC,%EB%A5%BC%20%EA%B0%80%EC%A7%80%EA%B3%A0%20%EC%9D%B4%ED%95%B4%ED%95%98%EB%A9%B4%20%EC%A2%8B%EC%8A%B5%EB%8B%88%EB%8B%A4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 모델들의 결과를 산술평균하여 블렌딩 모델을 만든다\n",
    "gboost = GradientBoostingRegressor(random_state=2019)\n",
    "xgboost = xgb.XGBRegressor(random_state=2019)\n",
    "lightgbm = lgb.LGBMRegressor(random_state=2019)\n",
    "\n",
    "models = [{'model':gboost, 'name':'GradientBoosting'}, {'model':xgboost, 'name':'XGBoost'},\n",
    "          {'model':lightgbm, 'name':'LightGBM'}]\n",
    "\n",
    "print('얍💢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 검증을 통해 모델의 성능을 평가한다\n",
    "def get_cv_score(models):\n",
    "    kfold = KFold(n_splits=5, random_state=2019).get_n_splits(x.values)\n",
    "    for m in models:\n",
    "        print(\"Model {} CV score : {:.4f}\".format(m['name'], np.mean(cross_val_score(m['model'], x.values, y)), \n",
    "                                                  kf=kfold))\n",
    "print('얍💢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cv_score(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission file 만들기\n",
    "# cross_val_score() 함수는 회귀모델 전달시 결정계수인 R^2 점수를 반환\n",
    "# R^2 값이 1에 가까울수록 모델이 잘 학슴됨을 의미한다\n",
    "# 3개 트리 모델이 모두 훈련 데이터에 대해 괜찮은 성능을 보여준다\n",
    "\n",
    "# baseline 모델에선 여러모델을 입력하면 각 모델에대한 예측 결과를 평균내어주는 \n",
    "# AveragingBlending() 함수를 만들어 사용하며, models 딕셔너리 안의 모델을 모두 x, y로 학습시킨 뒤 predictions에 예측결과값을 모아서 평균한 값을 반환한다\n",
    "def AveragingBlending(models, x, y, sub_x):\n",
    "    for m in models : \n",
    "        m['model'].fit(x.values, y)\n",
    "    \n",
    "    predictions = np.column_stack([\n",
    "        m['model'].predict(sub_x.values) for m in models\n",
    "    ])\n",
    "    return np.mean(predictions, axis=1)\n",
    "\n",
    "print('얍💢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 생성\n",
    "y_pred = AveragingBlending(models, x, y, sub)\n",
    "print(len(y_pred))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출할 csv파일의 샘플인 data/sample_submission.csv 확인\n",
    "data_dir = os.getenv('HOME')+'/aiffel/e/e08_kaggle_kakr_housing/data'\n",
    "\n",
    "submission_path = join(data_dir, 'sample_submission.csv')\n",
    "submission = pd.read_csv(submission_path)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id, price 두 가지 열로 구성되어있으므로 동일한 데이터프레임을 만들어준다\n",
    "result = pd.DataFrame({\n",
    "    'id' : sub_id, \n",
    "    'price' : y_pred\n",
    "})\n",
    "\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출파일 만들기\n",
    "my_submission_path = join(data_dir, 'submission.csv')\n",
    "result.to_csv(my_submission_path, index=False)\n",
    "\n",
    "print(my_submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-8. 일단 제출하고 시작해! Baseline 모델 (5) 캐글에 첫 결과 제출하기\n",
    "---\n",
    "- 이미 종료된 대회이므로 `Late Submission`만 가능하다    \n",
    "```kaggle competitions submit -c 2019-2nd-ml-month-with-kakr -f submission.csv -m \"Message\"```\n",
    "- 제출하면 score가 뜬다\n",
    "- score : 120031.23722점\n",
    "- `Jump to your position on the leaderboard` 클릭시 내 등수로 이동한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-9. 랭킹을 올리고 싶다면? (1) 다시 한 번, 내 입맛대로 데이터 준비하기\n",
    "---\n",
    "## 최적의 모델을 찾아서, hyperparameter 튜닝\n",
    "---\n",
    "- 랭킹을 올리기 위해 직접 다양한 Hyperparameter를 튜닝해보면서 모델 성능 개선\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터(model parameter) vs 하이퍼파라미터?\n",
    "---\n",
    "모델 파라미터는 모델이 학습을 하면서 점차 최적화되는, 그리고 최적화가 되어야 하는 파라미터입니다.\n",
    "\n",
    "예를 들어 선형 회귀의 경우 y_pred = W*x + b 라는 식으로 예측값을 만들어 낼 텐데, 여기에서 모델 파라미터는 W 입니다. 모델은 학습 과정을 거치면서 최적의 y_pred 값, 즉 y_true에 가장 가까운 값을 출력해낼 수 있는 최적의 W를 찾아나갈 것입니다.\n",
    "\n",
    "반면, 하이퍼 파라미터는 모델이 학습을 하기 위해서 사전에 사람이 직접 입력해주는 파라미터입니다.\n",
    "\n",
    "이는 모델이 학습하는 과정에서 변하지 않습니다. 예를 들어 학습 횟수에 해당하는 epoch 수, 가중치를 업데이트 할 학습률(learning rate), 또는 선형 규제를 담당하는 labmda 값 등이 이에 해당합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다시 한 번 내 입맛대로 데이터 준비하기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/kaggle_kakr_housing/data'\n",
    "\n",
    "train_data_path = join(data_dir, 'train.csv')\n",
    "test_data_path = join(data_dir, 'test.csv') \n",
    "\n",
    "train = pd.read_csv(train_data_path)\n",
    "test = pd.read_csv(test_data_path)\n",
    "\n",
    "print('얍💢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data lookup\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date 전처리해주기 (to int)\n",
    "# 모델이 date 또한 예측을 위한 특성으로 활용할 수 있다\n",
    "train['date'] = train['date'].apply(lambda i: i[:6]).astype(int)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟 데이터에 해당하는 price 컬럼 저일하기\n",
    "# y 변수에 price를 넣어두고 train에선 삭제하기\n",
    "y = train['price']\n",
    "del train['price']\n",
    "\n",
    "print(train.columns)\n",
    "\n",
    "# id컬럼도 삭제해두기\n",
    "del train['id']\n",
    "\n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 작업을 test데이터에 대해서도 동일하게!\n",
    "# 단, price가 없으므로 price처리는 해주지 않아도 된다\n",
    "test['date'] = test['date'].apply(lambda i: i[:6]).astype(int)\n",
    "\n",
    "del test['id']\n",
    "\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target data check\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가격 데이터 분포 확인하기\n",
    "sns.kdeplot(y)\n",
    "plt.show()\n",
    "# ==> 왼쪽으로 크게 치우쳐 있는 형태를 보인다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thus, y는 np.log1p()함수를 통해 로그 변환을 해주고 추후 모델이 값을 예측하고 나면 다시 np.exp1m()을 활용해서 되돌린다\n",
    "# np.exp1m()은 np.log1p()와는 반대로 각 원소 x마다 exp(x)-1의 값을 반환해준다\n",
    "y = np.log1p(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(y)\n",
    "plt.show()\n",
    "# ==> 비교적 완만한 정규분포의 형태로 잘 변환되었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info()함수로 전체 데이터의 자료형을 한 눈에 확인한다\n",
    "train.info()\n",
    "# ==> 모두 실수||정수 자료형으로 문제 없이 모델 학습에 활용할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-10. 랭킹을 올리고 싶다면? (2) 다양한 실험을 위해 함수로 만들어 쓰자\n",
    "- 본격적으로 모델 튜닝 들어가기\n",
    "- ML모델을 학습시키고 튜닝하다보면 실험해볼 것들이 워낙 많아서 시간이 호다닥 간다\n",
    "- 보다 다양하고 많은 실험 <-- 실험을 위한 도구들이 준비되어있어야 유리\n",
    "- thus,반복작업들은 함수로 먼저 만들어두고 맘껏 실험하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE 계산\n",
    "# 필요 라이브러리 import\n",
    "# train/test/valid dataset으로 나누기 위한 train_test_split, RMSE점수 계산을 위한 mean_squared_error import\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('얍💢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대회 점수 평가 척도인 RMSE 계산을 위한 함수 제작\n",
    "# warning) y_test, y_pred는 위에서 np.log1p()로 변환이 된 값이므로 원 데이터 단위에 맞게 되돌리기 위해 np.expm1()을 추가해야함\n",
    "# exp로 다시 변환하여 mse를 계산한 값에 np.sqrt를 취하면 RMSE값을 얻을 수 있다\n",
    "\n",
    "def rmse(y_test, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred)))\n",
    "\n",
    "print('얍💢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBRegressor, LGBMRegressor, GradientBoostingRegressor, RandomForestRegressor 모델 가져오기\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "print('얍💢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성 후 models라는 리스트에 입력\n",
    "# model param초기화나 dataset 구성에 사용되는 랜덤 시드값인 random_state값을 fixed로 하거나, 지정하지않고 None으로 세팅할 수 있다\n",
    "# random_state값이 fixed인 경우 모델==데이터셋 동일한 경우 ML학습결과도 항상 동일하게 재현된다\n",
    "# None으로 남겨두면 모델 내부에서 랜덤 시드값을 임의선택하므로 결과적으로 param초기화나 데이터셋 구성 양상이 달라져서\n",
    "# 모델과 데이터셋이 동일하더라도 ML 학습결과는 학습할 때마다 달라진다\n",
    "\n",
    "# baseline부터 시작해서 여러 실험을 통해 성능개선을 검증하자\n",
    "# a trial이 성능향상에 유효한지 여부 판단을 위해선 랜덤적 요소/변화에서 생기는 불확실성을 제거해야하므로\n",
    "# random_state값을 fixed한다.\n",
    "# fix 안했을 때 어떻게 될 지 궁금하면 random_state값을 None으로 남겨두고 실험을 반복하면 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state는 모델초기화나 데이터셋 구성에 사용되는 랜덤 시드값입니다. \n",
    "#random_state=None    # 이게 초기값입니다. 아무것도 지정하지 않고 None을 넘겨주면 모델 내부에서 임의로 선택합니다.  \n",
    "random_state=2020        # 하지만 우리는 이렇게 고정값을 세팅해 두겠습니다. \n",
    "\n",
    "gboost = GradientBoostingRegressor(random_state=random_state)\n",
    "xgboost = XGBRegressor(random_state=random_state)\n",
    "lightgbm = LGBMRegressor(random_state=random_state)\n",
    "rdforest = RandomForestRegressor(random_state=random_state)\n",
    "\n",
    "models = [gboost, xgboost, lightgbm, rdforest]\n",
    "\n",
    "print('얍💢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 모델의 이름 얻는 법) 클래스의 __name__속성에 접근\n",
    "gboost.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이름 접근가능하다면 for문안에서 각 모델별로 학습 및 예측이 가능하다\n",
    "df = {}\n",
    "\n",
    "for model in models:\n",
    "    # 모델 이름 획득\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    # train, test 데이터셋 분리 - 여기에도 random_state를 고정합니다. \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, y, random_state=random_state, test_size=0.2)\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 예측\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 예측 결과의 rmse값 저장\n",
    "    df[model_name] = rmse(y_test, y_pred)\n",
    "    \n",
    "    # data frame에 저장\n",
    "    score_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=False)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단히 네 가지 모델에 대해 모두 RMSE값을 빠르게 얻을 수 있음\n",
    "# get_scores(models, train, y)함수로 만들어보기\n",
    "def get_scores(models, train, y):\n",
    "    df = {}\n",
    "    for m in models:\n",
    "      m_name = m.__class__.__name__\n",
    "      \n",
    "      X_train, X_test, y_train, y_test = train_test_split(train, y, random_state=random_state, test_size=0.2)\n",
    "      m.fit(X_train, y_train)\n",
    "      y_pred = m.predict(X_test)\n",
    "      df[m_name] = rmse(y_test, y_pred)\n",
    "      \n",
    "      score_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=True)\n",
    "      \n",
    "    return score_df\n",
    "get_scores(models, train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-11. 랭킹을 올리고 싶다면? (3) 하이퍼 파라미터 튜닝의 최강자, 그리드 탐색\n",
    "---\n",
    "- 이제 모델과 데이터셋이 있다면 RMSE결과값을 나타내주는 함수가 준비되었으므로, 다양한 하이퍼파라미터로 실험해보자\n",
    "- sklearn.model_selection라이브러리 안에 있는 GridSearchCV 클래스 활용\n",
    "- `GridSearchCV` 란? [Random Search vs Grid Search](https://shwksl101.github.io/ml/dl/2019/01/30/Hyper_parameter_optimization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험을 위해 \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print('얍💢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
