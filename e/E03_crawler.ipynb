{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beautifulsoup & newspaper3k drawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "#- 파싱할 뉴스 기사 주소입니다.\n",
    "url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=030&aid=0002881076'\n",
    "\n",
    "#- 언어가 한국어이므로 language='ko'로 설정해줍니다.\n",
    "article = Article(url, language='ko')\n",
    "article.download()\n",
    "article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사 제목 :\n",
      "[AI 사피엔스 시대]자연어처리 기술, 컴퓨팅 파워 경쟁 시대로\n",
      "\n",
      "기사 내용 :\n",
      "[Copyright ⓒ 전자신문 & 전자신문인터넷, 무단전재 및 재배포 금지]\n",
      "\n",
      "주로 아이디어와 기술력으로 경쟁했던 자연어처리 인공지능(AI) 분야는 점차 컴퓨팅 파워 싸움으로 무게 추가 이동하고 있다. 모델이 대형화되면서 향상된 퍼포먼스 확보에 필요한 자금 규모도 커지고 있다. 자칫 대기업 자본력에 휘둘릴 수 있다는 우려도 함께 나온다.자연어처리(NLP)는 인간이 사용하는 언어 체계를 기계가 인식하도록 알고리즘을 디자인하는 기술이다. 흔히 말하는 컴퓨터 혹은 인간과 대화하는 컴퓨터 관련 기술이 포함된다.목적에 따라 세 가지 카테고리로 나뉜다. 인간이 제기한 질문에 자동으로 적절한 답을 찾아주는 '질의응답(QA)', 원하는 업무를 지시했을 때 작업을 수행하는 '테스크 컴플리션', 그리고 특별한 목적이 없는 대화를 의미하는 '오픈도메인 컨버세이션(비목적성 대화)'이 있다. 각기 발전해왔던 세 가지 기술은 지난 2018년 구글의 인공지능 언어모델 '버트(BERT)'의 등장으로 패러다임이 전환됐다. 압도적인 성능으로 대량의 프리트레이닝(사전학습)이 가능해지면서 굳이 셋을 구분할 필요가 없어진 것이다.기계학습 연구에서 모델을 학습할 때는 지도학습과 비지도학습, 강화학습 중 하나를 골라 활용한다. 지도학습은 사람이 적절한 입력과 출력을 부여하는 방식이다. 정답이 정해져 있고 기계의 정답률도 쉽게 측정할 수 있다. 반면에 비지도학습은 정답이 정해지지 않은 데이터에 대해서도 기계가 스스로 클러스터링 등을 통해 학습한다. 체계화되지 않은 대량의 데이터를 학습 가능하지만 학습이 맞게 됐는지 확인하기 어렵다.버트는 기존 AI 학습 방법을 혁신적으로 바꿔놨다는 평가를 받는다. 자연어처리를 교사 없이 양방향으로 사전 학습하는 최초의 시스템이다. 비지도학습 방식을 사용하면서도 기존 존재했던 어떤 기술보다 뛰어난 성능을 보여준다. 최근 1년 반 동안 버트를 필두로 AI 모델은 급격히 대형화되는 추세다.이는 기존의 빅데이터 개념을 훨씬 상회하는 초 대규모 데이터를 프리트레이닝 단계에서 학습 가능하게 만드는 결과로 이어졌다. 성능도 비약적으로 상승했다. 가장 최근 구글이 선보인 제너레이션 모델은 제시된 문장에 대해 인간의 80~90% 수준으로 답변 문장을 만들어 낸다.문제는 모델이 대형화되면서 기술을 구성하는 요소 중 알고리즘 대비 대량의 데이터셋과 컴퓨팅 파워 중요성이 더 커졌다는 점이다. 버트 자체는 '파인튜닝' 방식으로 다양한 분야에 확대 적용이 가능하지만 버트 성능에 준하는 최신 대형 모델을 만들어내려면 현존 최신 텐서처리장치(TPU)로도 학습에 수십 일이 걸린다.국내 한 AI 스타트업 대표는 “국내 상황은 데이터셋·인력·자금 인프라 모두 중국이나 영어권에 비해 떨어지는 게 사실이다. 자연어처리 모델 하나 만드는 데 10억~20억원씩 비용이 들어 고민이 많다”면서 “지금 글로벌 시장에서 격차가 나기 시작하면 이후에는 따라잡기 어려워진다. 정부 차원의 관심과 지원이 필요하다”고 말했다.이형두기자 dudu@etnews.com\n"
     ]
    }
   ],
   "source": [
    "#- 기사 제목을 출력합니다.\n",
    "print('기사 제목 :')\n",
    "print(article.title)\n",
    "print('')\n",
    "\n",
    "#- 기사 내용을 출력합니다.\n",
    "print('기사 내용 :')\n",
    "print(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤러를 만들기 전 필요한 도구들을 임포트합니다.\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 페이지 수, 카테고리, 날짜를 입력값으로 받습니다.\n",
    "def make_urllist(page_num, code, date): \n",
    "  urllist= []\n",
    "  for i in range(1, page_num + 1):\n",
    "    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)   \n",
    "    news = requests.get(url)\n",
    "\n",
    "    # BeautifulSoup의 인스턴스 생성합니다. 파서는 html.parser를 사용합니다.\n",
    "    soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "    # CASE 1\n",
    "    news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "    # CASE 2\n",
    "    news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "        \n",
    "    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.\n",
    "    for line in news_list:\n",
    "        urllist.append(line.a.get('href'))\n",
    "  return urllist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 기사의 개수:  40\n",
      "뉴스 기사의 개수:  40\n"
     ]
    }
   ],
   "source": [
    "#. 한 페이지당 뉴스 기사가 20개임을 감안하여 총 40개의 url리스트 받아오기\n",
    "url_list = make_urllist(2, 101, 20200506)\n",
    "print('뉴스 기사의 개수: ',len(url_list))\n",
    "\n",
    "url_list1 = make_urllist(2, 101, 20200804)\n",
    "print('뉴스 기사의 개수: ',len(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=057&aid=0001451723',\n",
       " 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=057&aid=0001451721',\n",
       " 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=057&aid=0001451718',\n",
       " 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=003&aid=0009849190',\n",
       " 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=057&aid=0001451717']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#. 5개만 출력해 보기\n",
    "url_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "#- 데이터프레임을 생성하는 함수입니다.\n",
    "def make_data(urllist, code):\n",
    "  text_list = []\n",
    "  for url in urllist:\n",
    "    article = Article(url, language='ko')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text_list.append(article.text)\n",
    "\n",
    "  #- 데이터프레임의 'news' 키 아래 파싱한 텍스트를 밸류로 붙여줍니다.\n",
    "  df = pd.DataFrame({'news': text_list})\n",
    "\n",
    "  #- 데이터프레임의 'code' 키 아래 한글 카테고리명을 붙여줍니다.\n",
    "  df['code'] = idx2word[str(code)]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경제 카테고리(코드 101)의 40개의 URL 리스트로부터 데이터프레임을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>고려은단이 5월을 맞아 응원 메시지를 공유하는 ‘5월 5글자로 응원 부탁해!’ 이벤...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>코리아나화장품의 민감성 피부를 위한 저자극 스킨케어 브랜드 '프리엔제'가 마르고 건...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>서울장수주식회사가 부드럽고 달콤한 맛으로 인기를 모으고 있는 생막걸리 ‘인생막걸리’...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[서울=뉴시스] 오동현 기자 = 모바일 게임 기업 컴투스는 3D 모바일 야구 게임 ...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>대원제약이 2020년 상반기 신입과 경력 정기 공채를 실시합니다.정기 공채 모집분야...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[AFP=연합뉴스] [AFP=연합뉴스]\\n\\n\"요즘은 잔인한 날\"…리프트도 앞서 9...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>이재용 삼성전자 부회장이 6일 삼성전자 서울 서초사옥에서 대국민 사과 회견을 하기 ...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JW중외제약이 A형 혈우병 예방요법제 ‘헴리브라피하주사를 출시하고 본격적인 마케팅 ...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>옵티팜과 휴벳바이오가 공동 개발중인 백신 후보 물질에 대해 마우스, 기니피그, 미니...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[한국경제TV 신동호 기자]\\n\\n전남 나주시와 충북 청주시가 방사광 가속기 구축사...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news code\n",
       "0  고려은단이 5월을 맞아 응원 메시지를 공유하는 ‘5월 5글자로 응원 부탁해!’ 이벤...   경제\n",
       "1  코리아나화장품의 민감성 피부를 위한 저자극 스킨케어 브랜드 '프리엔제'가 마르고 건...   경제\n",
       "2  서울장수주식회사가 부드럽고 달콤한 맛으로 인기를 모으고 있는 생막걸리 ‘인생막걸리’...   경제\n",
       "3  [서울=뉴시스] 오동현 기자 = 모바일 게임 기업 컴투스는 3D 모바일 야구 게임 ...   경제\n",
       "4  대원제약이 2020년 상반기 신입과 경력 정기 공채를 실시합니다.정기 공채 모집분야...   경제\n",
       "5  [AFP=연합뉴스] [AFP=연합뉴스]\\n\\n\"요즘은 잔인한 날\"…리프트도 앞서 9...   경제\n",
       "6  이재용 삼성전자 부회장이 6일 삼성전자 서울 서초사옥에서 대국민 사과 회견을 하기 ...   경제\n",
       "7  JW중외제약이 A형 혈우병 예방요법제 ‘헴리브라피하주사를 출시하고 본격적인 마케팅 ...   경제\n",
       "8  옵티팜과 휴벳바이오가 공동 개발중인 백신 후보 물질에 대해 마우스, 기니피그, 미니...   경제\n",
       "9  [한국경제TV 신동호 기자]\\n\\n전남 나주시와 충북 청주시가 방사광 가속기 구축사...   경제"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = make_data(url_list, 101)\n",
    "#- 상위 10개만 출력해봅니다.\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-8. 네이버 뉴스 기사 크롤링 (3) 데이터 수집 및 전처리\n",
    "여러 카테고리에 뉴스들도 수집해보기 -> 특저 날짜의 사회, 생활/문화, IT/과학"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102, 103, 105]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 수집을 원하는 카테고리 코드를을 저장한 리스트를 만들어두기\n",
    "code_list = [102, 103, 105]\n",
    "\n",
    "code_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_total_data(page_num, code_list, date):\n",
    "  df = None\n",
    "\n",
    "  for code in code_list:\n",
    "    url_list = make_urllist(page_num, code, date)\n",
    "    df_temp = make_data(url_list, code)\n",
    "    print(str(code)+'번 코드에 대한 데이터를 만들었습니다.')\n",
    "\n",
    "    if df is not None:\n",
    "      df = pd.concat([df, df_temp])\n",
    "    else:\n",
    "      df = df_temp\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_total_data(1, code_list, 20200506)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('뉴스 기사의 개수: ',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n v a <- exclude all. or extract n\n",
    "# 60개중 임의의 10개 샘플 선택하여 출력\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대량크롤링\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 주석처리된 코드의 주석을 해제하고 실행을 하면 대량 크롤링이 진행됩니다. \n",
    "# 위에서 수행했던 크롤링의 100배 분량이 수행될 것입니다. 한꺼번에 너무 많은 크롤링 요청이 서버에 전달되지 않도록 주의해 주세요. \n",
    "# 기사 일자를 바꿔보면서 데이터를 모으면 더욱 다양한 데이터를 얻을 수 있게 됩니다. \n",
    "\n",
    "#df = make_total_data(100, code_list, 20200506)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아래 블럭만 실행하지 말기!!! 누적되서 망함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 크롤링한 뉴스 기사의 양이 많으므로 실수로 유실하지 않도록 csv 파일로 별도 저장\n",
    "\n",
    "import os\n",
    "\n",
    "# 데이터프레임 파일을 csv 파일로 저장합니다.\n",
    "# 저장경로는 이번 프로젝트를 위해 만든 폴더로 지정해 주세요.\n",
    "###### csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "###### df.to_csv(csv_path, index=False)\n",
    "\n",
    "###### if os.path.exists(csv_path):\n",
    "######  print('{} File Saved!'.format(csv_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-9. naver news article crawling (4) data preprocessing\\\n",
    "데이터를 가공/정제하여 모델의 훈련에 사용하기 좋은 숫자의 형태로 바꿔준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. 저장해둔 csv파일을 다시 읽어서 dataframe으로 저장하기 위한 코드\n",
    "import os\n",
    "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "df = pd.read_table(csv_path, sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거\n",
    "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "df['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data에 null값이 있진 않은지 확인\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복된 샘플들을 제거합니다.\n",
    "df.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "print('뉴스 기사의 개수: ',len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 탐색\n",
    "---\n",
    "각 카테고리별 샘플 분포 확인 -> matplotlib에 한글폰트 적용안될 수 있음 주의\n",
    "[matplotlib 한글폰트 설정방법](https://followers.tistory.com/26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. 한글 출력\n",
    "import matplotlib.pyplot as plt\n",
    "plt.text(0.3, 0.3, '한글', size=100)\n",
    "\n",
    "from matplotlib import font_manager as fm\n",
    "font_location='/home/aiffel/.local/share/fonts/D2CodingBold-Ver1.3.2-20180524-ligature.ttf'\n",
    "font_name=fm.FontProperties(fname=font_location).get_name()\n",
    "plt.rc('font',family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "fm.get_fontconfig_fonts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_location='/usr/share/fonts/truetype/nanum/NanumMyeongjo.ttf'\n",
    "font_name=fm.FontProperties(fname=font_location).get_name()\n",
    "matplotlib.rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['code'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실 개수 확인\n",
    "print(df.groupby('code').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "자연어 처리 - 문자열은 특정 단위로 나누어지는데 이 특정 단위가 바로 토큰이다\n",
    "토크나이징 - 문자열을 토큰으로 break down into pieces\n",
    "[영어와 한국어의 토큰화의 차이](https://wikidocs.net/21698)   \n",
    "-> 한국어 자연어처리의 경우에는 토큰화 과정을 주로 형태소 분석기를 사용해서 수행    \n",
    "한국어의 경우 교착어라는 특성으로 인해 조사, 어미 등이 붙어있어 띄어쓰기 단위로\n",
    "토큰화를 할 경우에는 같은 단어임에도 다른 단어로 인식될 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 패키지 라이브러리 Mecab\n",
    "# Mecab의 .morphs를 사용하면 입력 문자열을 형태소 단위로 나눠줌\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "\n",
    "kor_text = '밤에 귀가하던 여성에게 범죄를 시도한 대 남성이 구속됐다서울 제주경찰서는 \\\n",
    "            상해 혐의로 씨를 구속해 수사하고 있다고 일 밝혔다씨는 지난달 일 피해 여성을 \\\n",
    "            인근 지하철 역에서부터 따라가 폭행을 시도하려다가 도망간 혐의를 받는다피해 \\\n",
    "            여성이 저항하자 놀란 씨는 도망갔으며 신고를 받고 주변을 수색하던 경찰에 \\\n",
    "            체포됐다피해 여성은 이 과정에서 경미한 부상을 입은 것으로 전해졌다'\n",
    "\n",
    "#- 형태소 분석, 즉 토큰화(tokenization)를 합니다.\n",
    "print(tokenizer.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 처리: 토큰화 결과가 얼마나 정확한지에 다라 그 성능에 많은 영향을 받는다    \n",
    "형태소 분석기의 결과가 머신러닝 결과의 성능에 큰 영향을 주기도 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 제거(stopwords)\n",
    "토큰화 전처리 필수과정 - 불필요한 토큰들을 제거하는 불용어 제거     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어 - 미리 짐작하여 한 번에 정의한는 것이 아니라 토큰화 과정을 거친 결과를 지속적으로 확인하면서 계속 추가하게 되는게 일반적이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수입니다.\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "    temp_data = []\n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer.morphs(sentence) \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터에 대해서 토큰화 및 불용어 제거를 수행한 뒤 첫 번째 샘플을 출력\n",
    "text_data = preprocessing(df['news'])\n",
    "print(text_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML모델 적용을 위해 필요한 도구들 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계는 텍스트보다 숫자를 더 잘 처리한다    \n",
    "ML모델인 나이브 베이즈 분류기를 사용하기 위해서는 각 뉴스의 텍스트 데이터를 벡터로 변환할 필요가 있다    \n",
    "TF-IDF 방법 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 뉴스 문서를 TF-IDF 벡터로 바꾼 뒤 나이브 베이즈 분류기를 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform()` 함수는 fit, transform을 연이어 수행하는 함수로, `CountVectorizer.fit_transform()` 은 단어 데이터를 학습하고 문서 데이터를 document-form matrix로 변환하는 두 가지 작업을 해 준다\n",
    "[사이킷런 참고자료](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 하기\n",
    "\n",
    "# 텍스트 입력을 자동으로 TF-IDF 벡터로 바꾸는 전처리 함수 만들기\n",
    "# 이걸로 바로 txt -> 나이브 베이즈 분류기 입력으로 사용함으로써 용이한 테스트 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 뉴스에 대한 카테고리 확인하기\n",
    "# 훈련, 테스트 데이터 아닌 임의 뉴스임\n",
    "# clf.predict() : 임의의 입력에 대해 나이브 베이즈 분류기가 예측한 값을 리턴함\n",
    "new_sent = preprocessing([\"민주당 일각에서 법사위의 체계·자구 심사 기능을 없애야 한다는 \\\n",
    "                           주장이 나오는 데 대해 “체계·자구 심사가 법안 지연의 수단으로 \\\n",
    "                          쓰이는 것은 바람직하지 않다”면서도 “국회를 통과하는 법안 중 위헌\\\n",
    "                          법률이 1년에 10건 넘게 나온다. 그런데 체계·자구 심사까지 없애면 매우 위험하다”고 반박했다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sent = preprocessing([\"인도 로맨틱 코미디 영화 <까립까립 싱글>(2017)을 봤을 때 나는 두 눈을 의심했다. \\\n",
    "                          저 사람이 남자 주인공이라고? 노안에 가까운 이목구비와 기름때로 뭉친 파마머리와, \\\n",
    "                          대충 툭툭 던지는 말투 등 전혀 로맨틱하지 않은 외모였다. 반감이 일면서 \\\n",
    "                          ‘난 외모지상주의자가 아니다’라고 자부했던 나에 대해 회의가 들었다.\\\n",
    "                           티브이를 꺼버릴까? 다른 걸 볼까? 그런데, 이상하다. 왜 이렇게 매력 있지? 개구리와\\\n",
    "                            같이 툭 불거진 눈망울 안에는 어떤 인도 배우에게서도 느끼지 못한 \\\n",
    "                            부드러움과 선량함, 무엇보다 슬픔이 있었다. 2시간 뒤 영화가 끝나고 나는 완전히 이 배우에게 빠졌다\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sent = preprocessing([\"20분기 연속으로 적자에 시달리는 LG전자가 브랜드 이름부터 성능, 디자인까지 대대적인 변화를 \\\n",
    "                          적용한 LG 벨벳은 등장 전부터 온라인 커뮤니티를 뜨겁게 달궜다. 사용자들은 “디자인이 예쁘다”, \\\n",
    "                          “슬림하다”는 반응을 보이며 LG 벨벳에 대한 기대감을 드러냈다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-11. 프로젝트 - 모델 성능 개선하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news    0\n",
      "code    0\n",
      "dtype: int64\n",
      "뉴스 기사의 개수:  2137\n"
     ]
    }
   ],
   "source": [
    "#. 저장해둔 csv파일을 다시 읽어서 dataframe으로 저장하기 위한 코드\n",
    "import os\n",
    "import pandas as pd\n",
    "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data2.csv\"\n",
    "df = pd.read_table(csv_path, sep=',')\n",
    "df.head()\n",
    "\n",
    "# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거\n",
    "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "df['news']\n",
    "\n",
    "# data에 null값이 있진 않은지 확인\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 중복된 샘플들을 제거합니다.\n",
    "df.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "print('뉴스 기사의 개수: ',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어\n",
    "stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 형태소 분석기 변경해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['밤', '에', '귀가', '하', '던', '여성', '에게', '범죄', '를', '시도', '한', '대', '남성', '이', '구속', '됐', '다', '서울', '제주', '경찰서', '는', '상해', '혐의', '로', '씨', '를', '구속', '해', '수사', '하', '고', '있', '다고', '일', '밝혔', '다', '씨', '는', '지난달', '일', '피해', '여성', '을', '인근', '지하철', '역', '에서부터', '따라가', '폭행', '을', '시도', '하', '려다가', '도망간', '혐의', '를', '받', '는다', '피해', '여성', '이', '저항', '하', '자', '놀란', '씨', '는', '도망갔으며', '신고', '를', '받', '고', '주변', '을', '수색', '하', '던', '경찰', '에', '체포', '됐', '다', '피해', '여성', '은', '이', '과정', '에서', '경미', '한', '부상', '을', '입', '은', '것', '으로', '전해졌', '다']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 패키지 라이브러리 Mecab\n",
    "# Mecab의 .morphs를 사용하면 입력 문자열을 형태소 단위로 나눠줌\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "\n",
    "kor_text = '밤에 귀가하던 여성에게 범죄를 시도한 대 남성이 구속됐다서울 제주경찰서는 \\\n",
    "            상해 혐의로 씨를 구속해 수사하고 있다고 일 밝혔다씨는 지난달 일 피해 여성을 \\\n",
    "            인근 지하철 역에서부터 따라가 폭행을 시도하려다가 도망간 혐의를 받는다피해 \\\n",
    "            여성이 저항하자 놀란 씨는 도망갔으며 신고를 받고 주변을 수색하던 경찰에 \\\n",
    "            체포됐다피해 여성은 이 과정에서 경미한 부상을 입은 것으로 전해졌다'\n",
    "\n",
    "#- 형태소 분석, 즉 토큰화(tokenization)를 합니다.\n",
    "print(tokenizer.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수입니다.\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "    temp_data = []\n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer.morphs(sentence) \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간측정\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사 섹션 분류 안내 기사 섹션 정보 해당 언론사 분류 를 따르 고 습니다 언론사 개별 기사 를 개 이상 섹션 으로 중복 분류 습니다 닫 기\n",
      "[Mecab 분석기 preprocessing 걸린 시간] 2.7707881927490234 sec\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "text_data=preprocessing(df['news'])\n",
    "print(text_data[0])\n",
    "\n",
    "print(\"[Mecab 분석기 preprocessing 걸린 시간]\", time.time()-start, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mecab 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 훈련 데이터와 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 10)\n",
    "\n",
    "# 나이브 베이즈 분류기 학습\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['경제']\n",
      "\n",
      "['생활/문화']\n",
      "\n",
      "['경제']\n",
      "\n",
      "['경제']\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "# 임의의 뉴스에 대한 카테고리 확인하기\n",
    "# 훈련, 테스트 데이터 아닌 임의 뉴스임\n",
    "# clf.predict() : 임의의 입력에 대해 나이브 베이즈 분류기가 예측한 값을 리턴함\n",
    "new_sent = preprocessing([\"민주당 일각에서 법사위의 체계·자구 심사 기능을 없애야 한다는 \\\n",
    "                           주장이 나오는 데 대해 “체계·자구 심사가 법안 지연의 수단으로 \\\n",
    "                          쓰이는 것은 바람직하지 않다”면서도 “국회를 통과하는 법안 중 위헌\\\n",
    "                          법률이 1년에 10건 넘게 나온다. 그런데 체계·자구 심사까지 없애면 매우 위험하다”고 반박했다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent = preprocessing([\"인도 로맨틱 코미디 영화 <까립까립 싱글>(2017)을 봤을 때 나는 두 눈을 의심했다. \\\n",
    "                          저 사람이 남자 주인공이라고? 노안에 가까운 이목구비와 기름때로 뭉친 파마머리와, \\\n",
    "                          대충 툭툭 던지는 말투 등 전혀 로맨틱하지 않은 외모였다. 반감이 일면서 \\\n",
    "                          ‘난 외모지상주의자가 아니다’라고 자부했던 나에 대해 회의가 들었다.\\\n",
    "                           티브이를 꺼버릴까? 다른 걸 볼까? 그런데, 이상하다. 왜 이렇게 매력 있지? 개구리와\\\n",
    "                            같이 툭 불거진 눈망울 안에는 어떤 인도 배우에게서도 느끼지 못한 \\\n",
    "                            부드러움과 선량함, 무엇보다 슬픔이 있었다. 2시간 뒤 영화가 끝나고 나는 완전히 이 배우에게 빠졌다\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent= preprocessing([\"20분기 연속으로 적자에 시달리는 LG전자가 브랜드 이름부터 성능, 디자인까지 대대적인 변화를 \\\n",
    "                          적용한 LG 벨벳은 등장 전부터 온라인 커뮤니티를 뜨겁게 달궜다. 사용자들은 “디자인이 예쁘다”, \\\n",
    "                          “슬림하다”는 반응을 보이며 LG 벨벳에 대한 기대감을 드러냈다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent = preprocessing([\"코로나19 이후 주목받기 시작한 비대면(언택트) 서비스에서 데이터와 AI는 핵심 역할을 한다. 데이터3법 개정으로 각 기업·기관이 가지고 있던 활용되지 못한 데이터가 유통됨에 따라 상당한 시너지 효과가 발생할 것으로 기대된다. 바야흐로 ‘데이터 경제 생태계’ 조성의 시작인 셈이다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       1.00      0.02      0.03        58\n",
      "          경제       0.66      0.98      0.79       235\n",
      "          사회       0.92      0.94      0.93       139\n",
      "       생활/문화       1.00      0.39      0.56       103\n",
      "\n",
      "    accuracy                           0.75       535\n",
      "   macro avg       0.89      0.58      0.58       535\n",
      "weighted avg       0.83      0.75      0.70       535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Hannanum 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['밤', '에', '귀가', '하', '던', '여성', '에게', '범죄', '를', '시도', '하', 'ㄴ', '대', '남성', '이', '구속됐다서울', '제주경찰', '서는', '상하', '어', '혐의', '로', '씨', '를', '구속해', '수사', '하고', '있', '다', '고', '일', '밝혔다씨', '는', '지난달', '일', '피하', '어', '여성', '을', '인근', '지하철', '역', '에서부터', '따르', '아', '가', '아', '폭행', '을', '시도', '하', '려', '다가', '도망가', 'ㄴ', '혐의', '를', '받는다피해', '여성', '이', '저항', '하', '자', '놀라', 'ㄴ', '씨', '는', '도망가', '아며', '신고', '를', '받', '고', '주변', '을', '수색', '하', '던', '경찰', '에', '체포됐다피해', '여성', '은', '이', '과정', '에서', '경미한', '부상', '을', '입', '은', '것', '으로', '전하', '어', '지', '었다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "tokenizer=Hannanum()\n",
    "\n",
    "kor_text = '밤에 귀가하던 여성에게 범죄를 시도한 대 남성이 구속됐다서울 제주경찰서는 \\\n",
    "            상해 혐의로 씨를 구속해 수사하고 있다고 일 밝혔다씨는 지난달 일 피해 여성을 \\\n",
    "            인근 지하철 역에서부터 따라가 폭행을 시도하려다가 도망간 혐의를 받는다피해 \\\n",
    "            여성이 저항하자 놀란 씨는 도망갔으며 신고를 받고 주변을 수색하던 경찰에 \\\n",
    "            체포됐다피해 여성은 이 과정에서 경미한 부상을 입은 것으로 전해졌다'\n",
    "\n",
    "print(tokenizer.morphs(kor_text))\n",
    "# stopwords에 ㄴ, ㅇ 을 추가해야 할까??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사 섹션 분류 안내기사 섹션 정보 해당 언론사 분류 를 따르 고 습니다 언론사 개별 기사 를 개 이상 섹션 으로 중복 분류 ㄹ 있습니다닫기\n",
      "[Hannanum 분석기 preprocessing 걸린 시간] 68.09888482093811 sec\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "text_data=preprocessing(df['news'])\n",
    "print(text_data[0])\n",
    "\n",
    "print(\"[Hannanum 분석기 preprocessing 걸린 시간]\", time.time()-start, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제대 총선 실시간 개표 현황 결과 보기총선 에서 여당 다시 ㄴ 번 승리 를 거두 면서 문재 ㄴ 정부 경제정책 힘 더 실리 게 되 었다는 평가 나 아 오 ㄴ다 친노동 크 ㄴ 정부 복지국가 내세우 정부 와 여당 정책 기조 더 강화 되 ㄹ 전망이다경제 전문가들 총선 이후 국회 와 집권 여당 역할 그 어느 때 보다 중요 고 입 모으 아다 올해 한국 경제 신종 코로나바이러스 감염증코로나 으로 극심한 침체 를 겪 것 으로 예상 되 기 올해 한국 경제 년 외환위 기 이후 년 만 역성장할 것 으로 관측 되 고 국제통화기금 지나 ㄴ 일 한국의 올해 경제성장률 전망치 를 어서 로 대폭 낮추 었다 코로나 로 인하 어 촉발 되 ㄴ 글로벌 경제위 기 를 한국 도 피하 어 기 힘들 ㄹ 것 이란 이유에서다전문가들 정부 와 여당 코 로나 위 기 극복 위하 어 적극적 ㄴ 대책 수립 어야 지만 막대 ㄴ 재정 투입 되 만큼 우선순위 를 정하 ㄹ 필요 고 조언 었다 안동현 서울대 경제학부 교수 소비 진작 차원 에서 긴급재난지원금코로나지원금 반대 경제학자 없 것 라면 서도 지급 대상 확대 기 보다 선별 통하 어 집중적 으로 주 것 맞 고 강조 었다 도 재정 지원 적시 한시적 고 선별적 으로 제공 ㄹ 필요 고 했다코로나 로 한계 상황 다다르 ㄴ 기업 대하 ㄴ 지원책 시급 다는 주문 도 나오 아다 . 이인호 서울대 경제학부 교수한국경제학회장 기업 소상공인 보다 안정적 ㄴ 일자리 를 창출 ㄹ 다며 국회 기업 무너지 지 않 도록 포괄적 지원 나 아 ㄴ다 고 말 었다 안동현 교수 주 시간 근로제 를 유예 하고 최저임금 제도 를 개선 어야 ㄴ다 고 주장 었다 외출 자제 영향 으로 매출 감소 ㄴ 대형마트 휴일 영업제한 철폐 어 소비 활성화 되 ㄹ 도록 어야 ㄴ다는 점 도 전문가들 공통 되 ㄴ 의견 알 ㄴ 교수 기업 힘들 어 지 면 고용 불안정 어 지 ㄴ다며 국회 원점 에서 경제 관련 법안 다시 들여다보 ㄹ 필요 고 말했다코로나 완전히 종식 되 ㄴ 뒤 상황 대비 것 도 대 국회 과제 로 꼽히 ㄴ다 전문가들 내년 이후 재정건전성 문제 불거지 ㄹ 것 으로 전망 었다 박기백 서울시립대 세무학 교수한국재정학회장 코 로나 대책 으로 재정 많 쓰 었고 경기침체 영향 으로 세수 도 많 쪼그라들 것 라며 내년 예산 심사 ㄹ 때 불필요한 예산 줄 고 과감 ㄴ 세출 구조조정 ㄹ 필요 고 강조 었다 교수 지금 돈 풀 것 맞 지 말 ㄴ 장기 화하 면 자산 거품 더 크 어 지 면서 최악 길 로 접어들 ㄹ 고 경고했다강진규김익환성수영 기 자 한국경제 무단전재 재배포\n",
      "2137\n"
     ]
    }
   ],
   "source": [
    "print(text_data[1])\n",
    "print(len(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가할 stopword : ㄹ, ㄴ, ㄴ다, 며, 고, 게, 와, 었, 었다, 으로, 한국경제, 재배포, 무단전재, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hannanum ML모델 적용하여 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 훈련 데이터와 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 10)\n",
    "\n",
    "# 나이브 베이즈 분류기 학습\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['경제']\n",
      "\n",
      "['경제']\n",
      "\n",
      "['경제']\n",
      "\n",
      "['경제']\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "# 임의의 뉴스에 대한 카테고리 확인하기\n",
    "# 훈련, 테스트 데이터 아닌 임의 뉴스임\n",
    "# clf.predict() : 임의의 입력에 대해 나이브 베이즈 분류기가 예측한 값을 리턴함\n",
    "new_sent = preprocessing([\"민주당 일각에서 법사위의 체계·자구 심사 기능을 없애야 한다는 \\\n",
    "                           주장이 나오는 데 대해 “체계·자구 심사가 법안 지연의 수단으로 \\\n",
    "                          쓰이는 것은 바람직하지 않다”면서도 “국회를 통과하는 법안 중 위헌\\\n",
    "                          법률이 1년에 10건 넘게 나온다. 그런데 체계·자구 심사까지 없애면 매우 위험하다”고 반박했다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent = preprocessing([\"인도 로맨틱 코미디 영화 <까립까립 싱글>(2017)을 봤을 때 나는 두 눈을 의심했다. \\\n",
    "                          저 사람이 남자 주인공이라고? 노안에 가까운 이목구비와 기름때로 뭉친 파마머리와, \\\n",
    "                          대충 툭툭 던지는 말투 등 전혀 로맨틱하지 않은 외모였다. 반감이 일면서 \\\n",
    "                          ‘난 외모지상주의자가 아니다’라고 자부했던 나에 대해 회의가 들었다.\\\n",
    "                           티브이를 꺼버릴까? 다른 걸 볼까? 그런데, 이상하다. 왜 이렇게 매력 있지? 개구리와\\\n",
    "                            같이 툭 불거진 눈망울 안에는 어떤 인도 배우에게서도 느끼지 못한 \\\n",
    "                            부드러움과 선량함, 무엇보다 슬픔이 있었다. 2시간 뒤 영화가 끝나고 나는 완전히 이 배우에게 빠졌다\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent= preprocessing([\"20분기 연속으로 적자에 시달리는 LG전자가 브랜드 이름부터 성능, 디자인까지 대대적인 변화를 \\\n",
    "                          적용한 LG 벨벳은 등장 전부터 온라인 커뮤니티를 뜨겁게 달궜다. 사용자들은 “디자인이 예쁘다”, \\\n",
    "                          “슬림하다”는 반응을 보이며 LG 벨벳에 대한 기대감을 드러냈다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent = preprocessing([\"코로나19 이후 주목받기 시작한 비대면(언택트) 서비스에서 데이터와 AI는 핵심 역할을 한다. 데이터3법 개정으로 각 기업·기관이 가지고 있던 활용되지 못한 데이터가 유통됨에 따라 상당한 시너지 효과가 발생할 것으로 기대된다. 바야흐로 ‘데이터 경제 생태계’ 조성의 시작인 셈이다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       1.00      0.02      0.03        58\n",
      "          경제       0.63      0.98      0.77       235\n",
      "          사회       0.91      0.91      0.91       139\n",
      "       생활/문화       1.00      0.27      0.43       103\n",
      "\n",
      "    accuracy                           0.72       535\n",
      "   macro avg       0.89      0.55      0.54       535\n",
      "weighted avg       0.81      0.72      0.66       535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_data[1])\n",
    "print(len(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Kkma 분석기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['밤', '에', '귀가', '하', '더', 'ㄴ', '여성', '에게', '범죄', '를', '시도', '하', 'ㄴ', '대', '남성', '이', '구속', '되', '었', '다', '서울', '제주', '경찰서', '는', '상해', '혐의', '로', '씨', '를', '구속', '하', '어', '수사', '하', '고', '있', '다고', '일', '밝히', '었', '다', '씨', '는', '지난달', '일', '피해', '여성', '을', '인근', '지하철', '역', '에서', '부터', '따라가', '폭행을', '시도', '하', '려', '다그', '아', '도망가', 'ㄴ', '혐의', '를', '받', '는', '다', '피해', '여성', '이', '저항', '하', '자', '놀라', 'ㄴ', '씨', '는', '도망가', '었', '으며', '신고', '를', '받', '고', '주변', '을', '수색', '하', '더', 'ㄴ', '경찰', '에', '체포', '되', '었', '다', '피해', '여성', '은', '이', '과정', '에서', '경미', '하', 'ㄴ', '부상', '을', '입', '은', '것', '으로', '전하', '어', '지', '었', '다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "tokenizer=Kkma()\n",
    "\n",
    "kor_text = '밤에 귀가하던 여성에게 범죄를 시도한 대 남성이 구속됐다서울 제주경찰서는 \\\n",
    "            상해 혐의로 씨를 구속해 수사하고 있다고 일 밝혔다씨는 지난달 일 피해 여성을 \\\n",
    "            인근 지하철 역에서부터 따라가 폭행을 시도하려다가 도망간 혐의를 받는다피해 \\\n",
    "            여성이 저항하자 놀란 씨는 도망갔으며 신고를 받고 주변을 수색하던 경찰에 \\\n",
    "            체포됐다피해 여성은 이 과정에서 경미한 부상을 입은 것으로 전해졌다'\n",
    "\n",
    "#- 형태소 분석, 즉 토큰화(tokenization)를 합니다.\n",
    "print(tokenizer.morphs(kor_text))\n",
    "\n",
    "# stopwords 었, ㄴ, 는, 다, 었다, 을 추가!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사 섹션 분류 안내 기사 섹션 정보 해당 언론사 분류 를 따르 고 습니다 언론사 개별 기사 를 개 이상 섹션 으로 중복 분류 ㄹ 습니다 닫기\n",
      "[Kkma 분석기 preprocessing 걸린 시간] 500.6342303752899 sec\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "text_data=preprocessing(df['news'])\n",
    "print(text_data[0])\n",
    "\n",
    "print(\"[Kkma 분석기 preprocessing 걸린 시간]\", time.time()-start, 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kkma 형태소 분석기 ML 모델 적용하여 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 훈련 데이터와 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 10)\n",
    "\n",
    "# 나이브 베이즈 분류기 학습\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['경제']\n",
      "\n",
      "['생활/문화']\n",
      "\n",
      "['경제']\n",
      "\n",
      "['경제']\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "# 임의의 뉴스에 대한 카테고리 확인하기\n",
    "# 훈련, 테스트 데이터 아닌 임의 뉴스임\n",
    "# clf.predict() : 임의의 입력에 대해 나이브 베이즈 분류기가 예측한 값을 리턴함\n",
    "new_sent = preprocessing([\"민주당 일각에서 법사위의 체계·자구 심사 기능을 없애야 한다는 \\\n",
    "                           주장이 나오는 데 대해 “체계·자구 심사가 법안 지연의 수단으로 \\\n",
    "                          쓰이는 것은 바람직하지 않다”면서도 “국회를 통과하는 법안 중 위헌\\\n",
    "                          법률이 1년에 10건 넘게 나온다. 그런데 체계·자구 심사까지 없애면 매우 위험하다”고 반박했다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent = preprocessing([\"인도 로맨틱 코미디 영화 <까립까립 싱글>(2017)을 봤을 때 나는 두 눈을 의심했다. \\\n",
    "                          저 사람이 남자 주인공이라고? 노안에 가까운 이목구비와 기름때로 뭉친 파마머리와, \\\n",
    "                          대충 툭툭 던지는 말투 등 전혀 로맨틱하지 않은 외모였다. 반감이 일면서 \\\n",
    "                          ‘난 외모지상주의자가 아니다’라고 자부했던 나에 대해 회의가 들었다.\\\n",
    "                           티브이를 꺼버릴까? 다른 걸 볼까? 그런데, 이상하다. 왜 이렇게 매력 있지? 개구리와\\\n",
    "                            같이 툭 불거진 눈망울 안에는 어떤 인도 배우에게서도 느끼지 못한 \\\n",
    "                            부드러움과 선량함, 무엇보다 슬픔이 있었다. 2시간 뒤 영화가 끝나고 나는 완전히 이 배우에게 빠졌다\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent= preprocessing([\"20분기 연속으로 적자에 시달리는 LG전자가 브랜드 이름부터 성능, 디자인까지 대대적인 변화를 \\\n",
    "                          적용한 LG 벨벳은 등장 전부터 온라인 커뮤니티를 뜨겁게 달궜다. 사용자들은 “디자인이 예쁘다”, \\\n",
    "                          “슬림하다”는 반응을 보이며 LG 벨벳에 대한 기대감을 드러냈다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent = preprocessing([\"코로나19 이후 주목받기 시작한 비대면(언택트) 서비스에서 데이터와 AI는 핵심 역할을 한다. 데이터3법 개정으로 각 기업·기관이 가지고 있던 활용되지 못한 데이터가 유통됨에 따라 상당한 시너지 효과가 발생할 것으로 기대된다. 바야흐로 ‘데이터 경제 생태계’ 조성의 시작인 셈이다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.50      0.02      0.03        58\n",
      "          경제       0.66      0.98      0.79       235\n",
      "          사회       0.92      0.94      0.93       139\n",
      "       생활/문화       1.00      0.40      0.57       103\n",
      "\n",
      "    accuracy                           0.75       535\n",
      "   macro avg       0.77      0.58      0.58       535\n",
      "weighted avg       0.77      0.75      0.70       535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Komoran 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에', '는', '은', '을', '했', '에게', '있', '이', '의', '하', '한', '다', '과', '때문', '할', '수', '무단', '따른', '및', '금지', '전재', '경향신문', '기자', '는데', '가', '등', '들', '파이낸셜', '저작', '등', '뉴스']\n",
      "['밤', '에', '귀가', '하', '던', '여성', '에게', '범죄', '를', '시도', '하', 'ㄴ', '대', '남성', '이', '구속', '되', '었', '다', '서울', '제주', '경찰서', '는', '상해', '혐의', '로', '씨', '를', '구속', '하', '아', '수사', '하', '고', '있', '다고', '일', '밝히', '었', '다', '씨', '는', '지난달', '일', '피해', '여성', '을', '인근', '지하철', '역', '에서부터', '따라가', '아', '폭행', '을', '시도', '하', '려다가', '도망가', 'ㄴ', '혐의', '를', '받', '는다', '피하', '아', '여성', '이', '저항', '하', '자', '놀라', 'ㄴ', '씨', '는', '도망가', '았', '으며', '신고', '를', '받', '고', '주변', '을', '수색', '하', '던', '경찰', '에', '체포', '되', '었', '다', '피하', '아', '여성', '은', '이', '과정', '에서', '경미', '하', 'ㄴ', '부상', '을', '입', '은', '것', '으로', '전하', '아', '지', '었', '다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "tokenizer=Komoran()\n",
    "# stopwords\n",
    "print(stopwords)\n",
    "# 형태소 분석\n",
    "print(tokenizer.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사 섹션 분류 안내 기사 섹션 정보 해당 언론사 분류 를 따르 고 습니다 언론사 개별 기사 를 개 이상 섹션 으로 중복 분류 ㄹ 습니다 닫 기\n",
      "[Komoran 분석기 preprocessing 걸린 시간] 27.643665552139282 sec\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "text_data=preprocessing(df['news'])\n",
    "print(text_data[0])\n",
    "\n",
    "print(\"[Komoran 분석기 preprocessing 걸린 시간]\", time.time()-start, 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komoran 형태소 분석기 ML 모델 적용하여 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 훈련 데이터와 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 10)\n",
    "\n",
    "# 나이브 베이즈 분류기 학습\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['경제']\n",
      "\n",
      "['생활/문화']\n",
      "\n",
      "['경제']\n",
      "\n",
      "['경제']\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "# 임의의 뉴스에 대한 카테고리 확인하기\n",
    "# 훈련, 테스트 데이터 아닌 임의 뉴스임\n",
    "# clf.predict() : 임의의 입력에 대해 나이브 베이즈 분류기가 예측한 값을 리턴함\n",
    "new_sent = preprocessing([\"민주당 일각에서 법사위의 체계·자구 심사 기능을 없애야 한다는 \\\n",
    "                           주장이 나오는 데 대해 “체계·자구 심사가 법안 지연의 수단으로 \\\n",
    "                          쓰이는 것은 바람직하지 않다”면서도 “국회를 통과하는 법안 중 위헌\\\n",
    "                          법률이 1년에 10건 넘게 나온다. 그런데 체계·자구 심사까지 없애면 매우 위험하다”고 반박했다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent = preprocessing([\"인도 로맨틱 코미디 영화 <까립까립 싱글>(2017)을 봤을 때 나는 두 눈을 의심했다. \\\n",
    "                          저 사람이 남자 주인공이라고? 노안에 가까운 이목구비와 기름때로 뭉친 파마머리와, \\\n",
    "                          대충 툭툭 던지는 말투 등 전혀 로맨틱하지 않은 외모였다. 반감이 일면서 \\\n",
    "                          ‘난 외모지상주의자가 아니다’라고 자부했던 나에 대해 회의가 들었다.\\\n",
    "                           티브이를 꺼버릴까? 다른 걸 볼까? 그런데, 이상하다. 왜 이렇게 매력 있지? 개구리와\\\n",
    "                            같이 툭 불거진 눈망울 안에는 어떤 인도 배우에게서도 느끼지 못한 \\\n",
    "                            부드러움과 선량함, 무엇보다 슬픔이 있었다. 2시간 뒤 영화가 끝나고 나는 완전히 이 배우에게 빠졌다\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent= preprocessing([\"20분기 연속으로 적자에 시달리는 LG전자가 브랜드 이름부터 성능, 디자인까지 대대적인 변화를 \\\n",
    "                          적용한 LG 벨벳은 등장 전부터 온라인 커뮤니티를 뜨겁게 달궜다. 사용자들은 “디자인이 예쁘다”, \\\n",
    "                          “슬림하다”는 반응을 보이며 LG 벨벳에 대한 기대감을 드러냈다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent = preprocessing([\"코로나19 이후 주목받기 시작한 비대면(언택트) 서비스에서 데이터와 AI는 핵심 역할을 한다. 데이터3법 개정으로 각 기업·기관이 가지고 있던 활용되지 못한 데이터가 유통됨에 따라 상당한 시너지 효과가 발생할 것으로 기대된다. 바야흐로 ‘데이터 경제 생태계’ 조성의 시작인 셈이다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       1.00      0.02      0.03        58\n",
      "          경제       0.66      0.98      0.79       235\n",
      "          사회       0.91      0.93      0.92       139\n",
      "       생활/문화       1.00      0.41      0.58       103\n",
      "\n",
      "    accuracy                           0.75       535\n",
      "   macro avg       0.89      0.58      0.58       535\n",
      "weighted avg       0.83      0.75      0.70       535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Okt 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에', '는', '은', '을', '했', '에게', '있', '이', '의', '하', '한', '다', '과', '때문', '할', '수', '무단', '따른', '및', '금지', '전재', '경향신문', '기자', '는데', '가', '등', '들', '파이낸셜', '저작', '등', '뉴스']\n",
      "['밤', '에', '귀가', '하던', '여성', '에게', '범죄', '를', '시도', '한', '대', '남성', '이', '구속', '됐다', '서울', '제', '주', '경찰서', '는', '상해', '혐의', '로', '씨', '를', '구속', '해', '수사', '하고', '있다고', '일', '밝혔다', '씨', '는', '지난달', '일', '피해', '여성', '을', '인근', '지하철', '역', '에서부터', '따라가', '폭행', '을', '시도', '하려다가', '도망간', '혐의', '를', '받는다', '피해', '여성', '이', '저항', '하자', '놀란', '씨', '는', '도망갔으며', '신고', '를', '받고', '주변', '을', '수색', '하던', '경찰', '에', '체포', '됐다', '피해', '여성', '은', '이', '과정', '에서', '경미한', '부상', '을', '입은', '것', '으로', '전해졌다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "tokenizer=Okt()\n",
    "# stopwords\n",
    "print(stopwords)\n",
    "# 형태소 분석\n",
    "print(tokenizer.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사 섹션 분류 안내 기사 섹션 정보 해당 언론사 분류 를 따르고 있습니다 언론사 개별 기사 를 개 이상 섹션 으로 중복 분류 있습니다 닫기\n",
      "[Okt 분석기 preprocessing 걸린 시간] 57.870362520217896 sec\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "text_data=preprocessing(df['news'])\n",
    "print(text_data[0])\n",
    "\n",
    "print(\"[Okt 분석기 preprocessing 걸린 시간]\", time.time()-start, 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okt 형태소 분석기 ML 모델 적용하여 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 훈련 데이터와 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 10)\n",
    "\n",
    "# 나이브 베이즈 분류기 학습\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['경제']\n",
      "\n",
      "['생활/문화']\n",
      "\n",
      "['경제']\n",
      "\n",
      "['경제']\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "# 임의의 뉴스에 대한 카테고리 확인하기\n",
    "# 훈련, 테스트 데이터 아닌 임의 뉴스임\n",
    "# clf.predict() : 임의의 입력에 대해 나이브 베이즈 분류기가 예측한 값을 리턴함\n",
    "new_sent = preprocessing([\"민주당 일각에서 법사위의 체계·자구 심사 기능을 없애야 한다는 \\\n",
    "                           주장이 나오는 데 대해 “체계·자구 심사가 법안 지연의 수단으로 \\\n",
    "                          쓰이는 것은 바람직하지 않다”면서도 “국회를 통과하는 법안 중 위헌\\\n",
    "                          법률이 1년에 10건 넘게 나온다. 그런데 체계·자구 심사까지 없애면 매우 위험하다”고 반박했다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent = preprocessing([\"인도 로맨틱 코미디 영화 <까립까립 싱글>(2017)을 봤을 때 나는 두 눈을 의심했다. \\\n",
    "                          저 사람이 남자 주인공이라고? 노안에 가까운 이목구비와 기름때로 뭉친 파마머리와, \\\n",
    "                          대충 툭툭 던지는 말투 등 전혀 로맨틱하지 않은 외모였다. 반감이 일면서 \\\n",
    "                          ‘난 외모지상주의자가 아니다’라고 자부했던 나에 대해 회의가 들었다.\\\n",
    "                           티브이를 꺼버릴까? 다른 걸 볼까? 그런데, 이상하다. 왜 이렇게 매력 있지? 개구리와\\\n",
    "                            같이 툭 불거진 눈망울 안에는 어떤 인도 배우에게서도 느끼지 못한 \\\n",
    "                            부드러움과 선량함, 무엇보다 슬픔이 있었다. 2시간 뒤 영화가 끝나고 나는 완전히 이 배우에게 빠졌다\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent= preprocessing([\"20분기 연속으로 적자에 시달리는 LG전자가 브랜드 이름부터 성능, 디자인까지 대대적인 변화를 \\\n",
    "                          적용한 LG 벨벳은 등장 전부터 온라인 커뮤니티를 뜨겁게 달궜다. 사용자들은 “디자인이 예쁘다”, \\\n",
    "                          “슬림하다”는 반응을 보이며 LG 벨벳에 대한 기대감을 드러냈다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))\n",
    "print()\n",
    "new_sent = preprocessing([\"코로나19 이후 주목받기 시작한 비대면(언택트) 서비스에서 데이터와 AI는 핵심 역할을 한다. 데이터3법 개정으로 각 기업·기관이 가지고 있던 활용되지 못한 데이터가 유통됨에 따라 상당한 시너지 효과가 발생할 것으로 기대된다. 바야흐로 ‘데이터 경제 생태계’ 조성의 시작인 셈이다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       1.00      0.02      0.03        58\n",
      "          경제       0.63      0.98      0.77       235\n",
      "          사회       0.92      0.91      0.92       139\n",
      "       생활/문화       1.00      0.30      0.46       103\n",
      "\n",
      "    accuracy                           0.73       535\n",
      "   macro avg       0.89      0.55      0.55       535\n",
      "weighted avg       0.82      0.73      0.67       535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 분석기별 장단점, 처리속도, 성능\n",
    "- 대략적인 장단점\n",
    "    - Kkma: 제일 느리다는 단점이 있다\n",
    "- 처리속도: Mecab <<<<< Kkma\n",
    "- 성능(f1 score): \n",
    "mecab: 2.77/0.75\n",
    "hannanum: 68.09/0.72\n",
    "Kkma: 500.63/0.75\n",
    "komoran: 27.64/0.75\n",
    "okt: 57.87/0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2. 성능튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
